{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953ed9c0",
   "metadata": {},
   "source": [
    "# Capstone Project Part 7: Optimizing Models\n",
    "\n",
    "**Authur:** Kate Meredith \n",
    "\n",
    "**Date:** September-November 2022\n",
    "\n",
    "**Notebook #**: 7 of\n",
    "\n",
    "## Background\n",
    "\n",
    "**Source:** Data was collected from [CoffeeReview.com](https://www.coffeereview.com/). See prior notebooks for details on scraping, cleaning, compilation and text transformation. \n",
    "\n",
    "**Goal:** Optimize models on full data set (both text transformed and original numerical values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2d3d1",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Documentation on [Ridge Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "- Documentation on [Lasso Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "- Documentation on [ElasticNet Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n",
    "- [Differences](https://towardsdatascience.com/whats-the-difference-between-linear-regression-lasso-ridge-and-elasticnet-8f997c60cf29) between linear, ridge, lasso and elastic net regression\n",
    "- How to [add headers back after scaling data](https://stackoverflow.com/questions/29586323/how-to-retain-column-headers-of-data-frame-after-pre-processing-in-scikit-learn)\n",
    "- Getting model [results as a dataframe](https://stackoverflow.com/questions/51734180/converting-statsmodels-summary-object-to-pandas-dataframe)\n",
    "- Using first row as [column headers](https://stackoverflow.com/questions/31328861/python-pandas-replacing-header-with-top-row)\n",
    "- Filtering sorted array to [get top N values](https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array)\n",
    "- [Math markdown](https://github.blog/2022-05-19-math-support-in-markdown/)\n",
    "- Getting coefficient labels using [SKLearn](https://stackoverflow.com/questions/26951880/scikit-learn-linear-regression-how-to-get-coefficients-respective-features)\n",
    "- Interpreting [Mean Absolute Error](https://towardsdatascience.com/what-are-rmse-and-mae-e405ce230383#:~:text=For%20an%20ideal%20model%2C%20RMSE,business%20solution%20is%20almost%20impossible!)\n",
    "- More on intepreting [Mean Absolute Error](https://canworksmart.com/using-mean-absolute-error-forecast-accuracy/)\n",
    "- Linear Regression [regularization](https://www.baeldung.com/cs/regularization-parameter-linear-regression)\n",
    "- Interpretable ML BrainStation kick-off notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d4ee2",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. Importing Libraries](#header1)\n",
    "* [2. Importing the Data and EDA](#header2)\n",
    "* [3. Scaling the Data](#header3)\n",
    "* [4. Principal Component Analysis](#header4)\n",
    "* [5. Fitting and Evaluating the Models](#header5)\n",
    "    * [5.1 Linear Regression](#subheader51)\n",
    "    * [5.2 XGBoost Regressor](#subheader52)\n",
    "* [6. Comparing all Models](#header6)\n",
    "* [7. Model Interpretation](#header7)\n",
    "    * [7.1 Using Stats Models for Additional Insights](#subheader71)\n",
    "    * [7.2 Comparing Stats Models and SKLearn Results](#subheader72)\n",
    "    * [7.3 Final Model Interpretation](#subheader73)\n",
    "* [8. Conclusion](#header8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235584d",
   "metadata": {},
   "source": [
    "## Importing Libraries  <a class=\"anchor\" id=\"header1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "7c3552bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95567a0a",
   "metadata": {},
   "source": [
    "## 2. Importing the Data and EDA <a class=\"anchor\" id=\"header2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fe582",
   "metadata": {},
   "source": [
    "In the last notebook, the text was transformed using a few different methods. Given time limitations, this will move forward with the data transformation method that worked best: TFIDF Vectorization without stemming. The data has already been split into remain (training), validation, and test data. These datasets will be imported here to use.\n",
    "\n",
    "Importing the remain (training) data and exploring some initial info about it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "1884d855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4194 entries, 0 to 4193\n",
      "Columns: 632 entries, coffee_name to zesty\n",
      "dtypes: float64(621), int64(9), object(2)\n",
      "memory usage: 20.2+ MB\n"
     ]
    }
   ],
   "source": [
    "Xremain_df_tfidf = pd.read_csv('tfidf_Xremain_combo_df.csv')\n",
    "Xremain_df_tfidf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "7ed88598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4194, 632)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xremain_df_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d7791",
   "metadata": {},
   "source": [
    "The remain data set has 4,194 rows and 632 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "13558f4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coffee_name</th>\n",
       "      <th>roaster_name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>bean_agtron</th>\n",
       "      <th>ground_agtron</th>\n",
       "      <th>aroma</th>\n",
       "      <th>acidity</th>\n",
       "      <th>body</th>\n",
       "      <th>flavor</th>\n",
       "      <th>...</th>\n",
       "      <th>wild</th>\n",
       "      <th>willem</th>\n",
       "      <th>wine</th>\n",
       "      <th>winey</th>\n",
       "      <th>winy</th>\n",
       "      <th>wisteria</th>\n",
       "      <th>wood</th>\n",
       "      <th>woody</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kenya Echo</td>\n",
       "      <td>CofFeeling</td>\n",
       "      <td>11</td>\n",
       "      <td>2012</td>\n",
       "      <td>46</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sumatra Takengon Gayo Natural</td>\n",
       "      <td>Coffee By Design</td>\n",
       "      <td>8</td>\n",
       "      <td>2022</td>\n",
       "      <td>62</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Colombia Finca Campoalegre</td>\n",
       "      <td>Durango Coffee Company</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>55</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Odin’s Viking Brew</td>\n",
       "      <td>Chaos Coffee Company</td>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "      <td>40</td>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tanzania Ruvuma</td>\n",
       "      <td>Lexington Coffee Roasters</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>50</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 632 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     coffee_name               roaster_name  month  year  \\\n",
       "0                     Kenya Echo                 CofFeeling     11  2012   \n",
       "1  Sumatra Takengon Gayo Natural           Coffee By Design      8  2022   \n",
       "2     Colombia Finca Campoalegre     Durango Coffee Company      9  2017   \n",
       "3             Odin’s Viking Brew       Chaos Coffee Company      4  2022   \n",
       "4                Tanzania Ruvuma  Lexington Coffee Roasters      4  2013   \n",
       "\n",
       "   bean_agtron  ground_agtron  aroma  acidity  body  flavor  ...  wild  \\\n",
       "0           46             58      8        8     9       9  ...   0.0   \n",
       "1           62             80      9        9     9       9  ...   0.0   \n",
       "2           55             77      9        9     9       9  ...   0.0   \n",
       "3           40             52      9        8     9       9  ...   0.0   \n",
       "4           50             63      8        9     9       9  ...   0.0   \n",
       "\n",
       "     willem  wine  winey  winy  wisteria  wood  woody  zest  zesty  \n",
       "0  0.000000   0.0    0.0   0.0       0.0   0.0    0.0   0.0    0.0  \n",
       "1  0.269005   0.0    0.0   0.0       0.0   0.0    0.0   0.0    0.0  \n",
       "2  0.000000   0.0    0.0   0.0       0.0   0.0    0.0   0.0    0.0  \n",
       "3  0.000000   0.0    0.0   0.0       0.0   0.0    0.0   0.0    0.0  \n",
       "4  0.000000   0.0    0.0   0.0       0.0   0.0    0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 632 columns]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xremain_df_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8d9ea",
   "metadata": {},
   "source": [
    "Most of the columns come from the vectorized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "8749f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1049 entries, 0 to 1048\n",
      "Columns: 632 entries, coffee_name to zesty\n",
      "dtypes: float64(618), int64(12), object(2)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#importing the validation data and checking out the info\n",
    "Xval_df_tfidf = pd.read_csv('tfidf_Xval_combo_df.csv')\n",
    "Xval_df_tfidf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "313c5ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1049, 632)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xval_df_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd1a98",
   "metadata": {},
   "source": [
    "The validation data set has 1,049 rows and 632 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "069355df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coffee_name</th>\n",
       "      <th>roaster_name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>bean_agtron</th>\n",
       "      <th>ground_agtron</th>\n",
       "      <th>aroma</th>\n",
       "      <th>acidity</th>\n",
       "      <th>body</th>\n",
       "      <th>flavor</th>\n",
       "      <th>...</th>\n",
       "      <th>wild</th>\n",
       "      <th>willem</th>\n",
       "      <th>wine</th>\n",
       "      <th>winey</th>\n",
       "      <th>winy</th>\n",
       "      <th>wisteria</th>\n",
       "      <th>wood</th>\n",
       "      <th>woody</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Peru Cajamarca</td>\n",
       "      <td>Beansmith Coffee</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>58</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classic Espresso</td>\n",
       "      <td>New Harvest Coffee Roasters</td>\n",
       "      <td>8</td>\n",
       "      <td>2003</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mini-Me Washed Kaffa Forest</td>\n",
       "      <td>Simon Hsieh’s Aroma Roast Coffees</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ethiopia Awassa</td>\n",
       "      <td>Paradise Roasters</td>\n",
       "      <td>11</td>\n",
       "      <td>2017</td>\n",
       "      <td>56</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140739</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Latte Espresso</td>\n",
       "      <td>Nongfu Spring Co., Ltd.</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>40</td>\n",
       "      <td>52</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 632 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   coffee_name                       roaster_name  month  \\\n",
       "0               Peru Cajamarca                   Beansmith Coffee      1   \n",
       "1             Classic Espresso        New Harvest Coffee Roasters      8   \n",
       "2  Mini-Me Washed Kaffa Forest  Simon Hsieh’s Aroma Roast Coffees      1   \n",
       "3              Ethiopia Awassa                  Paradise Roasters     11   \n",
       "4               Latte Espresso            Nongfu Spring Co., Ltd.     12   \n",
       "\n",
       "   year  bean_agtron  ground_agtron  aroma  acidity  body  flavor  ...  wild  \\\n",
       "0  2014           58             71      8        8     8       9  ...   0.0   \n",
       "1  2003           24             29      7        6     6       6  ...   0.0   \n",
       "2  2018           40             60      9        8     9       9  ...   0.0   \n",
       "3  2017           56             76      9        8     9       9  ...   0.0   \n",
       "4  2019           40             52      7        7     6       7  ...   0.0   \n",
       "\n",
       "   willem      wine  winey  winy  wisteria      wood  woody      zest  zesty  \n",
       "0     0.0  0.000000    0.0   0.0       0.0  0.000000    0.0  0.000000    0.0  \n",
       "1     0.0  0.666539    0.0   0.0       0.0  0.000000    0.0  0.000000    0.0  \n",
       "2     0.0  0.000000    0.0   0.0       0.0  0.000000    0.0  0.000000    0.0  \n",
       "3     0.0  0.000000    0.0   0.0       0.0  0.330093    0.0  0.140739    0.0  \n",
       "4     0.0  0.000000    0.0   0.0       0.0  0.167008    0.0  0.000000    0.0  \n",
       "\n",
       "[5 rows x 632 columns]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xval_df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "1ad82135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1311 entries, 0 to 1310\n",
      "Columns: 632 entries, coffee_name to zesty\n",
      "dtypes: float64(619), int64(11), object(2)\n",
      "memory usage: 6.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#importing the test data and checking out the info\n",
    "Xtest_df_tfidf = pd.read_csv('tfidf_Xtest_combo_df.csv')\n",
    "Xtest_df_tfidf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "be65476a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1311, 632)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_df_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b02b9a",
   "metadata": {},
   "source": [
    "The test dataframe has 1311 rows and 632 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "619f35ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coffee_name</th>\n",
       "      <th>roaster_name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>bean_agtron</th>\n",
       "      <th>ground_agtron</th>\n",
       "      <th>aroma</th>\n",
       "      <th>acidity</th>\n",
       "      <th>body</th>\n",
       "      <th>flavor</th>\n",
       "      <th>...</th>\n",
       "      <th>wild</th>\n",
       "      <th>willem</th>\n",
       "      <th>wine</th>\n",
       "      <th>winey</th>\n",
       "      <th>winy</th>\n",
       "      <th>wisteria</th>\n",
       "      <th>wood</th>\n",
       "      <th>woody</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kenya “Karie” Micro-lot</td>\n",
       "      <td>Paradise Roasters</td>\n",
       "      <td>7</td>\n",
       "      <td>2011</td>\n",
       "      <td>48</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ethiopia Natural Guji Kayon Mountain</td>\n",
       "      <td>Kakalove Cafe</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>56</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DTS8 Espresso Classic</td>\n",
       "      <td>DTS8 Coffee</td>\n",
       "      <td>12</td>\n",
       "      <td>2011</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Organic Sumatra</td>\n",
       "      <td>Great Northern Gourmet Coffees</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>54</td>\n",
       "      <td>71</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Organic Papua New Guinea Purosa A</td>\n",
       "      <td>Supreme Bean Coffee Roasters</td>\n",
       "      <td>4</td>\n",
       "      <td>2009</td>\n",
       "      <td>54</td>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 632 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            coffee_name                    roaster_name  \\\n",
       "0               Kenya “Karie” Micro-lot               Paradise Roasters   \n",
       "1  Ethiopia Natural Guji Kayon Mountain                   Kakalove Cafe   \n",
       "2                 DTS8 Espresso Classic                     DTS8 Coffee   \n",
       "3                       Organic Sumatra  Great Northern Gourmet Coffees   \n",
       "4     Organic Papua New Guinea Purosa A    Supreme Bean Coffee Roasters   \n",
       "\n",
       "   month  year  bean_agtron  ground_agtron  aroma  acidity  body  flavor  ...  \\\n",
       "0      7  2011           48             66      9        8     8       8  ...   \n",
       "1      6  2017           56             76      9        8     9       9  ...   \n",
       "2     12  2011           38             42      9        8     8       9  ...   \n",
       "3      2  1999           54             71      6        5     7       5  ...   \n",
       "4      4  2009           54             80      8        8     7       8  ...   \n",
       "\n",
       "   wild  willem  wine  winey  winy  wisteria      wood  woody  zest  zesty  \n",
       "0   0.0     0.0   0.0    0.0   0.0       0.0  0.000000    0.0   0.0    0.0  \n",
       "1   0.0     0.0   0.0    0.0   0.0       0.0  0.000000    0.0   0.0    0.0  \n",
       "2   0.0     0.0   0.0    0.0   0.0       0.0  0.000000    0.0   0.0    0.0  \n",
       "3   0.0     0.0   0.0    0.0   0.0       0.0  0.000000    0.0   0.0    0.0  \n",
       "4   0.0     0.0   0.0    0.0   0.0       0.0  0.153136    0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 632 columns]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_df_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8bc3c5",
   "metadata": {},
   "source": [
    "Importing the y values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "3c9c15b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall_score\n",
       "0             92\n",
       "1             94\n",
       "2             94\n",
       "3             93\n",
       "4             93"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_remain_df = pd.read_csv('y_remain_df.csv')\n",
    "y_remain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "4f8ea157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92\n",
       "1       94\n",
       "2       94\n",
       "3       93\n",
       "4       93\n",
       "        ..\n",
       "4189    90\n",
       "4190    92\n",
       "4191    92\n",
       "4192    79\n",
       "4193    95\n",
       "Name: overall_score, Length: 4194, dtype: int64"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formatting y to work for modeling later\n",
    "#note lengths match corresponding X dataframe\n",
    "y_remain = y_remain_df['overall_score']\n",
    "y_remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "4629e9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall_score\n",
       "0             91\n",
       "1             82\n",
       "2             95\n",
       "3             93\n",
       "4             83"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_df = pd.read_csv('y_val_df.csv')\n",
    "y_val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "22f42d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91\n",
       "1       82\n",
       "2       95\n",
       "3       93\n",
       "4       83\n",
       "        ..\n",
       "1044    91\n",
       "1045    94\n",
       "1046    93\n",
       "1047    93\n",
       "1048    93\n",
       "Name: overall_score, Length: 1049, dtype: int64"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formatting y to work for modeling later\n",
    "#note lengths match corresponding X dataframe\n",
    "y_val = y_val_df['overall_score']\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "7a4f5833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall_score\n",
       "0             92\n",
       "1             93\n",
       "2             92\n",
       "3             79\n",
       "4             86"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_df = pd.read_csv('y_test_df.csv')\n",
    "y_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "7130df20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92\n",
       "1       93\n",
       "2       92\n",
       "3       79\n",
       "4       86\n",
       "        ..\n",
       "1306    87\n",
       "1307    91\n",
       "1308    93\n",
       "1309    93\n",
       "1310    94\n",
       "Name: overall_score, Length: 1311, dtype: int64"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formatting y to work for modeling later\n",
    "#note lengths match corresponding X dataframe\n",
    "y_test = y_test_df['overall_score']\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "4966e262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifying no null values introduced during vectorizing process\n",
    "Xremain_df_tfidf.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "6e39c777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xval_df_tfidf.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "8d1093d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_df_tfidf.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "2c57fc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_remain.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "d362a7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "59014536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f722192",
   "metadata": {},
   "source": [
    "Dropping the remaining text columns `coffee_name` and `roaster_name`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "3080792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xremain_df_tfidf.drop(['coffee_name', 'roaster_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "b89be237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>bean_agtron</th>\n",
       "      <th>ground_agtron</th>\n",
       "      <th>aroma</th>\n",
       "      <th>acidity</th>\n",
       "      <th>body</th>\n",
       "      <th>flavor</th>\n",
       "      <th>aftertaste</th>\n",
       "      <th>roaster_lat</th>\n",
       "      <th>...</th>\n",
       "      <th>wild</th>\n",
       "      <th>willem</th>\n",
       "      <th>wine</th>\n",
       "      <th>winey</th>\n",
       "      <th>winy</th>\n",
       "      <th>wisteria</th>\n",
       "      <th>wood</th>\n",
       "      <th>woody</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>2012</td>\n",
       "      <td>46</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>23.128402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2022</td>\n",
       "      <td>62</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>43.661028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>55</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>37.276948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "      <td>40</td>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>35.142944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>50</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>37.784021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 630 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  year  bean_agtron  ground_agtron  aroma  acidity  body  flavor  \\\n",
       "0     11  2012           46             58      8        8     9       9   \n",
       "1      8  2022           62             80      9        9     9       9   \n",
       "2      9  2017           55             77      9        9     9       9   \n",
       "3      4  2022           40             52      9        8     9       9   \n",
       "4      4  2013           50             63      8        9     9       9   \n",
       "\n",
       "   aftertaste  roaster_lat  ...  wild    willem  wine  winey  winy  wisteria  \\\n",
       "0           8    23.128402  ...   0.0  0.000000   0.0    0.0   0.0       0.0   \n",
       "1           8    43.661028  ...   0.0  0.269005   0.0    0.0   0.0       0.0   \n",
       "2           8    37.276948  ...   0.0  0.000000   0.0    0.0   0.0       0.0   \n",
       "3           8    35.142944  ...   0.0  0.000000   0.0    0.0   0.0       0.0   \n",
       "4           8    37.784021  ...   0.0  0.000000   0.0    0.0   0.0       0.0   \n",
       "\n",
       "   wood  woody  zest  zesty  \n",
       "0   0.0    0.0   0.0    0.0  \n",
       "1   0.0    0.0   0.0    0.0  \n",
       "2   0.0    0.0   0.0    0.0  \n",
       "3   0.0    0.0   0.0    0.0  \n",
       "4   0.0    0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 630 columns]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xremain_df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "3d51e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval_df_tfidf.drop(['coffee_name', 'roaster_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "b86eeff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>bean_agtron</th>\n",
       "      <th>ground_agtron</th>\n",
       "      <th>aroma</th>\n",
       "      <th>acidity</th>\n",
       "      <th>body</th>\n",
       "      <th>flavor</th>\n",
       "      <th>aftertaste</th>\n",
       "      <th>roaster_lat</th>\n",
       "      <th>...</th>\n",
       "      <th>wild</th>\n",
       "      <th>willem</th>\n",
       "      <th>wine</th>\n",
       "      <th>winey</th>\n",
       "      <th>winy</th>\n",
       "      <th>wisteria</th>\n",
       "      <th>wood</th>\n",
       "      <th>woody</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>58</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>41.258746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2003</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>41.813712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>24.992999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2017</td>\n",
       "      <td>56</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>45.016573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140739</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>40</td>\n",
       "      <td>52</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>30.248963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 630 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  year  bean_agtron  ground_agtron  aroma  acidity  body  flavor  \\\n",
       "0      1  2014           58             71      8        8     8       9   \n",
       "1      8  2003           24             29      7        6     6       6   \n",
       "2      1  2018           40             60      9        8     9       9   \n",
       "3     11  2017           56             76      9        8     9       9   \n",
       "4     12  2019           40             52      7        7     6       7   \n",
       "\n",
       "   aftertaste  roaster_lat  ...  wild  willem      wine  winey  winy  \\\n",
       "0           8    41.258746  ...   0.0     0.0  0.000000    0.0   0.0   \n",
       "1           6    41.813712  ...   0.0     0.0  0.666539    0.0   0.0   \n",
       "2           9    24.992999  ...   0.0     0.0  0.000000    0.0   0.0   \n",
       "3           8    45.016573  ...   0.0     0.0  0.000000    0.0   0.0   \n",
       "4           6    30.248963  ...   0.0     0.0  0.000000    0.0   0.0   \n",
       "\n",
       "   wisteria      wood  woody      zest  zesty  \n",
       "0       0.0  0.000000    0.0  0.000000    0.0  \n",
       "1       0.0  0.000000    0.0  0.000000    0.0  \n",
       "2       0.0  0.000000    0.0  0.000000    0.0  \n",
       "3       0.0  0.330093    0.0  0.140739    0.0  \n",
       "4       0.0  0.167008    0.0  0.000000    0.0  \n",
       "\n",
       "[5 rows x 630 columns]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xval_df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "52ac45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_df_tfidf.drop(['coffee_name', 'roaster_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "23af1cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>bean_agtron</th>\n",
       "      <th>ground_agtron</th>\n",
       "      <th>aroma</th>\n",
       "      <th>acidity</th>\n",
       "      <th>body</th>\n",
       "      <th>flavor</th>\n",
       "      <th>aftertaste</th>\n",
       "      <th>roaster_lat</th>\n",
       "      <th>...</th>\n",
       "      <th>wild</th>\n",
       "      <th>willem</th>\n",
       "      <th>wine</th>\n",
       "      <th>winey</th>\n",
       "      <th>winy</th>\n",
       "      <th>wisteria</th>\n",
       "      <th>wood</th>\n",
       "      <th>woody</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>2011</td>\n",
       "      <td>48</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>45.016573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>56</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>23.487137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>2011</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>31.232276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>54</td>\n",
       "      <td>71</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>43.684017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2009</td>\n",
       "      <td>54</td>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>34.172904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 630 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  year  bean_agtron  ground_agtron  aroma  acidity  body  flavor  \\\n",
       "0      7  2011           48             66      9        8     8       8   \n",
       "1      6  2017           56             76      9        8     9       9   \n",
       "2     12  2011           38             42      9        8     8       9   \n",
       "3      2  1999           54             71      6        5     7       5   \n",
       "4      4  2009           54             80      8        8     7       8   \n",
       "\n",
       "   aftertaste  roaster_lat  ...  wild  willem  wine  winey  winy  wisteria  \\\n",
       "0           9    45.016573  ...   0.0     0.0   0.0    0.0   0.0       0.0   \n",
       "1           8    23.487137  ...   0.0     0.0   0.0    0.0   0.0       0.0   \n",
       "2           7    31.232276  ...   0.0     0.0   0.0    0.0   0.0       0.0   \n",
       "3           5    43.684017  ...   0.0     0.0   0.0    0.0   0.0       0.0   \n",
       "4           8    34.172904  ...   0.0     0.0   0.0    0.0   0.0       0.0   \n",
       "\n",
       "       wood  woody  zest  zesty  \n",
       "0  0.000000    0.0   0.0    0.0  \n",
       "1  0.000000    0.0   0.0    0.0  \n",
       "2  0.000000    0.0   0.0    0.0  \n",
       "3  0.000000    0.0   0.0    0.0  \n",
       "4  0.153136    0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 630 columns]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_df_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36a60a",
   "metadata": {},
   "source": [
    "## 3. Scaling the Data <a class=\"anchor\" id=\"header3\"></a>\n",
    "\n",
    "While scaling isn't required for all the model types (like linear regression), we'll go ahead and scale now to simplify the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "347b51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting min max scaler on remain data\n",
    "mm_scaler = MinMaxScaler()\n",
    "mm_scaler.fit(Xremain_df_tfidf)\n",
    "\n",
    "#transforming the all the X data\n",
    "X_mm_scaled_remain = mm_scaler.transform(Xremain_df_tfidf)\n",
    "X_mm_scaled_val = mm_scaler.transform(Xval_df_tfidf)\n",
    "X_mm_scaled_test = mm_scaler.transform(Xtest_df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "fed3cae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90909091, 0.6       , 0.4375    , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.63636364, 1.        , 0.6375    , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.72727273, 0.8       , 0.55      , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.18181818, 0.72      , 0.525     , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.09090909, 0.08      , 0.5375    , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.09090909, 0.44      , 0.5       , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview scaled date\n",
    "X_mm_scaled_remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "7cf71317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>bean_agtron</th>\n",
       "      <th>ground_agtron</th>\n",
       "      <th>aroma</th>\n",
       "      <th>acidity</th>\n",
       "      <th>body</th>\n",
       "      <th>flavor</th>\n",
       "      <th>aftertaste</th>\n",
       "      <th>roaster_lat</th>\n",
       "      <th>...</th>\n",
       "      <th>wild</th>\n",
       "      <th>willem</th>\n",
       "      <th>wine</th>\n",
       "      <th>winey</th>\n",
       "      <th>winy</th>\n",
       "      <th>wisteria</th>\n",
       "      <th>wood</th>\n",
       "      <th>woody</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.593722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.6375</td>\n",
       "      <td>0.779070</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.793723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.505764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.731538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.453488</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.710752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.4875</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.736477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 630 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      month  year  bean_agtron  ground_agtron  aroma   acidity      body  \\\n",
       "0  0.909091  0.60       0.4375       0.523256  0.750  0.777778  0.833333   \n",
       "1  0.636364  1.00       0.6375       0.779070  0.875  0.888889  0.833333   \n",
       "2  0.727273  0.80       0.5500       0.744186  0.875  0.888889  0.833333   \n",
       "3  0.272727  1.00       0.3625       0.453488  0.875  0.777778  0.833333   \n",
       "4  0.272727  0.64       0.4875       0.581395  0.750  0.888889  0.833333   \n",
       "\n",
       "     flavor  aftertaste  roaster_lat  ...  wild    willem  wine  winey  winy  \\\n",
       "0  0.888889        0.75     0.593722  ...   0.0  0.000000   0.0    0.0   0.0   \n",
       "1  0.888889        0.75     0.793723  ...   0.0  0.505764   0.0    0.0   0.0   \n",
       "2  0.888889        0.75     0.731538  ...   0.0  0.000000   0.0    0.0   0.0   \n",
       "3  0.888889        0.75     0.710752  ...   0.0  0.000000   0.0    0.0   0.0   \n",
       "4  0.888889        0.75     0.736477  ...   0.0  0.000000   0.0    0.0   0.0   \n",
       "\n",
       "   wisteria  wood  woody  zest  zesty  \n",
       "0       0.0   0.0    0.0   0.0    0.0  \n",
       "1       0.0   0.0    0.0   0.0    0.0  \n",
       "2       0.0   0.0    0.0   0.0    0.0  \n",
       "3       0.0   0.0    0.0   0.0    0.0  \n",
       "4       0.0   0.0    0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 630 columns]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add headers back to our scaled remain data for interpretability later \n",
    "X_mm_scaled_remain = pd.DataFrame(X_mm_scaled_remain, columns = Xremain_df_tfidf.columns)\n",
    "\n",
    "#verify headers added back\n",
    "X_mm_scaled_remain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "1557bca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.68      , 0.5875    , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.63636364, 0.24      , 0.1625    , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.84      , 0.3625    , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.09090909, 1.        , 0.4375    , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.90909091, 0.96      , 0.5875    , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.45454545, 0.96      , 0.5875    , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mm_scaled_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "91639020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>bean_agtron</th>\n",
       "      <th>ground_agtron</th>\n",
       "      <th>aroma</th>\n",
       "      <th>acidity</th>\n",
       "      <th>body</th>\n",
       "      <th>flavor</th>\n",
       "      <th>aftertaste</th>\n",
       "      <th>roaster_lat</th>\n",
       "      <th>...</th>\n",
       "      <th>wild</th>\n",
       "      <th>willem</th>\n",
       "      <th>wine</th>\n",
       "      <th>winey</th>\n",
       "      <th>winy</th>\n",
       "      <th>wisteria</th>\n",
       "      <th>wood</th>\n",
       "      <th>woody</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.770324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.775729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.852827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.611884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.806927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.453488</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.663081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.288976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 630 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      month  year  bean_agtron  ground_agtron  aroma   acidity      body  \\\n",
       "0  0.000000  0.68       0.5875       0.674419  0.750  0.777778  0.666667   \n",
       "1  0.636364  0.24       0.1625       0.186047  0.625  0.555556  0.333333   \n",
       "2  0.000000  0.84       0.3625       0.546512  0.875  0.777778  0.833333   \n",
       "3  0.909091  0.80       0.5625       0.732558  0.875  0.777778  0.833333   \n",
       "4  1.000000  0.88       0.3625       0.453488  0.625  0.666667  0.333333   \n",
       "\n",
       "     flavor  aftertaste  roaster_lat  ...  wild  willem      wine  winey  \\\n",
       "0  0.888889       0.750     0.770324  ...   0.0     0.0  0.000000    0.0   \n",
       "1  0.555556       0.500     0.775729  ...   0.0     0.0  0.852827    0.0   \n",
       "2  0.888889       0.875     0.611884  ...   0.0     0.0  0.000000    0.0   \n",
       "3  0.888889       0.750     0.806927  ...   0.0     0.0  0.000000    0.0   \n",
       "4  0.666667       0.500     0.663081  ...   0.0     0.0  0.000000    0.0   \n",
       "\n",
       "   winy  wisteria      wood  woody      zest  zesty  \n",
       "0   0.0       0.0  0.000000    0.0  0.000000    0.0  \n",
       "1   0.0       0.0  0.000000    0.0  0.000000    0.0  \n",
       "2   0.0       0.0  0.000000    0.0  0.000000    0.0  \n",
       "3   0.0       0.0  0.571167    0.0  0.369902    0.0  \n",
       "4   0.0       0.0  0.288976    0.0  0.000000    0.0  \n",
       "\n",
       "[5 rows x 630 columns]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add headers back to our scaled validation data for interpretability later \n",
    "X_mm_scaled_val = pd.DataFrame(X_mm_scaled_val, columns = Xval_df_tfidf.columns)\n",
    "X_mm_scaled_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "362fb5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54545455, 0.56      , 0.4625    , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.45454545, 0.8       , 0.5625    , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.56      , 0.3375    , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.27272727, 0.76      , 0.65      , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.09090909, 0.76      , 0.6       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.27272727, 0.8       , 0.5625    , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mm_scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "06b5f867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>bean_agtron</th>\n",
       "      <th>ground_agtron</th>\n",
       "      <th>aroma</th>\n",
       "      <th>acidity</th>\n",
       "      <th>body</th>\n",
       "      <th>flavor</th>\n",
       "      <th>aftertaste</th>\n",
       "      <th>roaster_lat</th>\n",
       "      <th>...</th>\n",
       "      <th>wild</th>\n",
       "      <th>willem</th>\n",
       "      <th>wine</th>\n",
       "      <th>winey</th>\n",
       "      <th>winy</th>\n",
       "      <th>wisteria</th>\n",
       "      <th>wood</th>\n",
       "      <th>woody</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.4625</td>\n",
       "      <td>0.616279</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.806927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.597216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.3375</td>\n",
       "      <td>0.337209</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.672659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.793947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.779070</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.701303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 630 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      month  year  bean_agtron  ground_agtron  aroma   acidity      body  \\\n",
       "0  0.545455  0.56       0.4625       0.616279  0.875  0.777778  0.666667   \n",
       "1  0.454545  0.80       0.5625       0.732558  0.875  0.777778  0.833333   \n",
       "2  1.000000  0.56       0.3375       0.337209  0.875  0.777778  0.666667   \n",
       "3  0.090909  0.08       0.5375       0.674419  0.500  0.444444  0.500000   \n",
       "4  0.272727  0.48       0.5375       0.779070  0.750  0.777778  0.500000   \n",
       "\n",
       "     flavor  aftertaste  roaster_lat  ...  wild  willem  wine  winey  winy  \\\n",
       "0  0.777778       0.875     0.806927  ...   0.0     0.0   0.0    0.0   0.0   \n",
       "1  0.888889       0.750     0.597216  ...   0.0     0.0   0.0    0.0   0.0   \n",
       "2  0.888889       0.625     0.672659  ...   0.0     0.0   0.0    0.0   0.0   \n",
       "3  0.444444       0.375     0.793947  ...   0.0     0.0   0.0    0.0   0.0   \n",
       "4  0.777778       0.750     0.701303  ...   0.0     0.0   0.0    0.0   0.0   \n",
       "\n",
       "   wisteria      wood  woody  zest  zesty  \n",
       "0       0.0  0.000000    0.0   0.0    0.0  \n",
       "1       0.0  0.000000    0.0   0.0    0.0  \n",
       "2       0.0  0.000000    0.0   0.0    0.0  \n",
       "3       0.0  0.000000    0.0   0.0    0.0  \n",
       "4       0.0  0.264974    0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 630 columns]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add headers back to our scaled test data for interpretability later \n",
    "X_mm_scaled_test = pd.DataFrame(X_mm_scaled_test, columns = Xtest_df_tfidf.columns)\n",
    "X_mm_scaled_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4ce64",
   "metadata": {},
   "source": [
    "## 4. Principal Component Analysis <a class=\"anchor\" id=\"header4\"></a>\n",
    "\n",
    "Given the number of dimensions in this dataset, simplifying our dimensions (e.g. reducing the number of columns) might be beneficial. However, interpretability will be lost with PCA. To find out if simplifying the dimensions is worth losing interpretability, the models will be run on both the full dataset and a simplified version. For the dimensionality reduced version, Pricipal Component Analysis (PCA) will be applied. If performance is similar, we can move ahead with the full data set and retain interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "456f96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instatiate and fit the PCA model\n",
    "my_PCA = PCA(n_components = 0.9) #retaining 90% of the variance\n",
    "my_PCA.fit(X_mm_scaled_val)\n",
    "\n",
    "# transform data \n",
    "X_remain_PCA = my_PCA.transform(X_mm_scaled_remain)\n",
    "X_val_PCA = my_PCA.transform(X_mm_scaled_val)\n",
    "X_test_PCA = my_PCA.transform(X_mm_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "41d187e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance captured by PC1:  0.250\n",
      "Variance captured by PC2:  0.154\n",
      "Proportion of variance captured by PC1:  0.049\n",
      "Proportion of variance captured by PC2:  0.030\n"
     ]
    }
   ],
   "source": [
    "print(f\"Variance captured by PC1: {my_PCA.explained_variance_[0]: 0.3f}\")\n",
    "print(f\"Variance captured by PC2: {my_PCA.explained_variance_[1]: 0.3f}\")\n",
    "\n",
    "print(f\"Proportion of variance captured by PC1: {my_PCA.explained_variance_ratio_[0]: 0.3f}\")\n",
    "print(f\"Proportion of variance captured by PC2: {my_PCA.explained_variance_ratio_[1]: 0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "c4f7be61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (4194, 630)\n",
      "PCA Transformed: (4194, 280)\n"
     ]
    }
   ],
   "source": [
    "print(f'Original: {Xremain_df_tfidf.shape}')\n",
    "print(f'PCA Transformed: {X_remain_PCA.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaee630",
   "metadata": {},
   "source": [
    "PCA is able to capture 90% of the variance while decreasing the number of features significantly to 280 (down from 630)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71290779",
   "metadata": {},
   "source": [
    "## 5. Fitting and Evaluating the Models <a class=\"anchor\" id=\"header5\"></a>\n",
    "\n",
    "Below will fit each of the models, trying both the 'full' dataset and the PCA dataset. In all of these, the data is scaled. At the end, $R^2$ and Mean Absolute Error results will be compared across all models.\n",
    "\n",
    "### 5.1 Linear Regression <a class=\"anchor\" id=\"subheader51\"></a>\n",
    "\n",
    "**\"Vanilla\" Linear Regression** \n",
    "\n",
    "This first uses a basic, \"vanilla\" linear regression model. This is what we used in previous notebooks to check in on how our data transformations were performing.\n",
    "\n",
    "For reference:\n",
    "- The best validation data $R^2$ from running Linear Regression on the text data alone was about 0.761.\n",
    "- The best validation data $R^2$ from running Linear Regression on the non-text data was about 0.898.\n",
    "\n",
    "**Linear Regression: Min Max, Full Dataset**\n",
    " \n",
    "Below runs the model on the full, scaled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "2b29d2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared score for lr_model_full training data is: 0.9478426614205148\n",
      "The R-squared score for lr_model_full validation data is: 0.8982269625899719\n"
     ]
    }
   ],
   "source": [
    "# 1. Instantiate the model\n",
    "lr_model_full = LinearRegression()\n",
    "\n",
    "# 2. Fit the model\n",
    "lr_model_full.fit(X_mm_scaled_remain, y_remain)\n",
    "\n",
    "# 3. Scoring the models\n",
    "print(f'The R-squared score for lr_model_full training data is: {lr_model_full.score(X_mm_scaled_remain, y_remain)}')\n",
    "\n",
    "lr_model_full_val_r2 = lr_model_full.score(X_mm_scaled_val, y_val)\n",
    "print(f'The R-squared score for lr_model_full validation data is: {lr_model_full_val_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f29966",
   "metadata": {},
   "source": [
    "Let's also check the mean absolute errors for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "3b3bc62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain MAE --- 0.6185051929005212\n",
      "Val MAE ---- 0.7724205905836047\n"
     ]
    }
   ],
   "source": [
    "y_pred_remain = lr_model_full.predict(X_mm_scaled_remain)\n",
    "y_pred_val = lr_model_full.predict(X_mm_scaled_val)\n",
    "\n",
    "print(f'Remain MAE --- {mean_absolute_error(y_remain, y_pred_remain)}')\n",
    "lr_full_val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "print(f'Val MAE ---- {lr_full_val_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f747bbd0",
   "metadata": {},
   "source": [
    "**Linear Regression: Min Max, PCA Dataset**\n",
    " \n",
    "Below runs the model on the simplified, scaled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "af9a091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared score for lr_model_pca training data is: 0.9240498186332646\n",
      "The R-squared score for lr_model_pca validation data is: 0.8981515772813482\n"
     ]
    }
   ],
   "source": [
    "# 1. Instantiate the model\n",
    "lr_model_pca = LinearRegression()\n",
    "\n",
    "# 2. Fit the model\n",
    "lr_model_pca.fit(X_remain_PCA, y_remain)\n",
    "\n",
    "# 3. Scoring the models\n",
    "print(f'The R-squared score for lr_model_pca training data is: {lr_model_pca.score(X_remain_PCA, y_remain)}')\n",
    "\n",
    "lr_model_pca_val_r2 = lr_model_pca.score(X_val_PCA, y_val)\n",
    "print(f'The R-squared score for lr_model_pca validation data is: {lr_model_pca_val_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "f6790fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain MAE --- 0.7376877706419198\n",
      "Val MAE ---- 0.7863830861370323\n"
     ]
    }
   ],
   "source": [
    "y_pred_remain_pca = lr_model_pca.predict(X_remain_PCA)\n",
    "y_pred_val_pca = lr_model_pca.predict(X_val_PCA)\n",
    "\n",
    "print(f'Remain MAE --- {mean_absolute_error(y_remain, y_pred_remain_pca)}')\n",
    "lr_pca_val_mae = mean_absolute_error(y_val, y_pred_val_pca)\n",
    "print(f'Val MAE ---- {lr_pca_val_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f795c",
   "metadata": {},
   "source": [
    "To avoid overfitting, we can try adding regularization to our linear regression model. Below we'll look at Ridge, Lasso and ElasticNet Regression to see how different regularization techniques affect the model.\n",
    "\n",
    "**Ridge Regression: Min Max, Full Dataset**\n",
    "\n",
    "The pipeline below will look for the optimal Ridge Regression model using the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "4a7f47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our pipeline\n",
    "#the values listed here for vectorization are placeholders\n",
    "linreg = Pipeline(\n",
    "    [\n",
    "        (\"model\", Ridge()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "5a9f01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ran the grid search using a series of changes on these paramaters\n",
    "#started with more extreme options for each paramater and then narrowed down to these\n",
    "\n",
    "parameters = {\n",
    "    \"model__alpha\": [6.2], \n",
    "    \"model__solver\": ['sag'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "09a19adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'model__alpha': 6.2, 'model__solver': 'sag'}\n",
      "Best score: 0.9216554918821132\n"
     ]
    }
   ],
   "source": [
    "#running the serach to find the best combination of these parameters\n",
    "grid_search = GridSearchCV(linreg, parameters)\n",
    "\n",
    "grid_search.fit(X_mm_scaled_remain, y_remain)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "#checking the R-squared\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48932369",
   "metadata": {},
   "source": [
    "Below will see how these parameters do with our remain (training) and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "d86cae2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared score for lr_model_ridge_full training data is: 0.9430169761280489\n",
      "The R-squared score for lr_model_ridge_full validation data is: 0.9054632001428237\n"
     ]
    }
   ],
   "source": [
    "#instatiate and fit best model\n",
    "lr_model_ridge_full = Ridge(alpha=6.2, solver='sag')\n",
    "lr_model_ridge_full.fit(X_mm_scaled_remain, y_remain)\n",
    "\n",
    "# 3. Scoring the models\n",
    "print(f'The R-squared score for lr_model_ridge_full training data is: {lr_model_ridge_full.score(X_mm_scaled_remain, y_remain)}')\n",
    "\n",
    "lr_model_ridge_full_val_r2 = lr_model_ridge_full.score(X_mm_scaled_val, y_val)\n",
    "print(f'The R-squared score for lr_model_ridge_full validation data is: {lr_model_ridge_full_val_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "d2e9b2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain MAE --- 0.6222814824362091\n",
      "Val MAE ---- 0.711038269821763\n"
     ]
    }
   ],
   "source": [
    "y_pred_remain_fridge = lr_model_ridge_full.predict(X_mm_scaled_remain)\n",
    "y_pred_val_fridge = lr_model_ridge_full.predict(X_mm_scaled_val)\n",
    "\n",
    "print(f'Remain MAE --- {mean_absolute_error(y_remain, y_pred_remain_fridge)}')\n",
    "ridge_full_val_mae = mean_absolute_error(y_val, y_pred_val_fridge)\n",
    "print(f'Val MAE ---- {ridge_full_val_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaccdb2",
   "metadata": {},
   "source": [
    "**Ridge Regression: Min Max, PCA Dataset**\n",
    "\n",
    "The pipeline below will look for the optimal Ridge Regression model using the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "749def42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our pipeline\n",
    "#the values listed here for vectorization are placeholders\n",
    "linreg = Pipeline(\n",
    "    [\n",
    "        (\"model\", Ridge()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "ef009b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ran the grid search using a series of changes on these paramaters\n",
    "#started with more extreme options for each paramater and then narrowed down to these\n",
    "\n",
    "parameters = {\n",
    "    \"model__alpha\": [3.5], \n",
    "    \"model__solver\": ['sag'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "8ce030db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'model__alpha': 3.5, 'model__solver': 'sag'}\n",
      "Best score: 0.9115825822708554\n"
     ]
    }
   ],
   "source": [
    "#running the serach to find the best combination of these parameters\n",
    "grid_search = GridSearchCV(linreg, parameters)\n",
    "\n",
    "grid_search.fit(X_remain_PCA, y_remain)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "#checking the R2\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262eb2dd",
   "metadata": {},
   "source": [
    "Ridge regression on the PCA data does slightly less well than the full dataset. The best parameters for it are shown above. Below will see how these do with the remain and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "d6731068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared score for lr_model_ridge_pca training data is: 0.9233023969135314\n",
      "The R-squared score for lr_model_ridge_pca validation data is: 0.9018803799615822\n"
     ]
    }
   ],
   "source": [
    "#instatiate and fit best model\n",
    "lr_model_ridge_pca = Ridge(alpha=3.5, solver='sag')\n",
    "lr_model_ridge_pca.fit(X_remain_PCA, y_remain)\n",
    "\n",
    "# 3. Scoring the models\n",
    "print(f'The R-squared score for lr_model_ridge_pca training data is: {lr_model_ridge_pca.score(X_remain_PCA, y_remain)}')\n",
    "\n",
    "lr_model_ridge_pca_val_r2 = lr_model_ridge_pca.score(X_val_PCA, y_val)\n",
    "print(f'The R-squared score for lr_model_ridge_pca validation data is: {lr_model_ridge_pca_val_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "ca9388c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain MAE --- 0.7340251379125515\n",
      "Val MAE ---- 0.7551809636224615\n"
     ]
    }
   ],
   "source": [
    "y_pred_remain_rpca = lr_model_ridge_pca.predict(X_remain_PCA)\n",
    "y_pred_val_rpca = lr_model_ridge_pca.predict(X_val_PCA)\n",
    "\n",
    "print(f'Remain MAE --- {mean_absolute_error(y_remain, y_pred_remain_rpca)}')\n",
    "ridge_pca_val_mae = mean_absolute_error(y_val, y_pred_val_rpca)\n",
    "print(f'Val MAE ---- {ridge_pca_val_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376d1c2",
   "metadata": {},
   "source": [
    "**Lasso Regression: Min Max, Full Dataset**\n",
    "\n",
    "The pipeline below will look for the optimal Lasso Regression model using the full datset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "16b58c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our pipeline\n",
    "#the values listed here for vectorization are placeholders\n",
    "linreg = Pipeline(\n",
    "    [\n",
    "        (\"model\", Lasso()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "65f76f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ran the grid search using a series of changes on these paramaters\n",
    "#started with more extreme options for each paramater and then narrowed down to these\n",
    "\n",
    "parameters = {\n",
    "    \"model__alpha\": [0.002], \n",
    "    \"model__positive\": [False],\n",
    "    \"model__warm_start\": [True]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "abb37dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'model__alpha': 0.002, 'model__positive': False, 'model__warm_start': True}\n",
      "Best score: 0.9202131414122207\n"
     ]
    }
   ],
   "source": [
    "#running the serach to find the best combination of these parameters\n",
    "grid_search = GridSearchCV(linreg, parameters)\n",
    "\n",
    "grid_search.fit(X_mm_scaled_remain, y_remain)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "#checking the R2\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa1ae01",
   "metadata": {},
   "source": [
    "The optimal Lasso Regression model uses parameters are shown above. Below will see how the model does with training and validation data using these optimized paramters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "6552a51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score for lr_model_lasso_full training data is: 0.9338342966150462\n",
      "The R2 score for lr_model_lasso_full validation data is: 0.9044889880368788\n"
     ]
    }
   ],
   "source": [
    "#instatiate and fit best model\n",
    "lr_model_lasso_full = Lasso(alpha=0.002, positive=False, warm_start=True)\n",
    "lr_model_lasso_full.fit(X_mm_scaled_remain, y_remain)\n",
    "\n",
    "# 3. Scoring the models\n",
    "print(f'The R2 score for lr_model_lasso_full training data is: {lr_model_lasso_full.score(X_mm_scaled_remain, y_remain)}')\n",
    "\n",
    "lr_model_lasso_full_val_r2 = lr_model_lasso_full.score(X_mm_scaled_val, y_val)\n",
    "print(f'The R2 score for lr_model_lasso_full validation data is: {lr_model_lasso_full_val_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "03e5926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain MAE --- 0.6284917012449459\n",
      "Val MAE ---- 0.674027287060549\n"
     ]
    }
   ],
   "source": [
    "y_pred_remain_flasso = lr_model_lasso_full.predict(X_mm_scaled_remain)\n",
    "y_pred_val_flasso = lr_model_lasso_full.predict(X_mm_scaled_val)\n",
    "\n",
    "print(f'Remain MAE --- {mean_absolute_error(y_remain, y_pred_remain_flasso)}')\n",
    "lasso_full_val_mae = mean_absolute_error(y_val, y_pred_val_flasso)\n",
    "print(f'Val MAE ---- {lasso_full_val_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db305a92",
   "metadata": {},
   "source": [
    "**Lasso Regression: Min Max, PCA Dataset**\n",
    "\n",
    "The pipeline below will look for the optimal Lasso Regression model using the PCA datset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "35f3fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our pipeline\n",
    "#the values listed here for vectorization are placeholders\n",
    "linreg = Pipeline(\n",
    "    [\n",
    "        (\"model\", Lasso()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "667b7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ran the grid search using a series of changes on these paramaters\n",
    "#started with more extreme options for each paramater and then narrowed down to these\n",
    "\n",
    "parameters = {\n",
    "    \"model__alpha\": [0.002], \n",
    "    \"model__positive\": [False],\n",
    "    \"model__warm_start\": [True]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "e1f7aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'model__alpha': 0.002, 'model__positive': False, 'model__warm_start': True}\n",
      "Best score: 0.9079396494677925\n"
     ]
    }
   ],
   "source": [
    "#running the serach to find the best combination of these parameters\n",
    "grid_search = GridSearchCV(linreg, parameters)\n",
    "\n",
    "grid_search.fit(X_remain_PCA, y_remain)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "#checking the R2\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "4a855433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score for lr_model_lasso_pca training data is: 0.9179972769726281\n",
      "The R2 score for lr_model_lasso_pca validation data is: 0.8990877606314609\n"
     ]
    }
   ],
   "source": [
    "#instatiate and fit best model\n",
    "lr_model_lasso_pca = Lasso(alpha=0.002, positive=False, warm_start=True)\n",
    "lr_model_lasso_pca.fit(X_remain_PCA, y_remain)\n",
    "\n",
    "# 3. Scoring the models\n",
    "print(f'The R2 score for lr_model_lasso_pca training data is: {lr_model_lasso_pca.score(X_remain_PCA, y_remain)}')\n",
    "\n",
    "lr_model_lasso_pca_val_r2 = lr_model_lasso_pca.score(X_val_PCA, y_val)\n",
    "print(f'The R2 score for lr_model_lasso_pca validation data is: {lr_model_lasso_pca_val_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "ef05d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain MAE --- 0.7609578078947562\n",
      "Val MAE ---- 0.7729746779391995\n"
     ]
    }
   ],
   "source": [
    "y_pred_remain_lpca = lr_model_lasso_pca.predict(X_remain_PCA)\n",
    "y_pred_val_lpca = lr_model_lasso_pca.predict(X_val_PCA)\n",
    "\n",
    "print(f'Remain MAE --- {mean_absolute_error(y_remain, y_pred_remain_lpca)}')\n",
    "lasso_pca_val_mae = mean_absolute_error(y_val, y_pred_val_lpca)\n",
    "print(f'Val MAE ---- {lasso_pca_val_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdde2e83",
   "metadata": {},
   "source": [
    "**ElasticNet Regression: Min Max, Full Dataset**\n",
    "\n",
    "The pipeline below will look for the optimal ElasticNet Regression model on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "30728ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our pipeline\n",
    "#the values listed here for vectorization are placeholders\n",
    "linreg = Pipeline(\n",
    "    [\n",
    "        (\"model\", ElasticNet()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "c0a46861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ran the grid search using a series of changes on these paramaters\n",
    "#started with more extreme options for each paramater and then narrowed down to these\n",
    "\n",
    "parameters = {\n",
    "    \"model__alpha\": [0.002], \n",
    "    \"model__l1_ratio\": [0.4],\n",
    "    \"model__warm_start\": [True],\n",
    "    \"model__positive\": [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "a7ffda66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'model__alpha': 0.002, 'model__l1_ratio': 0.4, 'model__positive': False, 'model__warm_start': True}\n",
      "Best score: 0.9222724226028243\n"
     ]
    }
   ],
   "source": [
    "#running the serach to find the best combination of these parameters\n",
    "grid_search = GridSearchCV(linreg, parameters)\n",
    "\n",
    "grid_search.fit(X_mm_scaled_remain, y_remain)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "#checking the R2\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc500db",
   "metadata": {},
   "source": [
    "Below will see how the optimized model does with the training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "3f10a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score for lr_model_elastic_full training data is: 0.9389272735735228\n",
      "The R2 score for lr_model_elastic_full validation data is: 0.9061706254817619\n"
     ]
    }
   ],
   "source": [
    "#instatiate and fit best model\n",
    "lr_model_elastic_full = ElasticNet(alpha=0.002, l1_ratio=0.4, positive=False, warm_start=True)\n",
    "lr_model_elastic_full.fit(X_mm_scaled_remain, y_remain)\n",
    "\n",
    "# 3. Scoring the models\n",
    "print(f'The R2 score for lr_model_elastic_full training data is: {lr_model_elastic_full.score(X_mm_scaled_remain, y_remain)}')\n",
    "\n",
    "lr_model_elastic_full_val_r2 = lr_model_elastic_full.score(X_mm_scaled_val, y_val)\n",
    "print(f'The R2 score for lr_model_elastic_full validation data is: {lr_model_elastic_full_val_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "d0f9c2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain MAE --- 0.6241567967911439\n",
      "Val MAE ---- 0.6845215802050642\n"
     ]
    }
   ],
   "source": [
    "y_pred_remain_fen = lr_model_elastic_full.predict(X_mm_scaled_remain)\n",
    "y_pred_val_fen = lr_model_elastic_full.predict(X_mm_scaled_val)\n",
    "\n",
    "print(f'Remain MAE --- {mean_absolute_error(y_remain, y_pred_remain_fen)}')\n",
    "elastic_full_val_mae = mean_absolute_error(y_val, y_pred_val_fen)\n",
    "print(f'Val MAE ---- {elastic_full_val_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e101e491",
   "metadata": {},
   "source": [
    "**ElasticeNet Regression: Min Max, PCA Dataset**\n",
    "\n",
    "The pipeline below will look for the optimal ElasticNet Regression model on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "46417187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our pipeline\n",
    "#the values listed here for vectorization are placeholders\n",
    "linreg = Pipeline(\n",
    "    [\n",
    "        (\"model\", ElasticNet()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "377b9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ran the grid search using a series of changes on these paramaters\n",
    "#started with more extreme options for each paramater and then narrowed down to these\n",
    "\n",
    "parameters = {\n",
    "    \"model__alpha\": [0.001], \n",
    "    \"model__l1_ratio\": [0.2],\n",
    "    \"model__warm_start\": [True],\n",
    "    \"model__positive\": [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "0e43c03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'model__alpha': 0.001, 'model__l1_ratio': 0.2, 'model__positive': False, 'model__warm_start': True}\n",
      "Best score: 0.9115883070556094\n"
     ]
    }
   ],
   "source": [
    "#running the serach to find the best combination of these parameters\n",
    "grid_search = GridSearchCV(linreg, parameters)\n",
    "\n",
    "grid_search.fit(X_remain_PCA, y_remain)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "#checking the R2\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "02763e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared score for lr_model_elastic_pca training data is: 0.9201445429364915\n",
      "The R-squared score for lr_model_elastic_pca validation data is: 0.9023188582292716\n"
     ]
    }
   ],
   "source": [
    "#instatiate and fit best model\n",
    "lr_model_elastic_pca = ElasticNet(alpha=0.002, l1_ratio=0.4, positive=False, warm_start=True)\n",
    "lr_model_elastic_pca.fit(X_remain_PCA, y_remain)\n",
    "\n",
    "# 3. Scoring the models\n",
    "print(f'The R-squared score for lr_model_elastic_pca training data is: {lr_model_elastic_pca.score(X_remain_PCA, y_remain)}')\n",
    "\n",
    "lr_model_elastic_pca_val_r2 = lr_model_elastic_pca.score(X_val_PCA, y_val)\n",
    "print(f'The R-squared score for lr_model_elastic_pca validation data is: {lr_model_elastic_pca_val_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "3b12cd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain MAE --- 0.7432943234409503\n",
      "Val MAE ---- 0.7446043094290764\n"
     ]
    }
   ],
   "source": [
    "y_pred_remain_enpca = lr_model_elastic_pca.predict(X_remain_PCA)\n",
    "y_pred_val_enpca = lr_model_elastic_pca.predict(X_val_PCA)\n",
    "\n",
    "print(f'Remain MAE --- {mean_absolute_error(y_remain, y_pred_remain_enpca)}')\n",
    "elastic_pca_val_mae = mean_absolute_error(y_val, y_pred_val_enpca)\n",
    "print(f'Val MAE ---- {elastic_pca_val_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4353908",
   "metadata": {},
   "source": [
    "**Looking at the Effects of Regularization: $R^2$**\n",
    "\n",
    "Below will compare $R^2$ values calculated on the validation data to see how these different regression models perform. A higher $R^2$ means we can explain a higher proportion the target variable (`overall_score`) using the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "ae9fe1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing the models R2\n",
    "R2_dictionary = {'Linear Full R2': lr_model_full_val_r2, 'Linear PCA R2' :lr_model_pca_val_r2, 'Ridge Full R2': lr_model_ridge_full_val_r2, 'Ridge PCA R2': lr_model_ridge_pca_val_r2, 'Lasso Full R2': lr_model_lasso_full_val_r2, 'Lasso PCA R2': lr_model_lasso_pca_val_r2, 'Elastic Full R2': lr_model_elastic_full_val_r2, 'Elastic PCA R2': lr_model_elastic_pca_val_r2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "efc5911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting scores\n",
    "R2_values_sorted = dict(sorted(R2_dictionary.items(), key = operator.itemgetter(1), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "af761a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Elastic Full R2': 0.9061706254817619,\n",
       " 'Ridge Full R2': 0.9054632001428237,\n",
       " 'Lasso Full R2': 0.9044889880368788,\n",
       " 'Elastic PCA R2': 0.9023188582292716,\n",
       " 'Ridge PCA R2': 0.9018803799615822,\n",
       " 'Lasso PCA R2': 0.8990877606314609,\n",
       " 'Linear Full R2': 0.8982269625899719,\n",
       " 'Linear PCA R2': 0.8981515772813482}"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_values_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898eff38",
   "metadata": {},
   "source": [
    "**Looking at the Effects of Regularization: MAE**\n",
    "\n",
    "In addition to $R^2$, we also want to get a sense of predictive accuracy. To do so, we'll use Mean Absolute Error (MAE). From CanWorkSmart \"MAE is simply, as the name suggests, the mean of the absolute errors. The absolute error is the absolute value of the difference between the forecasted value and the actual value. MAE tells us how big of an error we can expect from the forecast on average.\" When it comes to MAE, the closer the value to 0, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "74236c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing the models R2\n",
    "MAE_dictionary = {'Linear Full MAE': lr_full_val_mae, 'Linear PCA MAE' :lr_pca_val_mae, 'Ridge Full MAE': ridge_full_val_mae, 'Ridge PCA MAE': ridge_pca_val_mae, 'Lasso Full MAE': lasso_full_val_mae, 'Lasso PCA MAE': lasso_pca_val_mae, 'Elastic Full MAE': elastic_full_val_mae, 'Elastic PCA MAE': elastic_pca_val_mae} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "1e8a1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting MAE\n",
    "MAE_values_sorted = dict(sorted(MAE_dictionary.items(), key = operator.itemgetter(1), reverse=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "dee165da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lasso Full MAE': 0.674027287060549,\n",
       " 'Elastic Full MAE': 0.6845215802050642,\n",
       " 'Ridge Full MAE': 0.711038269821763,\n",
       " 'Elastic PCA MAE': 0.7446043094290764,\n",
       " 'Ridge PCA MAE': 0.7551809636224615,\n",
       " 'Linear Full MAE': 0.7724205905836047,\n",
       " 'Lasso PCA MAE': 0.7729746779391995,\n",
       " 'Linear PCA MAE': 0.7863830861370323}"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE_values_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931425a9",
   "metadata": {},
   "source": [
    "**Comparing Linear Model Performance**\n",
    "\n",
    "In general, using the full dataset provided better results than PCA, which is good because interpretability can be preserved then. \n",
    "\n",
    "In terms of $R^2$ score, ElasticNet does best, but the other models are all very close. \n",
    "\n",
    "We see more difference when it comes to MAE; Lasso performs best but Elastic and Ridge are both pretty close. In general, the MAE values are quite low, indicating strong accuracy for the models.\n",
    "\n",
    "All things considered, the models perform pretty similarly. Some regularization does improve the scores slightly. \n",
    "\n",
    "Below will look at another model type (XGBoost Regressor) before doing further interpretation of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4fdea0",
   "metadata": {},
   "source": [
    "### 5.2 XG Boost Regressor <a class=\"anchor\" id=\"subheader52\"></a>\n",
    "\n",
    "For reference, the best validation $R^2$ from running the baseline XGBoost Regressor model on numeric data alone (no vectorized text) was about 91.8. XG Boost was not run on the text only data.\n",
    "\n",
    "**XG Boost Regressor: Min Max, Full Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "4388ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score for XGBR_model training data is: 0.9938284219293739\n",
      "The R2 score for XGBR_model validation data is: 0.8942740887276298\n"
     ]
    }
   ],
   "source": [
    "# 1. Instantiate the model\n",
    "XGBR_model = XGBRegressor()\n",
    "\n",
    "# 2. Fit the model\n",
    "XGBR_model.fit(X_mm_scaled_remain, y_remain)\n",
    "\n",
    "# 3. Scoring the models\n",
    "print(f'The R2 score for XGBR_model training data is: {XGBR_model.score(X_mm_scaled_remain, y_remain)}')\n",
    "\n",
    "XGBR_model_val_r2 = XGBR_model.score(X_mm_scaled_val, y_val)\n",
    "print(f'The R2 score for XGBR_model validation data is: {XGBR_model_val_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d048a2",
   "metadata": {},
   "source": [
    "Optimizing the XG Boost Model using the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "5ca21d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our pipeline\n",
    "#the values listed here for vectorization are placeholders\n",
    "linreg = Pipeline(\n",
    "    [\n",
    "        (\"model\", XGBRegressor()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "9a6a9676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ran the grid search using a series of changes on these paramaters\n",
    "#started with more extreme options for each paramater and then narrowed down to these\n",
    "\n",
    "parameters = {\n",
    "    \"model__booster\": ['dart'], \n",
    "    \"model__eta\": [0.1],\n",
    "    \"model__gamma\": [1],\n",
    "    \"model__max_depth\": [11]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "22165e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'model__booster': 'dart', 'model__eta': 0.1, 'model__gamma': 1, 'model__max_depth': 11}\n",
      "Best score: 0.9211296710309472\n"
     ]
    }
   ],
   "source": [
    "#running the serach to find the best combination of these parameters\n",
    "grid_search = GridSearchCV(linreg, parameters)\n",
    "\n",
    "grid_search.fit(X_mm_scaled_remain, y_remain)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "#checking the R2\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "ae500970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared score for XGBR_model_full training data is: 0.992155753607633\n",
      "The R-squared score for XGBR_model_full validation data is: 0.9020025848412191\n"
     ]
    }
   ],
   "source": [
    "#instatiate and fit best model\n",
    "XGBR_model_full = XGBRegressor(booster='dart', eta=0.1, gamma=1, max_depth=11)\n",
    "XGBR_model_full.fit(X_mm_scaled_remain, y_remain)\n",
    "\n",
    "# 3. Scoring the models\n",
    "print(f'The R-squared score for XGBR_model_full training data is: {XGBR_model_full.score(X_mm_scaled_remain, y_remain)}')\n",
    "\n",
    "XGBR_model_full_val_r2 = XGBR_model_full.score(X_mm_scaled_val, y_val)\n",
    "print(f'The R-squared score for XGBR_model_full validation data is: {XGBR_model_full_val_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cedf429",
   "metadata": {},
   "source": [
    "The optimized XG Boost Model is the most overfit. \n",
    "\n",
    "Let's see how it does with MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "ace0e5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain MAE --- 0.2230198659610339\n",
      "Val MAE ---- 0.5929854313911315\n"
     ]
    }
   ],
   "source": [
    "y_pred_remain_fxgb = XGBR_model_full.predict(X_mm_scaled_remain)\n",
    "y_pred_val_fxgb = XGBR_model_full.predict(X_mm_scaled_val)\n",
    "\n",
    "print(f'Remain MAE --- {mean_absolute_error(y_remain, y_pred_remain_fxgb)}')\n",
    "xgbr_full_val_mae = mean_absolute_error(y_val, y_pred_val_fxgb)\n",
    "print(f'Val MAE ---- {xgbr_full_val_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf79c9c",
   "metadata": {},
   "source": [
    "XGBoost on the full data set has the lowest MAE so far.\n",
    "\n",
    "Fitting the XGBoost Regressor model on the PCA data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "1d7471e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our pipeline\n",
    "#the values listed here for vectorization are placeholders\n",
    "linreg = Pipeline(\n",
    "    [\n",
    "        (\"model\", XGBRegressor()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "1f7b665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ran the grid search using a series of changes on these paramaters\n",
    "#started with more extreme options for each paramater and then narrowed down to these\n",
    "\n",
    "parameters = {\n",
    "    \"model__booster\": ['dart'], \n",
    "    \"model__eta\": [0.1],\n",
    "    \"model__gamma\": [1],\n",
    "    \"model__max_depth\": [5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "c632a90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'model__booster': 'dart', 'model__eta': 0.1, 'model__gamma': 1, 'model__max_depth': 5}\n",
      "Best score: 0.8411728005938542\n"
     ]
    }
   ],
   "source": [
    "#running the serach to find the best combination of these parameters\n",
    "grid_search = GridSearchCV(linreg, parameters)\n",
    "\n",
    "grid_search.fit(X_remain_PCA, y_remain)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "#checking the R2\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "2c8c36ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared score for XGBR_model_pca training data is: 0.9694876680740316\n",
      "The R-squared score for XGBR_model_pca validation data is: 0.8355560074862549\n"
     ]
    }
   ],
   "source": [
    "#instatiate and fit best model\n",
    "XGBR_model_pca = XGBRegressor(booster='dart', eta=0.1, gamma=1, max_depth=5)\n",
    "XGBR_model_pca.fit(X_remain_PCA, y_remain)\n",
    "\n",
    "# 3. Scoring the models\n",
    "print(f'The R-squared score for XGBR_model_pca training data is: {XGBR_model_pca.score(X_remain_PCA, y_remain)}')\n",
    "\n",
    "XGBR_model_pca_val_r2 = XGBR_model_pca.score(X_val_PCA, y_val)\n",
    "print(f'The R-squared score for XGBR_model_pca validation data is: {XGBR_model_pca_val_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "a9429155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain MAE --- 0.5398887457822582\n",
      "Val MAE ---- 1.142429428173316\n"
     ]
    }
   ],
   "source": [
    "y_pred_remain_xgbpca = XGBR_model_pca.predict(X_remain_PCA)\n",
    "y_pred_val_xgbpca = XGBR_model_pca.predict(X_val_PCA)\n",
    "\n",
    "print(f'Remain MAE --- {mean_absolute_error(y_remain, y_pred_remain_xgbpca)}')\n",
    "xgbr_pca_val_mae = mean_absolute_error(y_val, y_pred_val_xgbpca)\n",
    "print(f'Val MAE ---- {xgbr_pca_val_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7809e",
   "metadata": {},
   "source": [
    "## 6. Selecting the Final Model <a class=\"anchor\" id=\"header6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484c017",
   "metadata": {},
   "source": [
    "### 6.1 Comparing the Models <a class=\"anchor\" id=\"header61\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d9a2e",
   "metadata": {},
   "source": [
    "So which model is best? To compare, we'll look at the $R^2$ value and the MAE for each.\n",
    "\n",
    "The following puts the validation $R^2$ scores into a dictionary so that scores can be sorted and compared. The higher the $R^2$ score, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "4caa4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing the models\n",
    "R2_dictionary = {'Linear Full R2': lr_model_full_val_r2, 'Linear PCA R2' :lr_model_pca_val_r2, 'Ridge Full R2': lr_model_ridge_full_val_r2, 'Ridge PCA R2': lr_model_ridge_pca_val_r2, 'Lasso Full R2': lr_model_lasso_full_val_r2, 'Lasso PCA R2': lr_model_lasso_pca_val_r2, 'Elastic Full R2': lr_model_elastic_full_val_r2, 'Elastic PCA R2': lr_model_elastic_pca_val_r2, 'XGBR Full R2': XGBR_model_full_val_r2, 'XGBR PCA R2': XGBR_model_pca_val_r2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "d236fa21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Elastic Full R2': 0.9061706254817619,\n",
       " 'Ridge Full R2': 0.9054632001428237,\n",
       " 'Lasso Full R2': 0.9044889880368788,\n",
       " 'Elastic PCA R2': 0.9023188582292716,\n",
       " 'XGBR Full R2': 0.9020025848412191,\n",
       " 'Ridge PCA R2': 0.9018803799615822,\n",
       " 'Lasso PCA R2': 0.8990877606314609,\n",
       " 'Linear Full R2': 0.8982269625899719,\n",
       " 'Linear PCA R2': 0.8981515772813482,\n",
       " 'XGBR PCA R2': 0.8355560074862549}"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting scores\n",
    "R2_values_sorted = dict(sorted(R2_dictionary.items(), key = operator.itemgetter(1), reverse=True))\n",
    "R2_values_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07893e39",
   "metadata": {},
   "source": [
    "Let's also look at the Mean Absolute Errors for the models too. The lower the MAE, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "156c96f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dictionary for the models MAE\n",
    "MAE_dictionary = {'Linear Full MAE': lr_full_val_mae, 'Linear PCA MAE' :lr_pca_val_mae, 'Ridge Full MAE': ridge_full_val_mae, 'Ridge PCA MAE': ridge_pca_val_mae, 'Lasso Full MAE': lasso_full_val_mae, 'Lasso PCA MAE': lasso_pca_val_mae, 'Elastic Full MAE': elastic_full_val_mae, 'Elastic PCA MAE': elastic_pca_val_mae, 'XGBR Full MAE': xgbr_full_val_mae, 'XGBR PCA MAE': xgbr_pca_val_mae} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "47b432f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'XGBR Full MAE': 0.5929854313911315,\n",
       " 'Lasso Full MAE': 0.674027287060549,\n",
       " 'Elastic Full MAE': 0.6845215802050642,\n",
       " 'Ridge Full MAE': 0.711038269821763,\n",
       " 'Elastic PCA MAE': 0.7446043094290764,\n",
       " 'Ridge PCA MAE': 0.7551809636224615,\n",
       " 'Linear Full MAE': 0.7724205905836047,\n",
       " 'Lasso PCA MAE': 0.7729746779391995,\n",
       " 'Linear PCA MAE': 0.7863830861370323,\n",
       " 'XGBR PCA MAE': 1.142429428173316}"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting MAE\n",
    "MAE_values_sorted = dict(sorted(MAE_dictionary.items(), key = operator.itemgetter(1), reverse=False))\n",
    "MAE_values_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c500fe",
   "metadata": {},
   "source": [
    "Under this particular remain/val/test split, ElasticNet on the full dataset has the highest $R^2$, while XGBR has the lowest MAE.\n",
    "\n",
    "Overall, the PCA versions perform worse. XGBoost Regressor in particular takes a peformance hit when using the PCA data. Across the board, model performance is pretty similar, with the exception of the XGBR PCA version. \n",
    "\n",
    "To pick the final model, we'll consider a few factors:\n",
    "- $R^2$\n",
    "- MAE\n",
    "- Interpretability of the model type\n",
    "\n",
    "Given how close most of the $R^2$ scores are, we'll give more weight to the MAE results. This puts XGBR as the top model. However, XGBR is much more challenging to interpret, and the performance gain is not huge. Therefore, we'll pick one of the linear regression models. Since lasso has the lowest MAE and is very close to Elastic on its $R^2$, we'll continue with Lasso as the final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9da1e1",
   "metadata": {},
   "source": [
    "### 6.2 Testing the Final Model <a class=\"anchor\" id=\"subheader61\"></a>\n",
    "\n",
    "Now that we've selected the final model, let's run the test data on it to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "b3115b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score for lr_model_lasso_full training data is: 0.9338342966150462\n",
      "The R2 score for lr_model_lasso_full validation data is: 0.9044889880368788\n",
      "The R2 score for lr_model_lasso_full validation data is: 0.9101456150330729\n"
     ]
    }
   ],
   "source": [
    "print(f'The R2 score for lr_model_lasso_full training data is: {lr_model_lasso_full.score(X_mm_scaled_remain, y_remain)}')\n",
    "\n",
    "lr_model_lasso_full_val_r2 = lr_model_lasso_full.score(X_mm_scaled_val, y_val)\n",
    "print(f'The R2 score for lr_model_lasso_full validation data is: {lr_model_lasso_full_val_r2}')\n",
    "\n",
    "lr_model_lasso_full_test_r2 = lr_model_lasso_full.score(X_mm_scaled_test, y_test)\n",
    "print(f'The R2 score for lr_model_lasso_full validation data is: {lr_model_lasso_full_test_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122fcd52",
   "metadata": {},
   "source": [
    "The model is slightly overfitted but not bad. The test data actually does a little better than the validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "4bef63dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain MAE --- 0.6284917012449459\n",
      "Val MAE ---- 0.674027287060549\n",
      "Test MAE ---- 0.6914886653585588\n"
     ]
    }
   ],
   "source": [
    "y_pred_remain_flasso = lr_model_lasso_full.predict(X_mm_scaled_remain)\n",
    "y_pred_val_flasso = lr_model_lasso_full.predict(X_mm_scaled_val)\n",
    "y_pred_test_flasso = lr_model_lasso_full.predict(X_mm_scaled_test)\n",
    "\n",
    "print(f'Remain MAE --- {mean_absolute_error(y_remain, y_pred_remain_flasso)}')\n",
    "\n",
    "lasso_full_val_mae = mean_absolute_error(y_val, y_pred_val_flasso)\n",
    "print(f'Val MAE ---- {lasso_full_val_mae}')\n",
    "\n",
    "lasso_full_test_mae = mean_absolute_error(y_test, y_pred_test_flasso)\n",
    "print(f'Test MAE ---- {lasso_full_test_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259ba97",
   "metadata": {},
   "source": [
    "The Means Absolute Errors for the model are all under 1, indicidating that the predicted value is on average within a point of the actual value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374839d",
   "metadata": {},
   "source": [
    "## 7. Model Interpretation <a class=\"anchor\" id=\"header7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f72475f",
   "metadata": {},
   "source": [
    "Now that we've examined the amount of variance the model accounts for, as well as accuracy, which features are important? What take-aways can we glean from the data? \n",
    "\n",
    "To answer this question, below will look at the model coefficients and p-values.\n",
    "\n",
    "### 7.1 Using Stats Models for Additional Insights <a class=\"anchor\" id=\"subheader71\"></a>\n",
    "\n",
    "Unfortunately, SKLearn doesn't offer an easy way to way to get the p-values for the model. Instead, we'll use stats models to check on p-values. If the p-values shown using the stats model are significant, we can feel more confident in our linear regression models. So, we'll start with checking in using Stats Models, and then check the coefficients generated by the final Lasso Regression model.\n",
    "\n",
    "With stats models, we do have to add a constant to X before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "128b03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the constant before modeling\n",
    "X_mm_remain_constant = sm.add_constant(X_mm_scaled_remain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbc05b",
   "metadata": {},
   "source": [
    "Running the model with stats models.\n",
    "\n",
    "The summary gives an informative print out about the model, but there area  lot of variables. Below we'll sort the coefficients and p-values to learn which ones are most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "d1d81166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>overall_score</td>  <th>  R-squared:         </th> <td>   0.948</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.939</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   102.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 05 Nov 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:43:28</td>     <th>  Log-Likelihood:    </th> <td> -5668.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  4194</td>      <th>  AIC:               </th> <td>1.260e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  3563</td>      <th>  BIC:               </th> <td>1.660e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   630</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>   65.5369</td> <td>    0.381</td> <td>  171.805</td> <td> 0.000</td> <td>   64.789</td> <td>   66.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month</th>           <td>    0.0725</td> <td>    0.057</td> <td>    1.266</td> <td> 0.205</td> <td>   -0.040</td> <td>    0.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>year</th>            <td>    0.1574</td> <td>    0.251</td> <td>    0.628</td> <td> 0.530</td> <td>   -0.334</td> <td>    0.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bean_agtron</th>     <td>   -0.4885</td> <td>    0.376</td> <td>   -1.299</td> <td> 0.194</td> <td>   -1.226</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ground_agtron</th>   <td>    1.2105</td> <td>    0.289</td> <td>    4.187</td> <td> 0.000</td> <td>    0.644</td> <td>    1.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>aroma</th>           <td>    8.7599</td> <td>    0.262</td> <td>   33.482</td> <td> 0.000</td> <td>    8.247</td> <td>    9.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acidity</th>         <td>    5.4980</td> <td>    0.336</td> <td>   16.387</td> <td> 0.000</td> <td>    4.840</td> <td>    6.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>body</th>            <td>    4.8438</td> <td>    0.210</td> <td>   23.047</td> <td> 0.000</td> <td>    4.432</td> <td>    5.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>flavor</th>          <td>    5.9368</td> <td>    0.393</td> <td>   15.125</td> <td> 0.000</td> <td>    5.167</td> <td>    6.706</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>aftertaste</th>      <td>    5.6823</td> <td>    0.323</td> <td>   17.607</td> <td> 0.000</td> <td>    5.050</td> <td>    6.315</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>roaster_lat</th>     <td>    0.2605</td> <td>    0.234</td> <td>    1.111</td> <td> 0.267</td> <td>   -0.199</td> <td>    0.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>roaster_lon</th>     <td>    0.0777</td> <td>    0.086</td> <td>    0.904</td> <td> 0.366</td> <td>   -0.091</td> <td>    0.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>origin_lat</th>      <td>   -0.7004</td> <td>    0.159</td> <td>   -4.393</td> <td> 0.000</td> <td>   -1.013</td> <td>   -0.388</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>origin_lon</th>      <td>   -0.1024</td> <td>    0.085</td> <td>   -1.200</td> <td> 0.230</td> <td>   -0.270</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acid</th>            <td>    0.5672</td> <td>    0.506</td> <td>    1.121</td> <td> 0.263</td> <td>   -0.425</td> <td>    1.559</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acidity.1</th>       <td>    0.2161</td> <td>    0.371</td> <td>    0.582</td> <td> 0.560</td> <td>   -0.511</td> <td>    0.944</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acidy</th>           <td>   -0.3506</td> <td>    0.176</td> <td>   -1.990</td> <td> 0.047</td> <td>   -0.696</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>admired</th>         <td>    0.9103</td> <td>    0.201</td> <td>    4.525</td> <td> 0.000</td> <td>    0.516</td> <td>    1.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>aftertaste.1</th>    <td>    0.4226</td> <td>    0.386</td> <td>    1.094</td> <td> 0.274</td> <td>   -0.335</td> <td>    1.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>agave</th>           <td>    0.6171</td> <td>    0.259</td> <td>    2.379</td> <td> 0.017</td> <td>    0.109</td> <td>    1.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>aged</th>            <td>    0.6206</td> <td>    0.498</td> <td>    1.245</td> <td> 0.213</td> <td>   -0.356</td> <td>    1.598</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>agreeable</th>       <td>    0.1411</td> <td>    0.497</td> <td>    0.284</td> <td> 0.777</td> <td>   -0.834</td> <td>    1.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>agreeably</th>       <td>   -0.6409</td> <td>    0.390</td> <td>   -1.644</td> <td> 0.100</td> <td>   -1.405</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alive</th>           <td>    0.9637</td> <td>    0.348</td> <td>    2.773</td> <td> 0.006</td> <td>    0.282</td> <td>    1.645</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>almond</th>          <td>    2.0959</td> <td>    0.339</td> <td>    6.175</td> <td> 0.000</td> <td>    1.430</td> <td>    2.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>amber</th>           <td>    0.1361</td> <td>    0.136</td> <td>    1.003</td> <td> 0.316</td> <td>   -0.130</td> <td>    0.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>amplified</th>       <td>    0.3812</td> <td>    0.319</td> <td>    1.195</td> <td> 0.232</td> <td>   -0.244</td> <td>    1.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>anise</th>           <td>    0.1805</td> <td>    0.285</td> <td>    0.634</td> <td> 0.526</td> <td>   -0.378</td> <td>    0.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>apple</th>           <td>    0.3129</td> <td>    0.318</td> <td>    0.982</td> <td> 0.326</td> <td>   -0.312</td> <td>    0.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>apricot</th>         <td>    0.8872</td> <td>    0.194</td> <td>    4.568</td> <td> 0.000</td> <td>    0.506</td> <td>    1.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>aromatic</th>        <td>    0.2424</td> <td>    0.135</td> <td>    1.795</td> <td> 0.073</td> <td>   -0.022</td> <td>    0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>aromatically</th>    <td>    0.1891</td> <td>    0.189</td> <td>    0.998</td> <td> 0.318</td> <td>   -0.182</td> <td>    0.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>aromatics</th>       <td>    0.4045</td> <td>    0.329</td> <td>    1.231</td> <td> 0.219</td> <td>   -0.240</td> <td>    1.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>astringency</th>     <td>   -0.1693</td> <td>    0.216</td> <td>   -0.782</td> <td> 0.434</td> <td>   -0.593</td> <td>    0.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>astringent</th>      <td>    0.7242</td> <td>    0.382</td> <td>    1.898</td> <td> 0.058</td> <td>   -0.024</td> <td>    1.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>attractive</th>      <td>   -0.8752</td> <td>    0.213</td> <td>   -4.106</td> <td> 0.000</td> <td>   -1.293</td> <td>   -0.457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>authority</th>       <td>   -0.6906</td> <td>    0.160</td> <td>   -4.313</td> <td> 0.000</td> <td>   -1.005</td> <td>   -0.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>b60</th>             <td>   -0.2995</td> <td>    0.265</td> <td>   -1.131</td> <td> 0.258</td> <td>   -0.819</td> <td>    0.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>b70</th>             <td>    0.1910</td> <td>    0.291</td> <td>    0.657</td> <td> 0.512</td> <td>   -0.379</td> <td>    0.762</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>backgrounded</th>    <td>    0.1934</td> <td>    0.805</td> <td>    0.240</td> <td> 0.810</td> <td>   -1.384</td> <td>    1.771</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>baker</th>           <td>    0.0102</td> <td>    0.711</td> <td>    0.014</td> <td> 0.989</td> <td>   -1.384</td> <td>    1.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>baking</th>          <td>   -0.1282</td> <td>    0.183</td> <td>   -0.699</td> <td> 0.484</td> <td>   -0.488</td> <td>    0.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th>         <td>    0.1350</td> <td>    0.168</td> <td>    0.804</td> <td> 0.422</td> <td>   -0.194</td> <td>    0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balanced</th>        <td>    0.1309</td> <td>    0.185</td> <td>    0.708</td> <td> 0.479</td> <td>   -0.232</td> <td>    0.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balancing</th>       <td>    0.8795</td> <td>    0.228</td> <td>    3.857</td> <td> 0.000</td> <td>    0.432</td> <td>    1.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>banana</th>          <td>    0.2875</td> <td>    0.110</td> <td>    2.613</td> <td> 0.009</td> <td>    0.072</td> <td>    0.503</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>barrel</th>          <td>   -0.0677</td> <td>    0.247</td> <td>   -0.274</td> <td> 0.784</td> <td>   -0.553</td> <td>    0.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bay</th>             <td>   -0.2520</td> <td>    0.342</td> <td>   -0.737</td> <td> 0.461</td> <td>   -0.922</td> <td>    0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bean</th>            <td>   -0.7042</td> <td>    0.568</td> <td>   -1.239</td> <td> 0.215</td> <td>   -1.819</td> <td>    0.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bergamot</th>        <td>   -0.2039</td> <td>    0.371</td> <td>   -0.550</td> <td> 0.582</td> <td>   -0.931</td> <td>    0.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>berries</th>         <td>    0.2975</td> <td>    0.246</td> <td>    1.209</td> <td> 0.227</td> <td>   -0.185</td> <td>    0.780</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>berry</th>           <td>    0.3265</td> <td>    0.412</td> <td>    0.792</td> <td> 0.428</td> <td>   -0.482</td> <td>    1.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>best</th>            <td>    1.0060</td> <td>    0.169</td> <td>    5.961</td> <td> 0.000</td> <td>    0.675</td> <td>    1.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>big</th>             <td>    1.4351</td> <td>    0.401</td> <td>    3.583</td> <td> 0.000</td> <td>    0.650</td> <td>    2.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bing</th>            <td>    0.2931</td> <td>    0.196</td> <td>    1.495</td> <td> 0.135</td> <td>   -0.091</td> <td>    0.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bit</th>             <td>    0.0113</td> <td>    0.403</td> <td>    0.028</td> <td> 0.978</td> <td>   -0.779</td> <td>    0.801</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bitter</th>          <td>   -0.3941</td> <td>    0.190</td> <td>   -2.070</td> <td> 0.039</td> <td>   -0.767</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bitterish</th>       <td>   -0.6807</td> <td>    0.295</td> <td>   -2.309</td> <td> 0.021</td> <td>   -1.259</td> <td>   -0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bitterness</th>      <td>   -0.8181</td> <td>    0.427</td> <td>   -1.917</td> <td> 0.055</td> <td>   -1.655</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bittersweet</th>     <td>    1.5487</td> <td>    0.418</td> <td>    3.706</td> <td> 0.000</td> <td>    0.729</td> <td>    2.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>black</th>           <td>   -0.1445</td> <td>    0.161</td> <td>   -0.898</td> <td> 0.369</td> <td>   -0.460</td> <td>    0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>blackberry</th>      <td>    0.3166</td> <td>    0.231</td> <td>    1.370</td> <td> 0.171</td> <td>   -0.137</td> <td>    0.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>blend</th>           <td>    0.3397</td> <td>    0.220</td> <td>    1.541</td> <td> 0.123</td> <td>   -0.092</td> <td>    0.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>blood</th>           <td>   -0.9816</td> <td>    0.246</td> <td>   -3.992</td> <td> 0.000</td> <td>   -1.464</td> <td>   -0.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bloom</th>           <td>    0.0599</td> <td>    0.224</td> <td>    0.267</td> <td> 0.789</td> <td>   -0.380</td> <td>    0.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>blooming</th>        <td>   -0.0288</td> <td>    0.242</td> <td>   -0.119</td> <td> 0.905</td> <td>   -0.504</td> <td>    0.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>blooms</th>          <td>    0.8627</td> <td>    0.396</td> <td>    2.181</td> <td> 0.029</td> <td>    0.087</td> <td>    1.638</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>blossom</th>         <td>   -0.1264</td> <td>    0.230</td> <td>   -0.550</td> <td> 0.583</td> <td>   -0.577</td> <td>    0.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>blueberry</th>       <td>    0.3572</td> <td>    0.211</td> <td>    1.694</td> <td> 0.090</td> <td>   -0.056</td> <td>    0.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bodied</th>          <td>    0.5206</td> <td>    0.221</td> <td>    2.358</td> <td> 0.018</td> <td>    0.088</td> <td>    0.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>body.1</th>          <td>    0.5155</td> <td>    0.172</td> <td>    2.990</td> <td> 0.003</td> <td>    0.178</td> <td>    0.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brandy</th>          <td>    0.1656</td> <td>    0.273</td> <td>    0.606</td> <td> 0.545</td> <td>   -0.370</td> <td>    0.701</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brazil</th>          <td>   -0.1505</td> <td>    0.431</td> <td>   -0.350</td> <td> 0.727</td> <td>   -0.995</td> <td>    0.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brewed</th>          <td>    0.6240</td> <td>    1.192</td> <td>    0.523</td> <td> 0.601</td> <td>   -1.714</td> <td>    2.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brewer</th>          <td>   -0.5035</td> <td>    0.842</td> <td>   -0.598</td> <td> 0.550</td> <td>   -2.154</td> <td>    1.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brewing</th>         <td>   -0.3925</td> <td>    0.963</td> <td>   -0.407</td> <td> 0.684</td> <td>   -2.281</td> <td>    1.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bright</th>          <td>    0.1899</td> <td>    0.128</td> <td>    1.479</td> <td> 0.139</td> <td>   -0.062</td> <td>    0.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brightly</th>        <td>    0.0368</td> <td>    0.185</td> <td>    0.199</td> <td> 0.842</td> <td>   -0.326</td> <td>    0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brightness</th>      <td>   -0.6608</td> <td>    0.445</td> <td>   -1.486</td> <td> 0.137</td> <td>   -1.533</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brisk</th>           <td>   -0.0785</td> <td>    0.146</td> <td>   -0.536</td> <td> 0.592</td> <td>   -0.366</td> <td>    0.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>briskly</th>         <td>    0.2126</td> <td>    0.287</td> <td>    0.740</td> <td> 0.459</td> <td>   -0.351</td> <td>    0.776</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brittle</th>         <td>    0.4083</td> <td>    0.270</td> <td>    1.512</td> <td> 0.131</td> <td>   -0.121</td> <td>    0.938</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>brown</th>           <td>    0.4395</td> <td>    0.645</td> <td>    0.682</td> <td> 0.495</td> <td>   -0.824</td> <td>    1.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>buoyant</th>         <td>    0.0134</td> <td>    0.143</td> <td>    0.094</td> <td> 0.925</td> <td>   -0.266</td> <td>    0.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>buoyantly</th>       <td>    0.2938</td> <td>    0.297</td> <td>    0.990</td> <td> 0.322</td> <td>   -0.288</td> <td>    0.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>burned</th>          <td>   -2.4507</td> <td>    0.349</td> <td>   -7.032</td> <td> 0.000</td> <td>   -3.134</td> <td>   -1.767</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>butter</th>          <td>    0.4298</td> <td>    0.150</td> <td>    2.856</td> <td> 0.004</td> <td>    0.135</td> <td>    0.725</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>butterscotch</th>    <td>    1.1491</td> <td>    0.281</td> <td>    4.083</td> <td> 0.000</td> <td>    0.597</td> <td>    1.701</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>buttery</th>         <td>    0.5200</td> <td>    0.231</td> <td>    2.248</td> <td> 0.025</td> <td>    0.067</td> <td>    0.974</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>byron</th>           <td>    0.2977</td> <td>    0.208</td> <td>    1.430</td> <td> 0.153</td> <td>   -0.110</td> <td>    0.706</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cacao</th>           <td>    1.3772</td> <td>    0.463</td> <td>    2.974</td> <td> 0.003</td> <td>    0.469</td> <td>    2.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>called</th>          <td>    0.2268</td> <td>    0.222</td> <td>    1.020</td> <td> 0.308</td> <td>   -0.209</td> <td>    0.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>candied</th>         <td>    0.3293</td> <td>    0.419</td> <td>    0.786</td> <td> 0.432</td> <td>   -0.492</td> <td>    1.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>candy</th>           <td>    0.1903</td> <td>    0.692</td> <td>    0.275</td> <td> 0.783</td> <td>   -1.167</td> <td>    1.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cane</th>            <td>   -0.3910</td> <td>    0.453</td> <td>   -0.863</td> <td> 0.388</td> <td>   -1.279</td> <td>    0.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cappuccino</th>      <td>    0.6316</td> <td>    0.565</td> <td>    1.118</td> <td> 0.264</td> <td>   -0.476</td> <td>    1.740</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>capsule</th>         <td>    0.3630</td> <td>    0.886</td> <td>    0.410</td> <td> 0.682</td> <td>   -1.374</td> <td>    2.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>caramel</th>         <td>    0.3176</td> <td>    0.162</td> <td>    1.965</td> <td> 0.049</td> <td>    0.001</td> <td>    0.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>caramelly</th>       <td>    0.4721</td> <td>    0.432</td> <td>    1.092</td> <td> 0.275</td> <td>   -0.375</td> <td>    1.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carbon</th>          <td>   -0.1225</td> <td>    0.283</td> <td>   -0.433</td> <td> 0.665</td> <td>   -0.677</td> <td>    0.432</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cardamom</th>        <td>   -1.4934</td> <td>    0.412</td> <td>   -3.622</td> <td> 0.000</td> <td>   -2.302</td> <td>   -0.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carob</th>           <td>    0.0386</td> <td>    0.380</td> <td>    0.101</td> <td> 0.919</td> <td>   -0.707</td> <td>    0.784</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carried</th>         <td>    0.0448</td> <td>    0.343</td> <td>    0.131</td> <td> 0.896</td> <td>   -0.628</td> <td>    0.718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carries</th>         <td>    0.0224</td> <td>    0.303</td> <td>    0.074</td> <td> 0.941</td> <td>   -0.572</td> <td>    0.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carry</th>           <td>    0.1748</td> <td>    0.159</td> <td>    1.097</td> <td> 0.273</td> <td>   -0.138</td> <td>    0.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carrying</th>        <td>    0.1116</td> <td>    0.125</td> <td>    0.894</td> <td> 0.371</td> <td>   -0.133</td> <td>    0.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cashew</th>          <td>    0.4039</td> <td>    0.306</td> <td>    1.319</td> <td> 0.187</td> <td>   -0.196</td> <td>    1.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cedar</th>           <td>    0.0646</td> <td>    0.204</td> <td>    0.316</td> <td> 0.752</td> <td>   -0.336</td> <td>    0.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cedary</th>          <td>    0.1796</td> <td>    0.115</td> <td>    1.555</td> <td> 0.120</td> <td>   -0.047</td> <td>    0.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>center</th>          <td>    0.5215</td> <td>    0.265</td> <td>    1.967</td> <td> 0.049</td> <td>    0.002</td> <td>    1.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>centered</th>        <td>    0.2883</td> <td>    0.417</td> <td>    0.692</td> <td> 0.489</td> <td>   -0.529</td> <td>    1.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>centers</th>         <td>    0.1302</td> <td>    0.227</td> <td>    0.573</td> <td> 0.567</td> <td>   -0.316</td> <td>    0.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>character</th>       <td>   -0.0820</td> <td>    0.135</td> <td>   -0.607</td> <td> 0.544</td> <td>   -0.347</td> <td>    0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>charred</th>         <td>    0.2653</td> <td>    0.195</td> <td>    1.360</td> <td> 0.174</td> <td>   -0.117</td> <td>    0.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cherry</th>          <td>    0.1797</td> <td>    0.372</td> <td>    0.483</td> <td> 0.629</td> <td>   -0.550</td> <td>    0.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cherryish</th>       <td>    0.4863</td> <td>    0.153</td> <td>    3.178</td> <td> 0.001</td> <td>    0.186</td> <td>    0.786</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chocolate</th>       <td>    0.8767</td> <td>    0.253</td> <td>    3.470</td> <td> 0.001</td> <td>    0.381</td> <td>    1.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chocolaty</th>       <td>    0.4638</td> <td>    0.136</td> <td>    3.400</td> <td> 0.001</td> <td>    0.196</td> <td>    0.731</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cinnamon</th>        <td>    0.3492</td> <td>    0.164</td> <td>    2.133</td> <td> 0.033</td> <td>    0.028</td> <td>    0.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cited</th>           <td>    0.3147</td> <td>    0.239</td> <td>    1.316</td> <td> 0.188</td> <td>   -0.154</td> <td>    0.783</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>citrus</th>          <td>    0.0954</td> <td>    0.488</td> <td>    0.196</td> <td> 0.845</td> <td>   -0.861</td> <td>    1.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>citrusy</th>         <td>    0.3247</td> <td>    0.163</td> <td>    1.995</td> <td> 0.046</td> <td>    0.006</td> <td>    0.644</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>classic</th>         <td>    0.2032</td> <td>    0.220</td> <td>    0.925</td> <td> 0.355</td> <td>   -0.228</td> <td>    0.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clean</th>           <td>    0.7038</td> <td>    0.292</td> <td>    2.411</td> <td> 0.016</td> <td>    0.131</td> <td>    1.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cleanly</th>         <td>    0.5443</td> <td>    0.181</td> <td>    3.009</td> <td> 0.003</td> <td>    0.190</td> <td>    0.899</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clear</th>           <td>    0.2540</td> <td>    0.172</td> <td>    1.479</td> <td> 0.139</td> <td>   -0.083</td> <td>    0.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clove</th>           <td>   -0.6064</td> <td>    0.327</td> <td>   -1.855</td> <td> 0.064</td> <td>   -1.247</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cocoa</th>           <td>    0.3844</td> <td>    0.243</td> <td>    1.580</td> <td> 0.114</td> <td>   -0.093</td> <td>    0.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cocoaish</th>        <td>    0.7469</td> <td>    0.153</td> <td>    4.884</td> <td> 0.000</td> <td>    0.447</td> <td>    1.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>coconut</th>         <td>    0.0768</td> <td>    0.211</td> <td>    0.363</td> <td> 0.716</td> <td>   -0.338</td> <td>    0.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>coffee</th>          <td>    0.7628</td> <td>    0.282</td> <td>    2.700</td> <td> 0.007</td> <td>    0.209</td> <td>    1.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>coffees</th>         <td>    0.2911</td> <td>    0.185</td> <td>    1.576</td> <td> 0.115</td> <td>   -0.071</td> <td>    0.653</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cold</th>            <td>   -0.1233</td> <td>    0.373</td> <td>   -0.331</td> <td> 0.741</td> <td>   -0.854</td> <td>    0.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>complete</th>        <td>    0.7757</td> <td>    0.691</td> <td>    1.122</td> <td> 0.262</td> <td>   -0.579</td> <td>    2.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>complex</th>         <td>   -1.3916</td> <td>    0.846</td> <td>   -1.646</td> <td> 0.100</td> <td>   -3.049</td> <td>    0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>complexity</th>      <td>    0.7170</td> <td>    0.215</td> <td>    3.330</td> <td> 0.001</td> <td>    0.295</td> <td>    1.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>complexly</th>       <td>    0.2943</td> <td>    0.125</td> <td>    2.355</td> <td> 0.019</td> <td>    0.049</td> <td>    0.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>complicate</th>      <td>    0.4732</td> <td>    0.216</td> <td>    2.192</td> <td> 0.028</td> <td>    0.050</td> <td>    0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>complicated</th>     <td>    0.3617</td> <td>    0.205</td> <td>    1.766</td> <td> 0.077</td> <td>   -0.040</td> <td>    0.763</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>complicates</th>     <td>   -0.2696</td> <td>    0.373</td> <td>   -0.722</td> <td> 0.470</td> <td>   -1.001</td> <td>    0.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>complicating</th>    <td>   -0.3606</td> <td>    0.155</td> <td>   -2.323</td> <td> 0.020</td> <td>   -0.665</td> <td>   -0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>complication</th>    <td>   -0.0480</td> <td>    0.342</td> <td>   -0.141</td> <td> 0.888</td> <td>   -0.718</td> <td>    0.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>complications</th>   <td>    0.2767</td> <td>    0.284</td> <td>    0.975</td> <td> 0.330</td> <td>   -0.280</td> <td>    0.833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>concord</th>         <td>   -0.3655</td> <td>    0.255</td> <td>   -1.434</td> <td> 0.152</td> <td>   -0.865</td> <td>    0.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>consolidates</th>    <td>    0.0185</td> <td>    0.267</td> <td>    0.069</td> <td> 0.945</td> <td>   -0.505</td> <td>    0.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>consolidating</th>   <td>    0.5403</td> <td>    0.416</td> <td>    1.298</td> <td> 0.195</td> <td>   -0.276</td> <td>    1.357</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>continued</th>       <td>    0.0041</td> <td>    0.098</td> <td>    0.041</td> <td> 0.967</td> <td>   -0.189</td> <td>    0.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>continuing</th>      <td>    0.0540</td> <td>    0.270</td> <td>    0.200</td> <td> 0.841</td> <td>   -0.474</td> <td>    0.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cools</th>           <td>    0.3463</td> <td>    0.171</td> <td>    2.031</td> <td> 0.042</td> <td>    0.012</td> <td>    0.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>creamy</th>          <td>    0.0719</td> <td>    0.215</td> <td>    0.334</td> <td> 0.739</td> <td>   -0.350</td> <td>    0.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>crisp</th>           <td>    0.8470</td> <td>    0.577</td> <td>    1.468</td> <td> 0.142</td> <td>   -0.284</td> <td>    1.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>crisply</th>         <td>    0.0616</td> <td>    0.217</td> <td>    0.284</td> <td> 0.777</td> <td>   -0.364</td> <td>    0.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cupper</th>          <td>   -0.0874</td> <td>    0.147</td> <td>   -0.595</td> <td> 0.552</td> <td>   -0.375</td> <td>    0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cupping</th>         <td>    0.3727</td> <td>    0.112</td> <td>    3.315</td> <td> 0.001</td> <td>    0.152</td> <td>    0.593</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cups</th>            <td>    0.6008</td> <td>    0.234</td> <td>    2.562</td> <td> 0.010</td> <td>    0.141</td> <td>    1.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>currant</th>         <td>    0.1777</td> <td>    0.387</td> <td>    0.459</td> <td> 0.646</td> <td>   -0.581</td> <td>    0.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cut</th>             <td>    1.2475</td> <td>    0.342</td> <td>    3.651</td> <td> 0.000</td> <td>    0.578</td> <td>    1.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dark</th>            <td>   -3.6816</td> <td>    0.443</td> <td>   -8.305</td> <td> 0.000</td> <td>   -4.551</td> <td>   -2.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>date</th>            <td>    0.4110</td> <td>    0.209</td> <td>    1.964</td> <td> 0.050</td> <td>    0.001</td> <td>    0.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deep</th>            <td>   -0.1001</td> <td>    0.217</td> <td>   -0.462</td> <td> 0.644</td> <td>   -0.525</td> <td>    0.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deepen</th>          <td>    0.0149</td> <td>    0.152</td> <td>    0.098</td> <td> 0.922</td> <td>   -0.284</td> <td>    0.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deepening</th>       <td>    0.4422</td> <td>    0.193</td> <td>    2.294</td> <td> 0.022</td> <td>    0.064</td> <td>    0.820</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deepens</th>         <td>    0.3825</td> <td>    0.152</td> <td>    2.514</td> <td> 0.012</td> <td>    0.084</td> <td>    0.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>deeply</th>          <td>    0.0136</td> <td>    0.304</td> <td>    0.045</td> <td> 0.964</td> <td>   -0.582</td> <td>    0.609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>delicate</th>        <td>    0.4511</td> <td>    0.312</td> <td>    1.445</td> <td> 0.149</td> <td>   -0.161</td> <td>    1.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>delicately</th>      <td>    0.7079</td> <td>    0.292</td> <td>    2.427</td> <td> 0.015</td> <td>    0.136</td> <td>    1.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>depth</th>           <td>    0.2614</td> <td>    0.123</td> <td>    2.131</td> <td> 0.033</td> <td>    0.021</td> <td>    0.502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>described</th>       <td>    0.1371</td> <td>    0.159</td> <td>    0.862</td> <td> 0.389</td> <td>   -0.175</td> <td>    0.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>device</th>          <td>    0.2952</td> <td>    0.130</td> <td>    2.265</td> <td> 0.024</td> <td>    0.040</td> <td>    0.551</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dimension</th>       <td>    0.6595</td> <td>    0.243</td> <td>    2.713</td> <td> 0.007</td> <td>    0.183</td> <td>    1.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dimensioned</th>     <td>    1.1463</td> <td>    0.515</td> <td>    2.227</td> <td> 0.026</td> <td>    0.137</td> <td>    2.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>displayed</th>       <td>   -1.1298</td> <td>    1.202</td> <td>   -0.940</td> <td> 0.347</td> <td>   -3.487</td> <td>    1.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>displays</th>        <td>   -0.7590</td> <td>    0.292</td> <td>   -2.595</td> <td> 0.009</td> <td>   -1.332</td> <td>   -0.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>distinct</th>        <td>   -0.8076</td> <td>    0.293</td> <td>   -2.755</td> <td> 0.006</td> <td>   -1.382</td> <td>   -0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>distinctive</th>     <td>    0.3806</td> <td>    0.225</td> <td>    1.693</td> <td> 0.091</td> <td>   -0.060</td> <td>    0.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>distinctly</th>      <td>   -0.1445</td> <td>    0.249</td> <td>   -0.580</td> <td> 0.562</td> <td>   -0.633</td> <td>    0.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dominate</th>        <td>   -0.1813</td> <td>    0.260</td> <td>   -0.698</td> <td> 0.485</td> <td>   -0.690</td> <td>    0.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dominated</th>       <td>   -0.2121</td> <td>    0.177</td> <td>   -1.199</td> <td> 0.231</td> <td>   -0.559</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dominates</th>       <td>   -0.3858</td> <td>    0.196</td> <td>   -1.965</td> <td> 0.049</td> <td>   -0.771</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dominating</th>      <td>   -0.1741</td> <td>    0.199</td> <td>   -0.875</td> <td> 0.382</td> <td>   -0.564</td> <td>    0.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dried</th>           <td>    0.0869</td> <td>    0.301</td> <td>    0.289</td> <td> 0.773</td> <td>   -0.503</td> <td>    0.677</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>drink</th>           <td>    0.0976</td> <td>    0.148</td> <td>    0.660</td> <td> 0.509</td> <td>   -0.192</td> <td>    0.387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>driven</th>          <td>    0.6476</td> <td>    0.827</td> <td>    0.783</td> <td> 0.434</td> <td>   -0.974</td> <td>    2.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dry</th>             <td>    0.0942</td> <td>    0.261</td> <td>    0.361</td> <td> 0.718</td> <td>   -0.417</td> <td>    0.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>drying</th>          <td>   -0.1449</td> <td>    0.141</td> <td>   -1.030</td> <td> 0.303</td> <td>   -0.421</td> <td>    0.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dusk</th>            <td>   -0.3752</td> <td>    0.119</td> <td>   -3.147</td> <td> 0.002</td> <td>   -0.609</td> <td>   -0.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>earth</th>           <td>    0.7918</td> <td>    0.324</td> <td>    2.441</td> <td> 0.015</td> <td>    0.156</td> <td>    1.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>earthy</th>          <td>    0.7676</td> <td>    0.264</td> <td>    2.903</td> <td> 0.004</td> <td>    0.249</td> <td>    1.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>edge</th>            <td>    0.0491</td> <td>    0.339</td> <td>    0.145</td> <td> 0.885</td> <td>   -0.616</td> <td>    0.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>edged</th>           <td>    0.7075</td> <td>    0.223</td> <td>    3.166</td> <td> 0.002</td> <td>    0.269</td> <td>    1.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>effervescent</th>    <td>   -0.0758</td> <td>    0.267</td> <td>   -0.284</td> <td> 0.777</td> <td>   -0.600</td> <td>    0.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>elegant</th>         <td>    0.7131</td> <td>    0.242</td> <td>    2.943</td> <td> 0.003</td> <td>    0.238</td> <td>    1.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>elegantly</th>       <td>    0.1603</td> <td>    0.229</td> <td>    0.702</td> <td> 0.483</td> <td>   -0.288</td> <td>    0.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emerge</th>          <td>    0.0004</td> <td>    0.257</td> <td>    0.002</td> <td> 0.999</td> <td>   -0.504</td> <td>    0.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emerges</th>         <td>   -0.3851</td> <td>    0.260</td> <td>   -1.482</td> <td> 0.138</td> <td>   -0.895</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>engaging</th>        <td>    0.2405</td> <td>    0.267</td> <td>    0.899</td> <td> 0.368</td> <td>   -0.284</td> <td>    0.765</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>enveloped</th>       <td>    0.7972</td> <td>    0.331</td> <td>    2.409</td> <td> 0.016</td> <td>    0.148</td> <td>    1.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>enveloping</th>      <td>    0.3082</td> <td>    0.322</td> <td>    0.957</td> <td> 0.338</td> <td>   -0.323</td> <td>    0.939</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>especially</th>      <td>    0.5926</td> <td>    0.436</td> <td>    1.361</td> <td> 0.174</td> <td>   -0.261</td> <td>    1.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>espresso</th>        <td>    1.6377</td> <td>    0.525</td> <td>    3.120</td> <td> 0.002</td> <td>    0.609</td> <td>    2.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ethan</th>           <td>   -0.0870</td> <td>    0.298</td> <td>   -0.292</td> <td> 0.770</td> <td>   -0.671</td> <td>    0.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>evaluated</th>       <td>   -0.2972</td> <td>    0.324</td> <td>   -0.916</td> <td> 0.360</td> <td>   -0.933</td> <td>    0.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>excellent</th>       <td>    0.3024</td> <td>    0.292</td> <td>    1.036</td> <td> 0.300</td> <td>   -0.270</td> <td>    0.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>exhilarating</th>    <td>    0.4727</td> <td>    0.303</td> <td>    1.559</td> <td> 0.119</td> <td>   -0.122</td> <td>    1.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>exhilaratingly</th>  <td>    0.5222</td> <td>    0.301</td> <td>    1.733</td> <td> 0.083</td> <td>   -0.069</td> <td>    1.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>exotic</th>          <td>    0.9812</td> <td>    0.359</td> <td>    2.733</td> <td> 0.006</td> <td>    0.277</td> <td>    1.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>explicit</th>        <td>    0.1573</td> <td>    0.267</td> <td>    0.590</td> <td> 0.555</td> <td>   -0.366</td> <td>    0.680</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>expressed</th>       <td>    0.4719</td> <td>    0.254</td> <td>    1.855</td> <td> 0.064</td> <td>   -0.027</td> <td>    0.971</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fade</th>            <td>    0.0553</td> <td>    0.262</td> <td>    0.212</td> <td> 0.832</td> <td>   -0.458</td> <td>    0.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fades</th>           <td>    0.1319</td> <td>    0.343</td> <td>    0.385</td> <td> 0.700</td> <td>   -0.540</td> <td>    0.804</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fading</th>          <td>   -1.0680</td> <td>    0.263</td> <td>   -4.055</td> <td> 0.000</td> <td>   -1.584</td> <td>   -0.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>faint</th>           <td>   -0.7892</td> <td>    0.501</td> <td>   -1.575</td> <td> 0.115</td> <td>   -1.772</td> <td>    0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>faintly</th>         <td>   -0.2426</td> <td>    0.242</td> <td>   -1.004</td> <td> 0.315</td> <td>   -0.716</td> <td>    0.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fallen</th>          <td>   -0.2219</td> <td>    0.432</td> <td>   -0.513</td> <td> 0.608</td> <td>   -1.070</td> <td>    0.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>far</th>             <td>   -1.3211</td> <td>    0.276</td> <td>   -4.778</td> <td> 0.000</td> <td>   -1.863</td> <td>   -0.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>felt</th>            <td>   -1.1621</td> <td>    0.368</td> <td>   -3.161</td> <td> 0.002</td> <td>   -1.883</td> <td>   -0.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ferment</th>         <td>   -0.0058</td> <td>    0.394</td> <td>   -0.015</td> <td> 0.988</td> <td>   -0.779</td> <td>    0.767</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fermented</th>       <td>    0.4442</td> <td>    0.311</td> <td>    1.428</td> <td> 0.153</td> <td>   -0.166</td> <td>    1.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fermenty</th>        <td>    0.8275</td> <td>    0.224</td> <td>    3.689</td> <td> 0.000</td> <td>    0.388</td> <td>    1.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fig</th>             <td>    0.0033</td> <td>    0.175</td> <td>    0.019</td> <td> 0.985</td> <td>   -0.340</td> <td>    0.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fine</th>            <td>   -1.6699</td> <td>    0.309</td> <td>   -5.407</td> <td> 0.000</td> <td>   -2.275</td> <td>   -1.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fir</th>             <td>   -0.2438</td> <td>    0.136</td> <td>   -1.799</td> <td> 0.072</td> <td>   -0.510</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>flat</th>            <td>   -0.3780</td> <td>    0.267</td> <td>   -1.414</td> <td> 0.157</td> <td>   -0.902</td> <td>    0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>flavor.1</th>        <td>    0.3737</td> <td>    0.104</td> <td>    3.596</td> <td> 0.000</td> <td>    0.170</td> <td>    0.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>flavors</th>         <td>    0.0476</td> <td>    0.180</td> <td>    0.265</td> <td> 0.791</td> <td>   -0.305</td> <td>    0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floral</th>          <td>    0.8194</td> <td>    0.245</td> <td>    3.345</td> <td> 0.001</td> <td>    0.339</td> <td>    1.300</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>florals</th>         <td>    0.5099</td> <td>    0.667</td> <td>    0.765</td> <td> 0.444</td> <td>   -0.797</td> <td>    1.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>flower</th>          <td>    0.7072</td> <td>    0.142</td> <td>    4.968</td> <td> 0.000</td> <td>    0.428</td> <td>    0.986</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>flowering</th>       <td>    0.3699</td> <td>    0.342</td> <td>    1.082</td> <td> 0.279</td> <td>   -0.300</td> <td>    1.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>flowers</th>         <td>   -0.0435</td> <td>    0.189</td> <td>   -0.230</td> <td> 0.818</td> <td>   -0.415</td> <td>    0.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>followed</th>        <td>    0.6321</td> <td>    0.291</td> <td>    2.174</td> <td> 0.030</td> <td>    0.062</td> <td>    1.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>forward</th>         <td>   -0.0803</td> <td>    0.279</td> <td>   -0.288</td> <td> 0.773</td> <td>   -0.626</td> <td>    0.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fragrant</th>        <td>    0.2637</td> <td>    0.228</td> <td>    1.159</td> <td> 0.247</td> <td>   -0.183</td> <td>    0.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>framed</th>          <td>    1.2677</td> <td>    0.404</td> <td>    3.140</td> <td> 0.002</td> <td>    0.476</td> <td>    2.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>frankincense</th>    <td>    0.1407</td> <td>    0.186</td> <td>    0.755</td> <td> 0.450</td> <td>   -0.224</td> <td>    0.506</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>freesia</th>         <td>    0.3989</td> <td>    0.334</td> <td>    1.193</td> <td> 0.233</td> <td>   -0.257</td> <td>    1.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fresh</th>           <td>    0.5196</td> <td>    0.143</td> <td>    3.634</td> <td> 0.000</td> <td>    0.239</td> <td>    0.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>freshly</th>         <td>    0.2636</td> <td>    0.246</td> <td>    1.074</td> <td> 0.283</td> <td>   -0.218</td> <td>    0.745</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fruit</th>           <td>    0.3648</td> <td>    0.209</td> <td>    1.749</td> <td> 0.080</td> <td>   -0.044</td> <td>    0.774</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fruity</th>          <td>    0.2417</td> <td>    0.235</td> <td>    1.029</td> <td> 0.304</td> <td>   -0.219</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fudge</th>           <td>    0.2339</td> <td>    0.162</td> <td>    1.448</td> <td> 0.148</td> <td>   -0.083</td> <td>    0.551</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gardenia</th>        <td>    0.3278</td> <td>    0.136</td> <td>    2.413</td> <td> 0.016</td> <td>    0.061</td> <td>    0.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gentle</th>          <td>    1.0949</td> <td>    0.315</td> <td>    3.471</td> <td> 0.001</td> <td>    0.477</td> <td>    1.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gently</th>          <td>    0.3037</td> <td>    0.298</td> <td>    1.020</td> <td> 0.308</td> <td>   -0.280</td> <td>    0.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ginger</th>          <td>   -0.3021</td> <td>    0.434</td> <td>   -0.696</td> <td> 0.487</td> <td>   -1.153</td> <td>    0.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goji</th>            <td>   -0.3575</td> <td>    0.243</td> <td>   -1.472</td> <td> 0.141</td> <td>   -0.834</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>good</th>            <td>   -1.2870</td> <td>    0.355</td> <td>   -3.630</td> <td> 0.000</td> <td>   -1.982</td> <td>   -0.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grace</th>           <td>    0.2216</td> <td>    0.250</td> <td>    0.888</td> <td> 0.375</td> <td>   -0.268</td> <td>    0.711</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grape</th>           <td>    0.4394</td> <td>    0.172</td> <td>    2.551</td> <td> 0.011</td> <td>    0.102</td> <td>    0.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grapefruit</th>      <td>    0.7124</td> <td>    0.434</td> <td>    1.642</td> <td> 0.101</td> <td>   -0.138</td> <td>    1.563</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grappa</th>          <td>    0.4943</td> <td>    0.591</td> <td>    0.836</td> <td> 0.403</td> <td>   -0.664</td> <td>    1.653</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grass</th>           <td>    0.6236</td> <td>    0.315</td> <td>    1.982</td> <td> 0.048</td> <td>    0.007</td> <td>    1.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>great</th>           <td>   -0.2827</td> <td>    0.258</td> <td>   -1.096</td> <td> 0.273</td> <td>   -0.788</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>green</th>           <td>    0.4470</td> <td>    0.236</td> <td>    1.892</td> <td> 0.059</td> <td>   -0.016</td> <td>    0.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>guava</th>           <td>   -4.3917</td> <td>    0.411</td> <td>  -10.685</td> <td> 0.000</td> <td>   -5.198</td> <td>   -3.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hard</th>            <td>    0.0302</td> <td>    0.314</td> <td>    0.096</td> <td> 0.923</td> <td>   -0.586</td> <td>    0.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>harmonious</th>      <td>    0.2337</td> <td>    0.148</td> <td>    1.575</td> <td> 0.115</td> <td>   -0.057</td> <td>    0.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hazelnut</th>        <td>    0.5933</td> <td>    0.526</td> <td>    1.127</td> <td> 0.260</td> <td>   -0.439</td> <td>    1.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>heavy</th>           <td>   -0.5592</td> <td>    0.274</td> <td>   -2.038</td> <td> 0.042</td> <td>   -1.097</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>herb</th>            <td>    0.7839</td> <td>    0.306</td> <td>    2.563</td> <td> 0.010</td> <td>    0.184</td> <td>    1.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>herbaceous</th>      <td>    0.0694</td> <td>    0.234</td> <td>    0.297</td> <td> 0.767</td> <td>   -0.389</td> <td>    0.528</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>herbal</th>          <td>   -0.2842</td> <td>    0.304</td> <td>   -0.935</td> <td> 0.350</td> <td>   -0.880</td> <td>    0.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>herbs</th>           <td>   -0.1880</td> <td>    0.319</td> <td>   -0.590</td> <td> 0.555</td> <td>   -0.813</td> <td>    0.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>herby</th>           <td>    0.1025</td> <td>    0.317</td> <td>    0.323</td> <td> 0.747</td> <td>   -0.520</td> <td>    0.725</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hibiscus</th>        <td>    0.7984</td> <td>    0.172</td> <td>    4.653</td> <td> 0.000</td> <td>    0.462</td> <td>    1.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>high</th>            <td>   -0.1069</td> <td>    0.181</td> <td>   -0.592</td> <td> 0.554</td> <td>   -0.461</td> <td>    0.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hint</th>            <td>   -0.0455</td> <td>    0.127</td> <td>   -0.359</td> <td> 0.720</td> <td>   -0.294</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hints</th>           <td>    0.4792</td> <td>    0.166</td> <td>    2.884</td> <td> 0.004</td> <td>    0.153</td> <td>    0.805</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>honey</th>           <td>    0.3049</td> <td>    0.303</td> <td>    1.007</td> <td> 0.314</td> <td>   -0.289</td> <td>    0.899</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>honeyish</th>        <td>    0.4174</td> <td>    0.201</td> <td>    2.076</td> <td> 0.038</td> <td>    0.023</td> <td>    0.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>honeysuckle</th>     <td>   -0.0242</td> <td>    0.301</td> <td>   -0.080</td> <td> 0.936</td> <td>   -0.615</td> <td>    0.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hop</th>             <td>   -0.7413</td> <td>    0.308</td> <td>   -2.406</td> <td> 0.016</td> <td>   -1.346</td> <td>   -0.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hot</th>             <td>   -0.4779</td> <td>    0.493</td> <td>   -0.969</td> <td> 0.332</td> <td>   -1.444</td> <td>    0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>impression</th>      <td>    0.0057</td> <td>    0.371</td> <td>    0.015</td> <td> 0.988</td> <td>   -0.721</td> <td>    0.732</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>impressive</th>      <td>    0.1434</td> <td>    0.205</td> <td>    0.698</td> <td> 0.485</td> <td>   -0.259</td> <td>    0.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>impressively</th>    <td>    0.3662</td> <td>    0.242</td> <td>    1.511</td> <td> 0.131</td> <td>   -0.109</td> <td>    0.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>influenced</th>      <td>    0.9392</td> <td>    0.362</td> <td>    2.596</td> <td> 0.009</td> <td>    0.230</td> <td>    1.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>integrated</th>      <td>    0.7996</td> <td>    0.302</td> <td>    2.646</td> <td> 0.008</td> <td>    0.207</td> <td>    1.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intense</th>         <td>    0.3563</td> <td>    0.207</td> <td>    1.723</td> <td> 0.085</td> <td>   -0.049</td> <td>    0.762</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intensely</th>       <td>    0.2445</td> <td>    0.300</td> <td>    0.815</td> <td> 0.415</td> <td>   -0.344</td> <td>    0.833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intensity</th>       <td>   -0.4625</td> <td>    0.400</td> <td>   -1.155</td> <td> 0.248</td> <td>   -1.248</td> <td>    0.323</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>interesting</th>     <td>    0.2541</td> <td>    0.165</td> <td>    1.537</td> <td> 0.124</td> <td>   -0.070</td> <td>    0.578</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intricate</th>       <td>    0.3496</td> <td>    0.266</td> <td>    1.315</td> <td> 0.188</td> <td>   -0.171</td> <td>    0.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intricately</th>     <td>    0.4992</td> <td>    0.329</td> <td>    1.519</td> <td> 0.129</td> <td>   -0.145</td> <td>    1.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intriguing</th>      <td>    0.3181</td> <td>    0.406</td> <td>    0.783</td> <td> 0.434</td> <td>   -0.479</td> <td>    1.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>jam</th>             <td>    0.3749</td> <td>    0.187</td> <td>    2.006</td> <td> 0.045</td> <td>    0.008</td> <td>    0.741</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>jasmine</th>         <td>    1.2109</td> <td>    0.515</td> <td>    2.349</td> <td> 0.019</td> <td>    0.200</td> <td>    2.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>juicy</th>           <td>    0.2752</td> <td>    0.131</td> <td>    2.093</td> <td> 0.036</td> <td>    0.017</td> <td>    0.533</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>juniper</th>         <td>    0.1107</td> <td>    0.349</td> <td>    0.317</td> <td> 0.751</td> <td>   -0.573</td> <td>    0.795</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>just</th>            <td>   -1.0510</td> <td>    0.348</td> <td>   -3.022</td> <td> 0.003</td> <td>   -1.733</td> <td>   -0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ken</th>             <td>    1.7360</td> <td>    0.385</td> <td>    4.514</td> <td> 0.000</td> <td>    0.982</td> <td>    2.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kenya</th>           <td>    2.5563</td> <td>    1.518</td> <td>    1.684</td> <td> 0.092</td> <td>   -0.419</td> <td>    5.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>keurig</th>          <td>   -0.2922</td> <td>    0.365</td> <td>   -0.801</td> <td> 0.423</td> <td>   -1.008</td> <td>    0.423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>key</th>             <td>    1.4538</td> <td>    0.501</td> <td>    2.899</td> <td> 0.004</td> <td>    0.471</td> <td>    2.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>laden</th>           <td>    0.5495</td> <td>    0.412</td> <td>    1.333</td> <td> 0.182</td> <td>   -0.258</td> <td>    1.357</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lasting</th>         <td>    0.1659</td> <td>    0.233</td> <td>    0.711</td> <td> 0.477</td> <td>   -0.291</td> <td>    0.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lavender</th>        <td>    0.2772</td> <td>    0.174</td> <td>    1.595</td> <td> 0.111</td> <td>   -0.064</td> <td>    0.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>layered</th>         <td>    0.0226</td> <td>    0.233</td> <td>    0.097</td> <td> 0.923</td> <td>   -0.435</td> <td>    0.480</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lead</th>            <td>    0.0470</td> <td>    0.238</td> <td>    0.197</td> <td> 0.844</td> <td>   -0.420</td> <td>    0.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>leads</th>           <td>    0.1505</td> <td>    0.296</td> <td>    0.508</td> <td> 0.611</td> <td>   -0.430</td> <td>    0.731</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>leaf</th>            <td>    0.0590</td> <td>    0.141</td> <td>    0.419</td> <td> 0.675</td> <td>   -0.217</td> <td>    0.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lean</th>            <td>    0.8662</td> <td>    0.474</td> <td>    1.827</td> <td> 0.068</td> <td>   -0.063</td> <td>    1.796</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>leaning</th>         <td>   -0.5858</td> <td>    0.303</td> <td>   -1.933</td> <td> 0.053</td> <td>   -1.180</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>leanish</th>         <td>    0.2545</td> <td>    0.216</td> <td>    1.176</td> <td> 0.240</td> <td>   -0.170</td> <td>    0.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>leather</th>         <td>   -0.8472</td> <td>    0.296</td> <td>   -2.859</td> <td> 0.004</td> <td>   -1.428</td> <td>   -0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>leaves</th>          <td>    0.2725</td> <td>    0.307</td> <td>    0.887</td> <td> 0.375</td> <td>   -0.330</td> <td>    0.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lemon</th>           <td>    0.6587</td> <td>    0.410</td> <td>    1.605</td> <td> 0.109</td> <td>   -0.146</td> <td>    1.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lemony</th>          <td>    0.2414</td> <td>    0.139</td> <td>    1.740</td> <td> 0.082</td> <td>   -0.031</td> <td>    0.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>light</th>           <td>    0.2974</td> <td>    0.284</td> <td>    1.048</td> <td> 0.295</td> <td>   -0.259</td> <td>    0.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lightly</th>         <td>   -0.3716</td> <td>    0.187</td> <td>   -1.990</td> <td> 0.047</td> <td>   -0.738</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>like</th>            <td>   -0.2311</td> <td>    0.133</td> <td>   -1.734</td> <td> 0.083</td> <td>   -0.492</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liked</th>           <td>   -0.3400</td> <td>    0.125</td> <td>   -2.725</td> <td> 0.006</td> <td>   -0.585</td> <td>   -0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lilac</th>           <td>    0.3072</td> <td>    0.183</td> <td>    1.678</td> <td> 0.094</td> <td>   -0.052</td> <td>    0.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lily</th>            <td>    0.2284</td> <td>    0.188</td> <td>    1.218</td> <td> 0.223</td> <td>   -0.139</td> <td>    0.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lime</th>            <td>    0.4429</td> <td>    0.241</td> <td>    1.841</td> <td> 0.066</td> <td>   -0.029</td> <td>    0.915</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>limited</th>         <td>   -0.5312</td> <td>    0.481</td> <td>   -1.103</td> <td> 0.270</td> <td>   -1.475</td> <td>    0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>linger</th>          <td>    0.2809</td> <td>    0.256</td> <td>    1.097</td> <td> 0.273</td> <td>   -0.221</td> <td>    0.783</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lingering</th>       <td>    0.0767</td> <td>    0.191</td> <td>    0.401</td> <td> 0.688</td> <td>   -0.298</td> <td>    0.452</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lingers</th>         <td>    0.3041</td> <td>    0.221</td> <td>    1.377</td> <td> 0.168</td> <td>   -0.129</td> <td>    0.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>little</th>          <td>   -0.5005</td> <td>    0.284</td> <td>   -1.764</td> <td> 0.078</td> <td>   -1.057</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lively</th>          <td>   -0.0165</td> <td>    0.148</td> <td>   -0.112</td> <td> 0.911</td> <td>   -0.306</td> <td>    0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>long</th>            <td>    0.6841</td> <td>    0.123</td> <td>    5.567</td> <td> 0.000</td> <td>    0.443</td> <td>    0.925</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lovely</th>          <td>    0.0723</td> <td>    0.367</td> <td>    0.197</td> <td> 0.844</td> <td>   -0.646</td> <td>    0.791</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>low</th>             <td>    0.3970</td> <td>    0.218</td> <td>    1.823</td> <td> 0.068</td> <td>   -0.030</td> <td>    0.824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lush</th>            <td>    0.4369</td> <td>    0.158</td> <td>    2.761</td> <td> 0.006</td> <td>    0.127</td> <td>    0.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lushly</th>          <td>    0.4225</td> <td>    0.223</td> <td>    1.897</td> <td> 0.058</td> <td>   -0.014</td> <td>    0.859</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lychee</th>          <td>    0.2875</td> <td>    0.273</td> <td>    1.055</td> <td> 0.292</td> <td>   -0.247</td> <td>    0.822</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lyric</th>           <td>    0.6615</td> <td>    0.318</td> <td>    2.079</td> <td> 0.038</td> <td>    0.038</td> <td>    1.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lyrically</th>       <td>    0.3483</td> <td>    0.305</td> <td>    1.143</td> <td> 0.253</td> <td>   -0.249</td> <td>    0.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>macadamia</th>       <td>    0.0127</td> <td>    0.447</td> <td>    0.028</td> <td> 0.977</td> <td>   -0.864</td> <td>    0.890</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>magnolia</th>        <td>    0.1759</td> <td>    0.168</td> <td>    1.044</td> <td> 0.296</td> <td>   -0.154</td> <td>    0.506</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>maintains</th>       <td>    0.4993</td> <td>    0.328</td> <td>    1.521</td> <td> 0.128</td> <td>   -0.144</td> <td>    1.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>malt</th>            <td>    0.3689</td> <td>    0.441</td> <td>    0.837</td> <td> 0.403</td> <td>   -0.495</td> <td>    1.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>malty</th>           <td>    1.4242</td> <td>    0.397</td> <td>    3.584</td> <td> 0.000</td> <td>    0.645</td> <td>    2.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mango</th>           <td>    0.2839</td> <td>    0.203</td> <td>    1.400</td> <td> 0.161</td> <td>   -0.114</td> <td>    0.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>maple</th>           <td>    0.8116</td> <td>    0.531</td> <td>    1.527</td> <td> 0.127</td> <td>   -0.230</td> <td>    1.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marjoram</th>        <td>    0.4084</td> <td>    0.242</td> <td>    1.688</td> <td> 0.091</td> <td>   -0.066</td> <td>    0.883</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>medium</th>          <td>    0.1925</td> <td>    0.157</td> <td>    1.228</td> <td> 0.219</td> <td>   -0.115</td> <td>    0.500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>melon</th>           <td>    0.4517</td> <td>    0.326</td> <td>    1.386</td> <td> 0.166</td> <td>   -0.187</td> <td>    1.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mesquite</th>        <td>    0.2633</td> <td>    0.340</td> <td>    0.775</td> <td> 0.439</td> <td>   -0.403</td> <td>    0.930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>meyer</th>           <td>    0.5324</td> <td>    0.307</td> <td>    1.737</td> <td> 0.082</td> <td>   -0.068</td> <td>    1.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mid</th>             <td>    0.0419</td> <td>    0.407</td> <td>    0.103</td> <td> 0.918</td> <td>   -0.756</td> <td>    0.840</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>miguel</th>          <td>    1.6655</td> <td>    0.455</td> <td>    3.658</td> <td> 0.000</td> <td>    0.773</td> <td>    2.558</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mild</th>            <td>   -0.0233</td> <td>    0.382</td> <td>   -0.061</td> <td> 0.951</td> <td>   -0.772</td> <td>    0.726</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mildly</th>          <td>   -0.2660</td> <td>    0.247</td> <td>   -1.077</td> <td> 0.282</td> <td>   -0.750</td> <td>    0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>milk</th>            <td>    0.6795</td> <td>    0.213</td> <td>    3.195</td> <td> 0.001</td> <td>    0.263</td> <td>    1.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mint</th>            <td>    0.7124</td> <td>    0.432</td> <td>    1.648</td> <td> 0.099</td> <td>   -0.135</td> <td>    1.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>minty</th>           <td>    0.6156</td> <td>    0.338</td> <td>    1.820</td> <td> 0.069</td> <td>   -0.048</td> <td>    1.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>moist</th>           <td>    0.3515</td> <td>    0.322</td> <td>    1.092</td> <td> 0.275</td> <td>   -0.280</td> <td>    0.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>molasses</th>        <td>    0.3738</td> <td>    0.189</td> <td>    1.981</td> <td> 0.048</td> <td>    0.004</td> <td>    0.744</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mulberry</th>        <td>    0.3314</td> <td>    0.298</td> <td>    1.111</td> <td> 0.267</td> <td>   -0.254</td> <td>    0.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>multi</th>           <td>    0.3282</td> <td>    0.247</td> <td>    1.327</td> <td> 0.185</td> <td>   -0.157</td> <td>    0.813</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mushroom</th>        <td>    0.4942</td> <td>    0.320</td> <td>    1.546</td> <td> 0.122</td> <td>   -0.133</td> <td>    1.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>musk</th>            <td>   -0.0681</td> <td>    0.236</td> <td>   -0.289</td> <td> 0.773</td> <td>   -0.530</td> <td>    0.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>musky</th>           <td>    0.1787</td> <td>    0.272</td> <td>    0.657</td> <td> 0.511</td> <td>   -0.354</td> <td>    0.712</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mustiness</th>       <td>   -1.0809</td> <td>    0.393</td> <td>   -2.749</td> <td> 0.006</td> <td>   -1.852</td> <td>   -0.310</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>musty</th>           <td>    0.1019</td> <td>    0.210</td> <td>    0.484</td> <td> 0.628</td> <td>   -0.311</td> <td>    0.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>muted</th>           <td>    0.3501</td> <td>    0.264</td> <td>    1.325</td> <td> 0.185</td> <td>   -0.168</td> <td>    0.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>myrrh</th>           <td>    0.3424</td> <td>    0.170</td> <td>    2.018</td> <td> 0.044</td> <td>    0.010</td> <td>    0.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>narcissus</th>       <td>    0.5554</td> <td>    0.343</td> <td>    1.620</td> <td> 0.105</td> <td>   -0.117</td> <td>    1.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nectar</th>          <td>    0.3814</td> <td>    0.277</td> <td>    1.378</td> <td> 0.168</td> <td>   -0.161</td> <td>    0.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nectarine</th>       <td>   -0.1965</td> <td>    0.174</td> <td>   -1.126</td> <td> 0.260</td> <td>   -0.539</td> <td>    0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nib</th>             <td>    0.3085</td> <td>    0.230</td> <td>    1.340</td> <td> 0.180</td> <td>   -0.143</td> <td>    0.760</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nice</th>            <td>    0.0045</td> <td>    0.262</td> <td>    0.017</td> <td> 0.986</td> <td>   -0.508</td> <td>    0.517</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nicely</th>          <td>   -0.2823</td> <td>    0.237</td> <td>   -1.191</td> <td> 0.234</td> <td>   -0.747</td> <td>    0.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>night</th>           <td>   -0.1114</td> <td>    0.187</td> <td>   -0.596</td> <td> 0.551</td> <td>   -0.478</td> <td>    0.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nominated</th>       <td>    0.2944</td> <td>    0.120</td> <td>    2.454</td> <td> 0.014</td> <td>    0.059</td> <td>    0.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nose</th>            <td>    0.1414</td> <td>    0.214</td> <td>    0.661</td> <td> 0.509</td> <td>   -0.278</td> <td>    0.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>note</th>            <td>   -0.1997</td> <td>    0.238</td> <td>   -0.840</td> <td> 0.401</td> <td>   -0.666</td> <td>    0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>notes</th>           <td>    0.3382</td> <td>    0.212</td> <td>    1.594</td> <td> 0.111</td> <td>   -0.078</td> <td>    0.754</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nougat</th>          <td>   -0.0185</td> <td>    0.171</td> <td>   -0.108</td> <td> 0.914</td> <td>   -0.354</td> <td>    0.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nuance</th>          <td>    0.5327</td> <td>    0.429</td> <td>    1.243</td> <td> 0.214</td> <td>   -0.307</td> <td>    1.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nuanced</th>         <td>    0.4283</td> <td>    0.364</td> <td>    1.177</td> <td> 0.239</td> <td>   -0.285</td> <td>    1.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nut</th>             <td>   -0.9985</td> <td>    0.368</td> <td>   -2.710</td> <td> 0.007</td> <td>   -1.721</td> <td>   -0.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nutella</th>         <td>    0.4089</td> <td>    0.216</td> <td>    1.889</td> <td> 0.059</td> <td>   -0.016</td> <td>    0.833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nutmeg</th>          <td>    0.1347</td> <td>    0.158</td> <td>    0.854</td> <td> 0.393</td> <td>   -0.175</td> <td>    0.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nuts</th>            <td>   -0.0435</td> <td>    0.271</td> <td>   -0.161</td> <td> 0.872</td> <td>   -0.575</td> <td>    0.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nutty</th>           <td>    0.6208</td> <td>    0.166</td> <td>    3.735</td> <td> 0.000</td> <td>    0.295</td> <td>    0.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>oak</th>             <td>    0.0866</td> <td>    0.197</td> <td>    0.440</td> <td> 0.660</td> <td>   -0.299</td> <td>    0.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>opulent</th>         <td>    0.5496</td> <td>    0.394</td> <td>    1.396</td> <td> 0.163</td> <td>   -0.222</td> <td>    1.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>opulently</th>       <td>    0.0573</td> <td>    0.267</td> <td>    0.215</td> <td> 0.830</td> <td>   -0.466</td> <td>    0.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>orange</th>          <td>   -0.3235</td> <td>    1.018</td> <td>   -0.318</td> <td> 0.751</td> <td>   -2.320</td> <td>    1.673</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>orangy</th>          <td>   -0.8124</td> <td>    0.696</td> <td>   -1.167</td> <td> 0.243</td> <td>   -2.177</td> <td>    0.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>orchid</th>          <td>   -0.6641</td> <td>    0.500</td> <td>   -1.328</td> <td> 0.184</td> <td>   -1.645</td> <td>    0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>original</th>        <td>    0.3482</td> <td>    0.296</td> <td>    1.178</td> <td> 0.239</td> <td>   -0.232</td> <td>    0.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ounce</th>           <td>   -0.1498</td> <td>    0.135</td> <td>   -1.110</td> <td> 0.267</td> <td>   -0.414</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ounces</th>          <td>   -0.1180</td> <td>    0.256</td> <td>   -0.461</td> <td> 0.645</td> <td>   -0.620</td> <td>    0.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>owing</th>           <td>   -0.1140</td> <td>    0.342</td> <td>   -0.333</td> <td> 0.739</td> <td>   -0.785</td> <td>    0.557</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>panelists</th>       <td>    0.0361</td> <td>    0.355</td> <td>    0.102</td> <td> 0.919</td> <td>   -0.660</td> <td>    0.732</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>papaya</th>          <td>    0.2041</td> <td>    0.470</td> <td>    0.434</td> <td> 0.664</td> <td>   -0.717</td> <td>    1.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>particular</th>      <td>    0.2382</td> <td>    0.161</td> <td>    1.477</td> <td> 0.140</td> <td>   -0.078</td> <td>    0.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>particularly</th>    <td>    0.3529</td> <td>    0.201</td> <td>    1.758</td> <td> 0.079</td> <td>   -0.041</td> <td>    0.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>parts</th>           <td>    0.0444</td> <td>    0.320</td> <td>    0.139</td> <td> 0.890</td> <td>   -0.583</td> <td>    0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>passion</th>         <td>   -0.0062</td> <td>    0.407</td> <td>   -0.015</td> <td> 0.988</td> <td>   -0.804</td> <td>    0.792</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>passionfruit</th>    <td>    0.3912</td> <td>    0.258</td> <td>    1.518</td> <td> 0.129</td> <td>   -0.114</td> <td>    0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>peach</th>           <td>    0.1607</td> <td>    0.371</td> <td>    0.433</td> <td> 0.665</td> <td>   -0.567</td> <td>    0.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pear</th>            <td>    0.6476</td> <td>    0.286</td> <td>    2.266</td> <td> 0.024</td> <td>    0.087</td> <td>    1.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pecan</th>           <td>    0.0961</td> <td>    0.208</td> <td>    0.463</td> <td> 0.644</td> <td>   -0.311</td> <td>    0.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>peel</th>            <td>    0.6682</td> <td>    0.353</td> <td>    1.894</td> <td> 0.058</td> <td>   -0.024</td> <td>    1.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>peppercorn</th>      <td>    1.1911</td> <td>    0.348</td> <td>    3.423</td> <td> 0.001</td> <td>    0.509</td> <td>    1.873</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>perfectly</th>       <td>   -0.0722</td> <td>    0.163</td> <td>   -0.444</td> <td> 0.657</td> <td>   -0.391</td> <td>    0.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>persimmon</th>       <td>    0.0271</td> <td>    0.225</td> <td>    0.121</td> <td> 0.904</td> <td>   -0.414</td> <td>    0.469</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>persist</th>         <td>    0.5905</td> <td>    0.369</td> <td>    1.602</td> <td> 0.109</td> <td>   -0.132</td> <td>    1.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>persistence</th>     <td>    0.5695</td> <td>    0.315</td> <td>    1.808</td> <td> 0.071</td> <td>   -0.048</td> <td>    1.187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>persistent</th>      <td>    0.1124</td> <td>    0.249</td> <td>    0.451</td> <td> 0.652</td> <td>   -0.376</td> <td>    0.601</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>persists</th>        <td>    0.0333</td> <td>    0.218</td> <td>    0.153</td> <td> 0.879</td> <td>   -0.395</td> <td>    0.461</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pert</th>            <td>    0.0631</td> <td>    0.374</td> <td>    0.169</td> <td> 0.866</td> <td>   -0.670</td> <td>    0.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pie</th>             <td>    0.1320</td> <td>    0.277</td> <td>    0.477</td> <td> 0.633</td> <td>   -0.411</td> <td>    0.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pine</th>            <td>   -0.7010</td> <td>    0.948</td> <td>   -0.739</td> <td> 0.460</td> <td>   -2.560</td> <td>    1.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pineapple</th>       <td>    1.0411</td> <td>    0.320</td> <td>    3.257</td> <td> 0.001</td> <td>    0.414</td> <td>    1.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pink</th>            <td>    0.6127</td> <td>    0.205</td> <td>    2.995</td> <td> 0.003</td> <td>    0.212</td> <td>    1.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pipe</th>            <td>    0.3857</td> <td>    0.192</td> <td>    2.013</td> <td> 0.044</td> <td>    0.010</td> <td>    0.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pistachio</th>       <td>    0.0187</td> <td>    0.226</td> <td>    0.083</td> <td> 0.934</td> <td>   -0.424</td> <td>    0.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>platinum</th>        <td>    0.3802</td> <td>    0.161</td> <td>    2.366</td> <td> 0.018</td> <td>    0.065</td> <td>    0.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pleasant</th>        <td>    0.5145</td> <td>    0.340</td> <td>    1.516</td> <td> 0.130</td> <td>   -0.151</td> <td>    1.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pleasantly</th>      <td>   -0.0096</td> <td>    0.194</td> <td>   -0.050</td> <td> 0.960</td> <td>   -0.389</td> <td>    0.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pleasing</th>        <td>   -0.0369</td> <td>    0.110</td> <td>   -0.336</td> <td> 0.737</td> <td>   -0.252</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pleasingly</th>      <td>   -0.0969</td> <td>    0.925</td> <td>   -0.105</td> <td> 0.917</td> <td>   -1.910</td> <td>    1.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>plum</th>            <td>    0.4757</td> <td>    0.212</td> <td>    2.249</td> <td> 0.025</td> <td>    0.061</td> <td>    0.890</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>plumeria</th>        <td>   -0.2378</td> <td>    0.608</td> <td>   -0.391</td> <td> 0.696</td> <td>   -1.429</td> <td>    0.954</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>plump</th>           <td>   -0.2777</td> <td>    0.247</td> <td>   -1.123</td> <td> 0.261</td> <td>   -0.763</td> <td>    0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>plush</th>           <td>   -0.5221</td> <td>    0.317</td> <td>   -1.645</td> <td> 0.100</td> <td>   -1.144</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pod</th>             <td>    0.5610</td> <td>    0.254</td> <td>    2.212</td> <td> 0.027</td> <td>    0.064</td> <td>    1.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pomegranate</th>     <td>   -0.3124</td> <td>    0.445</td> <td>   -0.702</td> <td> 0.483</td> <td>   -1.185</td> <td>    0.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>port</th>            <td>    0.0238</td> <td>    0.273</td> <td>    0.087</td> <td> 0.931</td> <td>   -0.512</td> <td>    0.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>powder</th>          <td>    1.3263</td> <td>    0.429</td> <td>    3.090</td> <td> 0.002</td> <td>    0.485</td> <td>    2.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>power</th>           <td>   -0.1052</td> <td>    0.457</td> <td>   -0.230</td> <td> 0.818</td> <td>   -1.001</td> <td>    0.790</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>powerful</th>        <td>    0.3375</td> <td>    0.866</td> <td>    0.390</td> <td> 0.697</td> <td>   -1.361</td> <td>    2.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>presence</th>        <td>   -0.2423</td> <td>    0.528</td> <td>   -0.459</td> <td> 0.646</td> <td>   -1.278</td> <td>    0.793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>probably</th>        <td>    0.9734</td> <td>    0.202</td> <td>    4.822</td> <td> 0.000</td> <td>    0.578</td> <td>    1.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>produce</th>         <td>    0.0774</td> <td>    0.322</td> <td>    0.241</td> <td> 0.810</td> <td>   -0.553</td> <td>    0.708</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>produced</th>        <td>   -0.1575</td> <td>    0.241</td> <td>   -0.654</td> <td> 0.513</td> <td>   -0.629</td> <td>    0.314</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>profile</th>         <td>    0.4300</td> <td>    0.302</td> <td>    1.422</td> <td> 0.155</td> <td>   -0.163</td> <td>    1.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>promise</th>         <td>   -2.0942</td> <td>    0.305</td> <td>   -6.866</td> <td> 0.000</td> <td>   -2.692</td> <td>   -1.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pronounced</th>      <td>    0.1113</td> <td>    0.264</td> <td>    0.421</td> <td> 0.673</td> <td>   -0.407</td> <td>    0.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prune</th>           <td>    0.3266</td> <td>    0.132</td> <td>    2.479</td> <td> 0.013</td> <td>    0.068</td> <td>    0.585</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pruny</th>           <td>    0.3032</td> <td>    0.273</td> <td>    1.110</td> <td> 0.267</td> <td>   -0.233</td> <td>    0.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pungency</th>        <td>    0.8300</td> <td>    0.247</td> <td>    3.358</td> <td> 0.001</td> <td>    0.345</td> <td>    1.315</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pungent</th>         <td>    0.2449</td> <td>    0.396</td> <td>    0.618</td> <td> 0.537</td> <td>   -0.532</td> <td>    1.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pungently</th>       <td>    0.2311</td> <td>    0.149</td> <td>    1.554</td> <td> 0.120</td> <td>   -0.060</td> <td>    0.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pure</th>            <td>    0.3149</td> <td>    0.151</td> <td>    2.092</td> <td> 0.037</td> <td>    0.020</td> <td>    0.610</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>quickly</th>         <td>   -0.5555</td> <td>    0.314</td> <td>   -1.767</td> <td> 0.077</td> <td>   -1.172</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>quiet</th>           <td>    0.2895</td> <td>    0.204</td> <td>    1.423</td> <td> 0.155</td> <td>   -0.110</td> <td>    0.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>quietly</th>         <td>    0.2967</td> <td>    0.384</td> <td>    0.772</td> <td> 0.440</td> <td>   -0.456</td> <td>    1.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>quite</th>           <td>    0.1574</td> <td>    0.260</td> <td>    0.604</td> <td> 0.546</td> <td>   -0.353</td> <td>    0.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>raisin</th>          <td>   -0.0566</td> <td>    0.338</td> <td>   -0.167</td> <td> 0.867</td> <td>   -0.720</td> <td>    0.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>raisiny</th>         <td>    0.3971</td> <td>    0.183</td> <td>    2.165</td> <td> 0.030</td> <td>    0.037</td> <td>    0.757</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>range</th>           <td>   -0.0230</td> <td>    0.363</td> <td>   -0.063</td> <td> 0.950</td> <td>   -0.735</td> <td>    0.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>raspberry</th>       <td>   -0.6823</td> <td>    0.501</td> <td>   -1.361</td> <td> 0.174</td> <td>   -1.666</td> <td>    0.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating</th>          <td>    0.0588</td> <td>    0.269</td> <td>    0.219</td> <td> 0.827</td> <td>   -0.468</td> <td>    0.585</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>raw</th>             <td>    0.1765</td> <td>    0.323</td> <td>    0.547</td> <td> 0.585</td> <td>   -0.456</td> <td>    0.809</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>read</th>            <td>    0.7792</td> <td>    0.402</td> <td>    1.937</td> <td> 0.053</td> <td>   -0.009</td> <td>    1.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reader</th>          <td>   -0.3179</td> <td>    0.306</td> <td>   -1.039</td> <td> 0.299</td> <td>   -0.918</td> <td>    0.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reads</th>           <td>    0.4626</td> <td>    1.002</td> <td>    0.462</td> <td> 0.644</td> <td>   -1.503</td> <td>    2.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ready</th>           <td>    0.1225</td> <td>    0.213</td> <td>    0.575</td> <td> 0.565</td> <td>   -0.295</td> <td>    0.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>red</th>             <td>    0.0732</td> <td>    0.267</td> <td>    0.275</td> <td> 0.784</td> <td>   -0.449</td> <td>    0.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>redolent</th>        <td>    0.0717</td> <td>    0.328</td> <td>    0.219</td> <td> 0.827</td> <td>   -0.571</td> <td>    0.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>redwood</th>         <td>    0.5052</td> <td>    0.314</td> <td>    1.611</td> <td> 0.107</td> <td>   -0.110</td> <td>    1.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>refreshing</th>      <td>   -0.0644</td> <td>    0.344</td> <td>   -0.187</td> <td> 0.851</td> <td>   -0.738</td> <td>    0.609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>relatively</th>      <td>   -0.6204</td> <td>    0.274</td> <td>   -2.267</td> <td> 0.023</td> <td>   -1.157</td> <td>   -0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>remains</th>         <td>   -0.0519</td> <td>    0.346</td> <td>   -0.150</td> <td> 0.881</td> <td>   -0.731</td> <td>    0.627</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>resonance</th>       <td>    0.2080</td> <td>    0.109</td> <td>    1.902</td> <td> 0.057</td> <td>   -0.006</td> <td>    0.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>resonant</th>        <td>    0.4524</td> <td>    0.285</td> <td>    1.587</td> <td> 0.113</td> <td>   -0.106</td> <td>    1.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>resonantly</th>      <td>    0.4185</td> <td>    0.318</td> <td>    1.314</td> <td> 0.189</td> <td>   -0.206</td> <td>    1.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>resonate</th>        <td>    0.1137</td> <td>    0.318</td> <td>    0.358</td> <td> 0.720</td> <td>   -0.509</td> <td>    0.736</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>resonates</th>       <td>   -0.3838</td> <td>    0.299</td> <td>   -1.282</td> <td> 0.200</td> <td>   -0.971</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>resurface</th>       <td>    0.3384</td> <td>    0.276</td> <td>    1.227</td> <td> 0.220</td> <td>   -0.202</td> <td>    0.879</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>resurfacing</th>     <td>    0.4497</td> <td>    0.115</td> <td>    3.912</td> <td> 0.000</td> <td>    0.224</td> <td>    0.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rhododendron</th>    <td>    0.3743</td> <td>    0.106</td> <td>    3.518</td> <td> 0.000</td> <td>    0.166</td> <td>    0.583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rich</th>            <td>    0.0866</td> <td>    0.284</td> <td>    0.305</td> <td> 0.760</td> <td>   -0.470</td> <td>    0.643</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>richly</th>          <td>    0.2964</td> <td>    0.296</td> <td>    1.003</td> <td> 0.316</td> <td>   -0.283</td> <td>    0.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>richness</th>        <td>    0.2867</td> <td>    0.168</td> <td>    1.706</td> <td> 0.088</td> <td>   -0.043</td> <td>    0.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rings</th>           <td>   -0.3172</td> <td>    0.219</td> <td>   -1.449</td> <td> 0.147</td> <td>   -0.746</td> <td>    0.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ripe</th>            <td>    0.1144</td> <td>    0.174</td> <td>    0.659</td> <td> 0.510</td> <td>   -0.226</td> <td>    0.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>roast</th>           <td>    0.8158</td> <td>    0.290</td> <td>    2.816</td> <td> 0.005</td> <td>    0.248</td> <td>    1.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>roasted</th>         <td>    0.2486</td> <td>    0.234</td> <td>    1.064</td> <td> 0.287</td> <td>   -0.209</td> <td>    0.706</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>roastiness</th>      <td>    0.3068</td> <td>    0.197</td> <td>    1.558</td> <td> 0.119</td> <td>   -0.079</td> <td>    0.693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>roasty</th>          <td>   -0.7752</td> <td>    0.388</td> <td>   -2.000</td> <td> 0.046</td> <td>   -1.535</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rose</th>            <td>   -0.2192</td> <td>    0.267</td> <td>   -0.822</td> <td> 0.411</td> <td>   -0.742</td> <td>    0.304</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rough</th>           <td>    0.0536</td> <td>    0.138</td> <td>    0.389</td> <td> 0.697</td> <td>   -0.217</td> <td>    0.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>round</th>           <td>    0.1547</td> <td>    0.198</td> <td>    0.780</td> <td> 0.436</td> <td>   -0.234</td> <td>    0.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rounded</th>         <td>    0.0265</td> <td>    0.184</td> <td>    0.144</td> <td> 0.885</td> <td>   -0.334</td> <td>    0.387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rounding</th>        <td>    0.1973</td> <td>    0.175</td> <td>    1.130</td> <td> 0.259</td> <td>   -0.145</td> <td>    0.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>roundly</th>         <td>    0.1438</td> <td>    0.198</td> <td>    0.727</td> <td> 0.467</td> <td>   -0.244</td> <td>    0.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rounds</th>          <td>    0.6327</td> <td>    0.363</td> <td>    1.745</td> <td> 0.081</td> <td>   -0.078</td> <td>    1.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rum</th>             <td>   -0.1511</td> <td>    0.500</td> <td>   -0.302</td> <td> 0.762</td> <td>   -1.131</td> <td>    0.829</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rye</th>             <td>    0.0816</td> <td>    0.325</td> <td>    0.251</td> <td> 0.802</td> <td>   -0.555</td> <td>    0.718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sage</th>            <td>   -1.0513</td> <td>    0.360</td> <td>   -2.920</td> <td> 0.004</td> <td>   -1.757</td> <td>   -0.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>salted</th>          <td>   -0.3125</td> <td>    0.287</td> <td>   -1.089</td> <td> 0.276</td> <td>   -0.875</td> <td>    0.250</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>salty</th>           <td>   -1.4160</td> <td>    0.494</td> <td>   -2.867</td> <td> 0.004</td> <td>   -2.384</td> <td>   -0.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sample</th>          <td>    0.2781</td> <td>    0.137</td> <td>    2.025</td> <td> 0.043</td> <td>    0.009</td> <td>    0.547</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sandalwood</th>      <td>   -0.2376</td> <td>    0.105</td> <td>   -2.274</td> <td> 0.023</td> <td>   -0.443</td> <td>   -0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sassafras</th>       <td>    0.2901</td> <td>    0.296</td> <td>    0.980</td> <td> 0.327</td> <td>   -0.290</td> <td>    0.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>satiny</th>          <td>    0.5064</td> <td>    0.285</td> <td>    1.779</td> <td> 0.075</td> <td>   -0.052</td> <td>    1.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>satisfying</th>      <td>    0.4972</td> <td>    0.190</td> <td>    2.616</td> <td> 0.009</td> <td>    0.125</td> <td>    0.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>saturated</th>       <td>    0.2542</td> <td>    0.277</td> <td>    0.918</td> <td> 0.359</td> <td>   -0.289</td> <td>    0.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>saturates</th>       <td>    0.0296</td> <td>    0.138</td> <td>    0.214</td> <td> 0.831</td> <td>   -0.242</td> <td>    0.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>savory</th>          <td>   -0.3185</td> <td>    0.574</td> <td>   -0.555</td> <td> 0.579</td> <td>   -1.444</td> <td>    0.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>scaled</th>          <td>    0.0089</td> <td>    0.203</td> <td>    0.044</td> <td> 0.965</td> <td>   -0.388</td> <td>    0.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>scorched</th>        <td>    0.3825</td> <td>    0.254</td> <td>    1.505</td> <td> 0.132</td> <td>   -0.116</td> <td>    0.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>semi</th>            <td>   -0.0115</td> <td>    0.274</td> <td>   -0.042</td> <td> 0.966</td> <td>   -0.548</td> <td>    0.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sensation</th>       <td>    0.4572</td> <td>    1.657</td> <td>    0.276</td> <td> 0.783</td> <td>   -2.791</td> <td>    3.706</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>serve</th>           <td>    1.5432</td> <td>    1.315</td> <td>    1.174</td> <td> 0.241</td> <td>   -1.034</td> <td>    4.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>serving</th>         <td>   -0.5729</td> <td>    0.393</td> <td>   -1.457</td> <td> 0.145</td> <td>   -1.344</td> <td>    0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shadow</th>          <td>   -2.0501</td> <td>    0.312</td> <td>   -6.570</td> <td> 0.000</td> <td>   -2.662</td> <td>   -1.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shallow</th>         <td>   -2.1043</td> <td>    0.396</td> <td>   -5.313</td> <td> 0.000</td> <td>   -2.881</td> <td>   -1.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sharp</th>           <td>   -1.5187</td> <td>    0.358</td> <td>   -4.239</td> <td> 0.000</td> <td>   -2.221</td> <td>   -0.816</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sharpness</th>       <td>   -0.1321</td> <td>    0.332</td> <td>   -0.398</td> <td> 0.691</td> <td>   -0.783</td> <td>    0.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shifting</th>        <td>    0.8846</td> <td>    0.254</td> <td>    3.484</td> <td> 0.000</td> <td>    0.387</td> <td>    1.382</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shimmer</th>         <td>   -0.4472</td> <td>    0.113</td> <td>   -3.956</td> <td> 0.000</td> <td>   -0.669</td> <td>   -0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>short</th>           <td>    0.1148</td> <td>    0.392</td> <td>    0.293</td> <td> 0.770</td> <td>   -0.655</td> <td>    0.884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shot</th>            <td>   -0.0383</td> <td>    0.103</td> <td>   -0.371</td> <td> 0.711</td> <td>   -0.241</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>silky</th>           <td>   -0.5856</td> <td>    0.190</td> <td>   -3.074</td> <td> 0.002</td> <td>   -0.959</td> <td>   -0.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>simple</th>          <td>    0.0295</td> <td>    0.208</td> <td>    0.142</td> <td> 0.887</td> <td>   -0.379</td> <td>    0.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>simplifies</th>      <td>   -0.1514</td> <td>    0.326</td> <td>   -0.464</td> <td> 0.643</td> <td>   -0.791</td> <td>    0.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>simplify</th>        <td>   -1.4182</td> <td>    0.820</td> <td>   -1.730</td> <td> 0.084</td> <td>   -3.025</td> <td>    0.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>singed</th>          <td>   -0.4510</td> <td>    0.538</td> <td>   -0.838</td> <td> 0.402</td> <td>   -1.506</td> <td>    0.604</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>single</th>          <td>    0.4441</td> <td>    0.204</td> <td>    2.182</td> <td> 0.029</td> <td>    0.045</td> <td>    0.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>size</th>            <td>    0.3368</td> <td>    0.200</td> <td>    1.686</td> <td> 0.092</td> <td>   -0.055</td> <td>    0.728</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>slight</th>          <td>    0.1423</td> <td>    0.254</td> <td>    0.561</td> <td> 0.575</td> <td>   -0.355</td> <td>    0.640</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>slightly</th>        <td>   -0.2937</td> <td>    0.341</td> <td>   -0.861</td> <td> 0.389</td> <td>   -0.962</td> <td>    0.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>small</th>           <td>    0.4133</td> <td>    0.243</td> <td>    1.704</td> <td> 0.089</td> <td>   -0.062</td> <td>    0.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>smoke</th>           <td>    0.0246</td> <td>    0.098</td> <td>    0.252</td> <td> 0.801</td> <td>   -0.167</td> <td>    0.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>smoky</th>           <td>    0.2561</td> <td>    0.234</td> <td>    1.095</td> <td> 0.273</td> <td>   -0.202</td> <td>    0.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>smooth</th>          <td>    0.4064</td> <td>    0.189</td> <td>    2.145</td> <td> 0.032</td> <td>    0.035</td> <td>    0.778</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>smoothly</th>        <td>   -0.6767</td> <td>    0.287</td> <td>   -2.356</td> <td> 0.019</td> <td>   -1.240</td> <td>   -0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>soft</th>            <td>    0.4635</td> <td>    0.361</td> <td>    1.283</td> <td> 0.200</td> <td>   -0.245</td> <td>    1.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>soften</th>          <td>    0.0549</td> <td>    0.310</td> <td>    0.177</td> <td> 0.859</td> <td>   -0.552</td> <td>    0.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>softened</th>        <td>   -0.2937</td> <td>    0.233</td> <td>   -1.258</td> <td> 0.208</td> <td>   -0.751</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>softening</th>       <td>    0.2451</td> <td>    0.201</td> <td>    1.221</td> <td> 0.222</td> <td>   -0.148</td> <td>    0.639</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>softens</th>         <td>   -0.5461</td> <td>    0.268</td> <td>   -2.036</td> <td> 0.042</td> <td>   -1.072</td> <td>   -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>softly</th>          <td>   -0.5390</td> <td>    0.395</td> <td>   -1.364</td> <td> 0.173</td> <td>   -1.314</td> <td>    0.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>solid</th>           <td>   -0.0170</td> <td>    0.278</td> <td>   -0.061</td> <td> 0.951</td> <td>   -0.563</td> <td>    0.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sort</th>            <td>   -1.3963</td> <td>    0.427</td> <td>   -3.273</td> <td> 0.001</td> <td>   -2.233</td> <td>   -0.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>spearmint</th>       <td>    0.3884</td> <td>    0.362</td> <td>    1.074</td> <td> 0.283</td> <td>   -0.321</td> <td>    1.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>spice</th>           <td>    0.1236</td> <td>    0.196</td> <td>    0.631</td> <td> 0.528</td> <td>   -0.261</td> <td>    0.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>spices</th>          <td>    0.1936</td> <td>    0.311</td> <td>    0.623</td> <td> 0.533</td> <td>   -0.416</td> <td>    0.803</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>spicy</th>           <td>    0.2039</td> <td>    0.206</td> <td>    0.989</td> <td> 0.323</td> <td>   -0.201</td> <td>    0.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>star</th>            <td>    0.3888</td> <td>    0.302</td> <td>    1.285</td> <td> 0.199</td> <td>   -0.204</td> <td>    0.982</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>stone</th>           <td>   -0.0553</td> <td>    0.251</td> <td>   -0.220</td> <td> 0.826</td> <td>   -0.547</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>straight</th>        <td>   -0.2575</td> <td>    0.342</td> <td>   -0.754</td> <td> 0.451</td> <td>   -0.927</td> <td>    0.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>straightforward</th> <td>   -0.2390</td> <td>    0.254</td> <td>   -0.940</td> <td> 0.347</td> <td>   -0.738</td> <td>    0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>strawberry</th>      <td>    0.2725</td> <td>    0.210</td> <td>    1.300</td> <td> 0.194</td> <td>   -0.138</td> <td>    0.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>structure</th>       <td>   -0.1309</td> <td>    0.131</td> <td>   -0.998</td> <td> 0.318</td> <td>   -0.388</td> <td>    0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>style</th>           <td>    0.1115</td> <td>    0.308</td> <td>    0.362</td> <td> 0.718</td> <td>   -0.493</td> <td>    0.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>subdued</th>         <td>   -0.7553</td> <td>    0.335</td> <td>   -2.258</td> <td> 0.024</td> <td>   -1.411</td> <td>   -0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>substantial</th>     <td>    0.0231</td> <td>    0.254</td> <td>    0.091</td> <td> 0.927</td> <td>   -0.474</td> <td>    0.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>subtle</th>          <td>   -0.7370</td> <td>    0.403</td> <td>   -1.831</td> <td> 0.067</td> <td>   -1.526</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>subtly</th>          <td>    0.9848</td> <td>    0.286</td> <td>    3.445</td> <td> 0.001</td> <td>    0.424</td> <td>    1.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sugar</th>           <td>    0.1203</td> <td>    0.181</td> <td>    0.666</td> <td> 0.506</td> <td>   -0.234</td> <td>    0.474</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sugary</th>          <td>   -0.0773</td> <td>    0.632</td> <td>   -0.122</td> <td> 0.903</td> <td>   -1.316</td> <td>    1.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>suggest</th>         <td>    1.0795</td> <td>    0.300</td> <td>    3.595</td> <td> 0.000</td> <td>    0.491</td> <td>    1.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>suggesting</th>      <td>    0.6856</td> <td>    0.338</td> <td>    2.031</td> <td> 0.042</td> <td>    0.024</td> <td>    1.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>suggestion</th>      <td>   -0.0085</td> <td>    0.263</td> <td>   -0.032</td> <td> 0.974</td> <td>   -0.524</td> <td>    0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>suggestions</th>     <td>   -0.0397</td> <td>    0.203</td> <td>   -0.195</td> <td> 0.845</td> <td>   -0.438</td> <td>    0.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>suggests</th>        <td>   -0.1120</td> <td>    0.168</td> <td>   -0.668</td> <td> 0.504</td> <td>   -0.441</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sumatra</th>         <td>    0.2093</td> <td>    0.277</td> <td>    0.754</td> <td> 0.451</td> <td>   -0.335</td> <td>    0.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>superb</th>          <td>    0.7611</td> <td>    0.351</td> <td>    2.167</td> <td> 0.030</td> <td>    0.072</td> <td>    1.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>support</th>         <td>    0.8032</td> <td>    0.294</td> <td>    2.736</td> <td> 0.006</td> <td>    0.228</td> <td>    1.379</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>supported</th>       <td>    0.1387</td> <td>    0.353</td> <td>    0.393</td> <td> 0.694</td> <td>   -0.553</td> <td>    0.831</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>supporting</th>      <td>    0.0957</td> <td>    0.124</td> <td>    0.772</td> <td> 0.440</td> <td>   -0.147</td> <td>    0.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>surfaces</th>        <td>   -0.8807</td> <td>    0.274</td> <td>   -3.212</td> <td> 0.001</td> <td>   -1.418</td> <td>   -0.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>surprising</th>      <td>    0.3754</td> <td>    0.239</td> <td>    1.569</td> <td> 0.117</td> <td>   -0.094</td> <td>    0.845</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>surprisingly</th>    <td>    0.1984</td> <td>    0.350</td> <td>    0.566</td> <td> 0.571</td> <td>   -0.489</td> <td>    0.885</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sustained</th>       <td>    0.2757</td> <td>    0.119</td> <td>    2.307</td> <td> 0.021</td> <td>    0.041</td> <td>    0.510</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sweet</th>           <td>    0.1520</td> <td>    0.233</td> <td>    0.653</td> <td> 0.514</td> <td>   -0.304</td> <td>    0.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sweetens</th>        <td>    0.2073</td> <td>    0.401</td> <td>    0.518</td> <td> 0.605</td> <td>   -0.578</td> <td>    0.993</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sweetly</th>         <td>    0.1494</td> <td>    0.118</td> <td>    1.265</td> <td> 0.206</td> <td>   -0.082</td> <td>    0.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sweetness</th>       <td>    0.1197</td> <td>    0.247</td> <td>    0.484</td> <td> 0.628</td> <td>   -0.365</td> <td>    0.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>syrup</th>           <td>   -0.5728</td> <td>    0.527</td> <td>   -1.087</td> <td> 0.277</td> <td>   -1.606</td> <td>    0.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>syrupy</th>          <td>   -0.2637</td> <td>    0.152</td> <td>   -1.739</td> <td> 0.082</td> <td>   -0.561</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tamarind</th>        <td>    0.4873</td> <td>    0.234</td> <td>    2.087</td> <td> 0.037</td> <td>    0.029</td> <td>    0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tangerine</th>       <td>    0.3708</td> <td>    0.171</td> <td>    2.171</td> <td> 0.030</td> <td>    0.036</td> <td>    0.706</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tangy</th>           <td>    0.3335</td> <td>    0.409</td> <td>    0.815</td> <td> 0.415</td> <td>   -0.469</td> <td>    1.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tart</th>            <td>   -0.0119</td> <td>    0.134</td> <td>   -0.089</td> <td> 0.929</td> <td>   -0.274</td> <td>    0.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tartly</th>          <td>   -0.1709</td> <td>    0.223</td> <td>   -0.766</td> <td> 0.444</td> <td>   -0.608</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>taste</th>           <td>   -0.8590</td> <td>    0.345</td> <td>   -2.492</td> <td> 0.013</td> <td>   -1.535</td> <td>   -0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>taster</th>          <td>    0.1036</td> <td>    0.531</td> <td>    0.195</td> <td> 0.845</td> <td>   -0.937</td> <td>    1.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tea</th>             <td>   -0.3699</td> <td>    0.324</td> <td>   -1.140</td> <td> 0.254</td> <td>   -1.006</td> <td>    0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ted</th>             <td>    0.4856</td> <td>    0.480</td> <td>    1.011</td> <td> 0.312</td> <td>   -0.456</td> <td>    1.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tested</th>          <td>   -1.2060</td> <td>    0.706</td> <td>   -1.709</td> <td> 0.088</td> <td>   -2.589</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>think</th>           <td>    0.1954</td> <td>    0.298</td> <td>    0.656</td> <td> 0.512</td> <td>   -0.389</td> <td>    0.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>throughline</th>     <td>   -0.0545</td> <td>    0.334</td> <td>   -0.163</td> <td> 0.870</td> <td>   -0.709</td> <td>    0.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>thyme</th>           <td>    0.2655</td> <td>    0.217</td> <td>    1.224</td> <td> 0.221</td> <td>   -0.160</td> <td>    0.691</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tickle</th>          <td>    0.9421</td> <td>    0.339</td> <td>    2.776</td> <td> 0.006</td> <td>    0.277</td> <td>    1.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tight</th>           <td>   -1.2351</td> <td>    0.527</td> <td>   -2.346</td> <td> 0.019</td> <td>   -2.267</td> <td>   -0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>toast</th>           <td>   -0.0037</td> <td>    0.253</td> <td>   -0.014</td> <td> 0.988</td> <td>   -0.500</td> <td>    0.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>toasted</th>         <td>   -0.4080</td> <td>    0.394</td> <td>   -1.036</td> <td> 0.300</td> <td>   -1.180</td> <td>    0.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>toasty</th>          <td>    0.9783</td> <td>    0.416</td> <td>    2.352</td> <td> 0.019</td> <td>    0.163</td> <td>    1.794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tobacco</th>         <td>    0.2870</td> <td>    0.315</td> <td>    0.911</td> <td> 0.362</td> <td>   -0.331</td> <td>    0.905</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>toffee</th>          <td>    0.0950</td> <td>    0.240</td> <td>    0.396</td> <td> 0.692</td> <td>   -0.375</td> <td>    0.565</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tomato</th>          <td>   -0.1929</td> <td>    0.285</td> <td>   -0.677</td> <td> 0.498</td> <td>   -0.751</td> <td>    0.365</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>toned</th>           <td>   -0.0782</td> <td>    0.129</td> <td>   -0.606</td> <td> 0.544</td> <td>   -0.331</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tones</th>           <td>   -0.3447</td> <td>    0.186</td> <td>   -1.856</td> <td> 0.063</td> <td>   -0.709</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>touch</th>           <td>    0.9531</td> <td>    0.284</td> <td>    3.354</td> <td> 0.001</td> <td>    0.396</td> <td>    1.510</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tropical</th>        <td>    0.5248</td> <td>    0.272</td> <td>    1.927</td> <td> 0.054</td> <td>   -0.009</td> <td>    1.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>turn</th>            <td>    0.4286</td> <td>    0.279</td> <td>    1.536</td> <td> 0.125</td> <td>   -0.118</td> <td>    0.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>turned</th>          <td>    0.6992</td> <td>    0.401</td> <td>    1.742</td> <td> 0.082</td> <td>   -0.088</td> <td>    1.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>turning</th>         <td>   -0.4028</td> <td>    0.324</td> <td>   -1.244</td> <td> 0.214</td> <td>   -1.038</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>turns</th>           <td>   -0.3378</td> <td>    0.177</td> <td>   -1.905</td> <td> 0.057</td> <td>   -0.685</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>underlying</th>      <td>    1.0703</td> <td>    0.362</td> <td>    2.957</td> <td> 0.003</td> <td>    0.361</td> <td>    1.780</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>understated</th>     <td>    0.6765</td> <td>    0.290</td> <td>    2.335</td> <td> 0.020</td> <td>    0.109</td> <td>    1.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>undertones</th>      <td>    0.1769</td> <td>    0.140</td> <td>    1.260</td> <td> 0.208</td> <td>   -0.098</td> <td>    0.452</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>unusual</th>         <td>    0.6468</td> <td>    0.272</td> <td>    2.381</td> <td> 0.017</td> <td>    0.114</td> <td>    1.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>using</th>           <td>   -0.6848</td> <td>    0.751</td> <td>   -0.912</td> <td> 0.362</td> <td>   -2.157</td> <td>    0.788</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vanilla</th>         <td>   -0.0399</td> <td>    0.209</td> <td>   -0.191</td> <td> 0.848</td> <td>   -0.449</td> <td>    0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>velvety</th>         <td>   -0.2081</td> <td>    0.138</td> <td>   -1.511</td> <td> 0.131</td> <td>   -0.478</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>verbena</th>         <td>    0.0794</td> <td>    0.212</td> <td>    0.375</td> <td> 0.708</td> <td>   -0.336</td> <td>    0.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vibrant</th>         <td>   -0.1947</td> <td>    0.355</td> <td>   -0.548</td> <td> 0.584</td> <td>   -0.891</td> <td>    0.502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vibrantly</th>       <td>    0.1343</td> <td>    0.114</td> <td>    1.176</td> <td> 0.240</td> <td>   -0.090</td> <td>    0.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>violet</th>          <td>    0.4258</td> <td>    0.218</td> <td>    1.955</td> <td> 0.051</td> <td>   -0.001</td> <td>    0.853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>viscous</th>         <td>    0.2179</td> <td>    0.242</td> <td>    0.900</td> <td> 0.368</td> <td>   -0.257</td> <td>    0.692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>vivacious</th>       <td>    0.1529</td> <td>    0.167</td> <td>    0.916</td> <td> 0.360</td> <td>   -0.175</td> <td>    0.480</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>volume</th>          <td>    0.7320</td> <td>    0.335</td> <td>    2.184</td> <td> 0.029</td> <td>    0.075</td> <td>    1.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>walnut</th>          <td>    0.3812</td> <td>    0.640</td> <td>    0.596</td> <td> 0.551</td> <td>   -0.873</td> <td>    1.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>water</th>           <td>    0.2322</td> <td>    0.217</td> <td>    1.070</td> <td> 0.285</td> <td>   -0.193</td> <td>    0.658</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>watermelon</th>      <td>   -0.0691</td> <td>    0.560</td> <td>   -0.124</td> <td> 0.902</td> <td>   -1.166</td> <td>    1.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>way</th>             <td>    0.3625</td> <td>    0.392</td> <td>    0.925</td> <td> 0.355</td> <td>   -0.406</td> <td>    1.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wendy</th>           <td>    0.4112</td> <td>    0.365</td> <td>    1.126</td> <td> 0.260</td> <td>   -0.305</td> <td>    1.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>whisky</th>          <td>   -0.1341</td> <td>    0.382</td> <td>   -0.351</td> <td> 0.726</td> <td>   -0.883</td> <td>    0.615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>white</th>           <td>    0.5468</td> <td>    0.475</td> <td>    1.151</td> <td> 0.250</td> <td>   -0.385</td> <td>    1.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wild</th>            <td>    0.1057</td> <td>    0.262</td> <td>    0.404</td> <td> 0.687</td> <td>   -0.408</td> <td>    0.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>willem</th>          <td>   -0.0802</td> <td>    0.307</td> <td>   -0.261</td> <td> 0.794</td> <td>   -0.683</td> <td>    0.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wine</th>            <td>   -0.1421</td> <td>    0.466</td> <td>   -0.305</td> <td> 0.761</td> <td>   -1.056</td> <td>    0.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>winey</th>           <td>    1.9087</td> <td>    0.225</td> <td>    8.470</td> <td> 0.000</td> <td>    1.467</td> <td>    2.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>winy</th>            <td>    0.8862</td> <td>    0.290</td> <td>    3.055</td> <td> 0.002</td> <td>    0.318</td> <td>    1.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wisteria</th>        <td>    0.2840</td> <td>    0.229</td> <td>    1.238</td> <td> 0.216</td> <td>   -0.166</td> <td>    0.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wood</th>            <td>   -0.0446</td> <td>    0.254</td> <td>   -0.175</td> <td> 0.861</td> <td>   -0.543</td> <td>    0.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>woody</th>           <td>   -2.4231</td> <td>    0.360</td> <td>   -6.726</td> <td> 0.000</td> <td>   -3.129</td> <td>   -1.717</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zest</th>            <td>   -0.0436</td> <td>    0.126</td> <td>   -0.346</td> <td> 0.729</td> <td>   -0.290</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zesty</th>           <td>   -0.0805</td> <td>    0.277</td> <td>   -0.291</td> <td> 0.771</td> <td>   -0.623</td> <td>    0.462</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>694.171</td> <th>  Durbin-Watson:     </th> <td>   2.022</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>11031.627</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.271</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>10.927</td>  <th>  Cond. No.          </th> <td>    379.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          overall_score   R-squared:                       0.948\n",
       "Model:                            OLS   Adj. R-squared:                  0.939\n",
       "Method:                 Least Squares   F-statistic:                     102.8\n",
       "Date:                Sat, 05 Nov 2022   Prob (F-statistic):               0.00\n",
       "Time:                        14:43:28   Log-Likelihood:                -5668.5\n",
       "No. Observations:                4194   AIC:                         1.260e+04\n",
       "Df Residuals:                    3563   BIC:                         1.660e+04\n",
       "Df Model:                         630                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const              65.5369      0.381    171.805      0.000      64.789      66.285\n",
       "month               0.0725      0.057      1.266      0.205      -0.040       0.185\n",
       "year                0.1574      0.251      0.628      0.530      -0.334       0.649\n",
       "bean_agtron        -0.4885      0.376     -1.299      0.194      -1.226       0.249\n",
       "ground_agtron       1.2105      0.289      4.187      0.000       0.644       1.777\n",
       "aroma               8.7599      0.262     33.482      0.000       8.247       9.273\n",
       "acidity             5.4980      0.336     16.387      0.000       4.840       6.156\n",
       "body                4.8438      0.210     23.047      0.000       4.432       5.256\n",
       "flavor              5.9368      0.393     15.125      0.000       5.167       6.706\n",
       "aftertaste          5.6823      0.323     17.607      0.000       5.050       6.315\n",
       "roaster_lat         0.2605      0.234      1.111      0.267      -0.199       0.720\n",
       "roaster_lon         0.0777      0.086      0.904      0.366      -0.091       0.246\n",
       "origin_lat         -0.7004      0.159     -4.393      0.000      -1.013      -0.388\n",
       "origin_lon         -0.1024      0.085     -1.200      0.230      -0.270       0.065\n",
       "acid                0.5672      0.506      1.121      0.263      -0.425       1.559\n",
       "acidity.1           0.2161      0.371      0.582      0.560      -0.511       0.944\n",
       "acidy              -0.3506      0.176     -1.990      0.047      -0.696      -0.005\n",
       "admired             0.9103      0.201      4.525      0.000       0.516       1.305\n",
       "aftertaste.1        0.4226      0.386      1.094      0.274      -0.335       1.180\n",
       "agave               0.6171      0.259      2.379      0.017       0.109       1.126\n",
       "aged                0.6206      0.498      1.245      0.213      -0.356       1.598\n",
       "agreeable           0.1411      0.497      0.284      0.777      -0.834       1.116\n",
       "agreeably          -0.6409      0.390     -1.644      0.100      -1.405       0.123\n",
       "alive               0.9637      0.348      2.773      0.006       0.282       1.645\n",
       "almond              2.0959      0.339      6.175      0.000       1.430       2.761\n",
       "amber               0.1361      0.136      1.003      0.316      -0.130       0.402\n",
       "amplified           0.3812      0.319      1.195      0.232      -0.244       1.007\n",
       "anise               0.1805      0.285      0.634      0.526      -0.378       0.739\n",
       "apple               0.3129      0.318      0.982      0.326      -0.312       0.937\n",
       "apricot             0.8872      0.194      4.568      0.000       0.506       1.268\n",
       "aromatic            0.2424      0.135      1.795      0.073      -0.022       0.507\n",
       "aromatically        0.1891      0.189      0.998      0.318      -0.182       0.561\n",
       "aromatics           0.4045      0.329      1.231      0.219      -0.240       1.049\n",
       "astringency        -0.1693      0.216     -0.782      0.434      -0.593       0.255\n",
       "astringent          0.7242      0.382      1.898      0.058      -0.024       1.472\n",
       "attractive         -0.8752      0.213     -4.106      0.000      -1.293      -0.457\n",
       "authority          -0.6906      0.160     -4.313      0.000      -1.005      -0.377\n",
       "b60                -0.2995      0.265     -1.131      0.258      -0.819       0.220\n",
       "b70                 0.1910      0.291      0.657      0.512      -0.379       0.762\n",
       "backgrounded        0.1934      0.805      0.240      0.810      -1.384       1.771\n",
       "baker               0.0102      0.711      0.014      0.989      -1.384       1.404\n",
       "baking             -0.1282      0.183     -0.699      0.484      -0.488       0.231\n",
       "balance             0.1350      0.168      0.804      0.422      -0.194       0.464\n",
       "balanced            0.1309      0.185      0.708      0.479      -0.232       0.494\n",
       "balancing           0.8795      0.228      3.857      0.000       0.432       1.327\n",
       "banana              0.2875      0.110      2.613      0.009       0.072       0.503\n",
       "barrel             -0.0677      0.247     -0.274      0.784      -0.553       0.417\n",
       "bay                -0.2520      0.342     -0.737      0.461      -0.922       0.418\n",
       "bean               -0.7042      0.568     -1.239      0.215      -1.819       0.410\n",
       "bergamot           -0.2039      0.371     -0.550      0.582      -0.931       0.523\n",
       "berries             0.2975      0.246      1.209      0.227      -0.185       0.780\n",
       "berry               0.3265      0.412      0.792      0.428      -0.482       1.135\n",
       "best                1.0060      0.169      5.961      0.000       0.675       1.337\n",
       "big                 1.4351      0.401      3.583      0.000       0.650       2.220\n",
       "bing                0.2931      0.196      1.495      0.135      -0.091       0.678\n",
       "bit                 0.0113      0.403      0.028      0.978      -0.779       0.801\n",
       "bitter             -0.3941      0.190     -2.070      0.039      -0.767      -0.021\n",
       "bitterish          -0.6807      0.295     -2.309      0.021      -1.259      -0.103\n",
       "bitterness         -0.8181      0.427     -1.917      0.055      -1.655       0.018\n",
       "bittersweet         1.5487      0.418      3.706      0.000       0.729       2.368\n",
       "black              -0.1445      0.161     -0.898      0.369      -0.460       0.171\n",
       "blackberry          0.3166      0.231      1.370      0.171      -0.137       0.770\n",
       "blend               0.3397      0.220      1.541      0.123      -0.092       0.772\n",
       "blood              -0.9816      0.246     -3.992      0.000      -1.464      -0.499\n",
       "bloom               0.0599      0.224      0.267      0.789      -0.380       0.499\n",
       "blooming           -0.0288      0.242     -0.119      0.905      -0.504       0.446\n",
       "blooms              0.8627      0.396      2.181      0.029       0.087       1.638\n",
       "blossom            -0.1264      0.230     -0.550      0.583      -0.577       0.325\n",
       "blueberry           0.3572      0.211      1.694      0.090      -0.056       0.770\n",
       "bodied              0.5206      0.221      2.358      0.018       0.088       0.953\n",
       "body.1              0.5155      0.172      2.990      0.003       0.178       0.854\n",
       "brandy              0.1656      0.273      0.606      0.545      -0.370       0.701\n",
       "brazil             -0.1505      0.431     -0.350      0.727      -0.995       0.694\n",
       "brewed              0.6240      1.192      0.523      0.601      -1.714       2.962\n",
       "brewer             -0.5035      0.842     -0.598      0.550      -2.154       1.147\n",
       "brewing            -0.3925      0.963     -0.407      0.684      -2.281       1.496\n",
       "bright              0.1899      0.128      1.479      0.139      -0.062       0.442\n",
       "brightly            0.0368      0.185      0.199      0.842      -0.326       0.400\n",
       "brightness         -0.6608      0.445     -1.486      0.137      -1.533       0.211\n",
       "brisk              -0.0785      0.146     -0.536      0.592      -0.366       0.209\n",
       "briskly             0.2126      0.287      0.740      0.459      -0.351       0.776\n",
       "brittle             0.4083      0.270      1.512      0.131      -0.121       0.938\n",
       "brown               0.4395      0.645      0.682      0.495      -0.824       1.703\n",
       "buoyant             0.0134      0.143      0.094      0.925      -0.266       0.293\n",
       "buoyantly           0.2938      0.297      0.990      0.322      -0.288       0.876\n",
       "burned             -2.4507      0.349     -7.032      0.000      -3.134      -1.767\n",
       "butter              0.4298      0.150      2.856      0.004       0.135       0.725\n",
       "butterscotch        1.1491      0.281      4.083      0.000       0.597       1.701\n",
       "buttery             0.5200      0.231      2.248      0.025       0.067       0.974\n",
       "byron               0.2977      0.208      1.430      0.153      -0.110       0.706\n",
       "cacao               1.3772      0.463      2.974      0.003       0.469       2.285\n",
       "called              0.2268      0.222      1.020      0.308      -0.209       0.663\n",
       "candied             0.3293      0.419      0.786      0.432      -0.492       1.151\n",
       "candy               0.1903      0.692      0.275      0.783      -1.167       1.548\n",
       "cane               -0.3910      0.453     -0.863      0.388      -1.279       0.497\n",
       "cappuccino          0.6316      0.565      1.118      0.264      -0.476       1.740\n",
       "capsule             0.3630      0.886      0.410      0.682      -1.374       2.100\n",
       "caramel             0.3176      0.162      1.965      0.049       0.001       0.634\n",
       "caramelly           0.4721      0.432      1.092      0.275      -0.375       1.320\n",
       "carbon             -0.1225      0.283     -0.433      0.665      -0.677       0.432\n",
       "cardamom           -1.4934      0.412     -3.622      0.000      -2.302      -0.685\n",
       "carob               0.0386      0.380      0.101      0.919      -0.707       0.784\n",
       "carried             0.0448      0.343      0.131      0.896      -0.628       0.718\n",
       "carries             0.0224      0.303      0.074      0.941      -0.572       0.617\n",
       "carry               0.1748      0.159      1.097      0.273      -0.138       0.487\n",
       "carrying            0.1116      0.125      0.894      0.371      -0.133       0.356\n",
       "cashew              0.4039      0.306      1.319      0.187      -0.196       1.004\n",
       "cedar               0.0646      0.204      0.316      0.752      -0.336       0.465\n",
       "cedary              0.1796      0.115      1.555      0.120      -0.047       0.406\n",
       "center              0.5215      0.265      1.967      0.049       0.002       1.041\n",
       "centered            0.2883      0.417      0.692      0.489      -0.529       1.105\n",
       "centers             0.1302      0.227      0.573      0.567      -0.316       0.576\n",
       "character          -0.0820      0.135     -0.607      0.544      -0.347       0.183\n",
       "charred             0.2653      0.195      1.360      0.174      -0.117       0.648\n",
       "cherry              0.1797      0.372      0.483      0.629      -0.550       0.909\n",
       "cherryish           0.4863      0.153      3.178      0.001       0.186       0.786\n",
       "chocolate           0.8767      0.253      3.470      0.001       0.381       1.372\n",
       "chocolaty           0.4638      0.136      3.400      0.001       0.196       0.731\n",
       "cinnamon            0.3492      0.164      2.133      0.033       0.028       0.670\n",
       "cited               0.3147      0.239      1.316      0.188      -0.154       0.783\n",
       "citrus              0.0954      0.488      0.196      0.845      -0.861       1.052\n",
       "citrusy             0.3247      0.163      1.995      0.046       0.006       0.644\n",
       "classic             0.2032      0.220      0.925      0.355      -0.228       0.634\n",
       "clean               0.7038      0.292      2.411      0.016       0.131       1.276\n",
       "cleanly             0.5443      0.181      3.009      0.003       0.190       0.899\n",
       "clear               0.2540      0.172      1.479      0.139      -0.083       0.591\n",
       "clove              -0.6064      0.327     -1.855      0.064      -1.247       0.035\n",
       "cocoa               0.3844      0.243      1.580      0.114      -0.093       0.861\n",
       "cocoaish            0.7469      0.153      4.884      0.000       0.447       1.047\n",
       "coconut             0.0768      0.211      0.363      0.716      -0.338       0.491\n",
       "coffee              0.7628      0.282      2.700      0.007       0.209       1.317\n",
       "coffees             0.2911      0.185      1.576      0.115      -0.071       0.653\n",
       "cold               -0.1233      0.373     -0.331      0.741      -0.854       0.607\n",
       "complete            0.7757      0.691      1.122      0.262      -0.579       2.131\n",
       "complex            -1.3916      0.846     -1.646      0.100      -3.049       0.266\n",
       "complexity          0.7170      0.215      3.330      0.001       0.295       1.139\n",
       "complexly           0.2943      0.125      2.355      0.019       0.049       0.539\n",
       "complicate          0.4732      0.216      2.192      0.028       0.050       0.897\n",
       "complicated         0.3617      0.205      1.766      0.077      -0.040       0.763\n",
       "complicates        -0.2696      0.373     -0.722      0.470      -1.001       0.462\n",
       "complicating       -0.3606      0.155     -2.323      0.020      -0.665      -0.056\n",
       "complication       -0.0480      0.342     -0.141      0.888      -0.718       0.622\n",
       "complications       0.2767      0.284      0.975      0.330      -0.280       0.833\n",
       "concord            -0.3655      0.255     -1.434      0.152      -0.865       0.134\n",
       "consolidates        0.0185      0.267      0.069      0.945      -0.505       0.542\n",
       "consolidating       0.5403      0.416      1.298      0.195      -0.276       1.357\n",
       "continued           0.0041      0.098      0.041      0.967      -0.189       0.197\n",
       "continuing          0.0540      0.270      0.200      0.841      -0.474       0.582\n",
       "cools               0.3463      0.171      2.031      0.042       0.012       0.681\n",
       "creamy              0.0719      0.215      0.334      0.739      -0.350       0.494\n",
       "crisp               0.8470      0.577      1.468      0.142      -0.284       1.978\n",
       "crisply             0.0616      0.217      0.284      0.777      -0.364       0.487\n",
       "cupper             -0.0874      0.147     -0.595      0.552      -0.375       0.200\n",
       "cupping             0.3727      0.112      3.315      0.001       0.152       0.593\n",
       "cups                0.6008      0.234      2.562      0.010       0.141       1.061\n",
       "currant             0.1777      0.387      0.459      0.646      -0.581       0.937\n",
       "cut                 1.2475      0.342      3.651      0.000       0.578       1.917\n",
       "dark               -3.6816      0.443     -8.305      0.000      -4.551      -2.812\n",
       "date                0.4110      0.209      1.964      0.050       0.001       0.821\n",
       "deep               -0.1001      0.217     -0.462      0.644      -0.525       0.325\n",
       "deepen              0.0149      0.152      0.098      0.922      -0.284       0.313\n",
       "deepening           0.4422      0.193      2.294      0.022       0.064       0.820\n",
       "deepens             0.3825      0.152      2.514      0.012       0.084       0.681\n",
       "deeply              0.0136      0.304      0.045      0.964      -0.582       0.609\n",
       "delicate            0.4511      0.312      1.445      0.149      -0.161       1.063\n",
       "delicately          0.7079      0.292      2.427      0.015       0.136       1.280\n",
       "depth               0.2614      0.123      2.131      0.033       0.021       0.502\n",
       "described           0.1371      0.159      0.862      0.389      -0.175       0.449\n",
       "device              0.2952      0.130      2.265      0.024       0.040       0.551\n",
       "dimension           0.6595      0.243      2.713      0.007       0.183       1.136\n",
       "dimensioned         1.1463      0.515      2.227      0.026       0.137       2.155\n",
       "displayed          -1.1298      1.202     -0.940      0.347      -3.487       1.228\n",
       "displays           -0.7590      0.292     -2.595      0.009      -1.332      -0.186\n",
       "distinct           -0.8076      0.293     -2.755      0.006      -1.382      -0.233\n",
       "distinctive         0.3806      0.225      1.693      0.091      -0.060       0.821\n",
       "distinctly         -0.1445      0.249     -0.580      0.562      -0.633       0.344\n",
       "dominate           -0.1813      0.260     -0.698      0.485      -0.690       0.328\n",
       "dominated          -0.2121      0.177     -1.199      0.231      -0.559       0.135\n",
       "dominates          -0.3858      0.196     -1.965      0.049      -0.771      -0.001\n",
       "dominating         -0.1741      0.199     -0.875      0.382      -0.564       0.216\n",
       "dried               0.0869      0.301      0.289      0.773      -0.503       0.677\n",
       "drink               0.0976      0.148      0.660      0.509      -0.192       0.387\n",
       "driven              0.6476      0.827      0.783      0.434      -0.974       2.270\n",
       "dry                 0.0942      0.261      0.361      0.718      -0.417       0.605\n",
       "drying             -0.1449      0.141     -1.030      0.303      -0.421       0.131\n",
       "dusk               -0.3752      0.119     -3.147      0.002      -0.609      -0.141\n",
       "earth               0.7918      0.324      2.441      0.015       0.156       1.428\n",
       "earthy              0.7676      0.264      2.903      0.004       0.249       1.286\n",
       "edge                0.0491      0.339      0.145      0.885      -0.616       0.714\n",
       "edged               0.7075      0.223      3.166      0.002       0.269       1.146\n",
       "effervescent       -0.0758      0.267     -0.284      0.777      -0.600       0.449\n",
       "elegant             0.7131      0.242      2.943      0.003       0.238       1.188\n",
       "elegantly           0.1603      0.229      0.702      0.483      -0.288       0.608\n",
       "emerge              0.0004      0.257      0.002      0.999      -0.504       0.505\n",
       "emerges            -0.3851      0.260     -1.482      0.138      -0.895       0.124\n",
       "engaging            0.2405      0.267      0.899      0.368      -0.284       0.765\n",
       "enveloped           0.7972      0.331      2.409      0.016       0.148       1.446\n",
       "enveloping          0.3082      0.322      0.957      0.338      -0.323       0.939\n",
       "especially          0.5926      0.436      1.361      0.174      -0.261       1.446\n",
       "espresso            1.6377      0.525      3.120      0.002       0.609       2.667\n",
       "ethan              -0.0870      0.298     -0.292      0.770      -0.671       0.497\n",
       "evaluated          -0.2972      0.324     -0.916      0.360      -0.933       0.339\n",
       "excellent           0.3024      0.292      1.036      0.300      -0.270       0.875\n",
       "exhilarating        0.4727      0.303      1.559      0.119      -0.122       1.067\n",
       "exhilaratingly      0.5222      0.301      1.733      0.083      -0.069       1.113\n",
       "exotic              0.9812      0.359      2.733      0.006       0.277       1.685\n",
       "explicit            0.1573      0.267      0.590      0.555      -0.366       0.680\n",
       "expressed           0.4719      0.254      1.855      0.064      -0.027       0.971\n",
       "fade                0.0553      0.262      0.212      0.832      -0.458       0.568\n",
       "fades               0.1319      0.343      0.385      0.700      -0.540       0.804\n",
       "fading             -1.0680      0.263     -4.055      0.000      -1.584      -0.552\n",
       "faint              -0.7892      0.501     -1.575      0.115      -1.772       0.193\n",
       "faintly            -0.2426      0.242     -1.004      0.315      -0.716       0.231\n",
       "fallen             -0.2219      0.432     -0.513      0.608      -1.070       0.626\n",
       "far                -1.3211      0.276     -4.778      0.000      -1.863      -0.779\n",
       "felt               -1.1621      0.368     -3.161      0.002      -1.883      -0.441\n",
       "ferment            -0.0058      0.394     -0.015      0.988      -0.779       0.767\n",
       "fermented           0.4442      0.311      1.428      0.153      -0.166       1.054\n",
       "fermenty            0.8275      0.224      3.689      0.000       0.388       1.267\n",
       "fig                 0.0033      0.175      0.019      0.985      -0.340       0.346\n",
       "fine               -1.6699      0.309     -5.407      0.000      -2.275      -1.064\n",
       "fir                -0.2438      0.136     -1.799      0.072      -0.510       0.022\n",
       "flat               -0.3780      0.267     -1.414      0.157      -0.902       0.146\n",
       "flavor.1            0.3737      0.104      3.596      0.000       0.170       0.577\n",
       "flavors             0.0476      0.180      0.265      0.791      -0.305       0.400\n",
       "floral              0.8194      0.245      3.345      0.001       0.339       1.300\n",
       "florals             0.5099      0.667      0.765      0.444      -0.797       1.817\n",
       "flower              0.7072      0.142      4.968      0.000       0.428       0.986\n",
       "flowering           0.3699      0.342      1.082      0.279      -0.300       1.040\n",
       "flowers            -0.0435      0.189     -0.230      0.818      -0.415       0.328\n",
       "followed            0.6321      0.291      2.174      0.030       0.062       1.202\n",
       "forward            -0.0803      0.279     -0.288      0.773      -0.626       0.466\n",
       "fragrant            0.2637      0.228      1.159      0.247      -0.183       0.710\n",
       "framed              1.2677      0.404      3.140      0.002       0.476       2.059\n",
       "frankincense        0.1407      0.186      0.755      0.450      -0.224       0.506\n",
       "freesia             0.3989      0.334      1.193      0.233      -0.257       1.054\n",
       "fresh               0.5196      0.143      3.634      0.000       0.239       0.800\n",
       "freshly             0.2636      0.246      1.074      0.283      -0.218       0.745\n",
       "fruit               0.3648      0.209      1.749      0.080      -0.044       0.774\n",
       "fruity              0.2417      0.235      1.029      0.304      -0.219       0.702\n",
       "fudge               0.2339      0.162      1.448      0.148      -0.083       0.551\n",
       "gardenia            0.3278      0.136      2.413      0.016       0.061       0.594\n",
       "gentle              1.0949      0.315      3.471      0.001       0.477       1.713\n",
       "gently              0.3037      0.298      1.020      0.308      -0.280       0.887\n",
       "ginger             -0.3021      0.434     -0.696      0.487      -1.153       0.549\n",
       "goji               -0.3575      0.243     -1.472      0.141      -0.834       0.119\n",
       "good               -1.2870      0.355     -3.630      0.000      -1.982      -0.592\n",
       "grace               0.2216      0.250      0.888      0.375      -0.268       0.711\n",
       "grape               0.4394      0.172      2.551      0.011       0.102       0.777\n",
       "grapefruit          0.7124      0.434      1.642      0.101      -0.138       1.563\n",
       "grappa              0.4943      0.591      0.836      0.403      -0.664       1.653\n",
       "grass               0.6236      0.315      1.982      0.048       0.007       1.240\n",
       "great              -0.2827      0.258     -1.096      0.273      -0.788       0.223\n",
       "green               0.4470      0.236      1.892      0.059      -0.016       0.910\n",
       "guava              -4.3917      0.411    -10.685      0.000      -5.198      -3.586\n",
       "hard                0.0302      0.314      0.096      0.923      -0.586       0.646\n",
       "harmonious          0.2337      0.148      1.575      0.115      -0.057       0.525\n",
       "hazelnut            0.5933      0.526      1.127      0.260      -0.439       1.626\n",
       "heavy              -0.5592      0.274     -2.038      0.042      -1.097      -0.021\n",
       "herb                0.7839      0.306      2.563      0.010       0.184       1.384\n",
       "herbaceous          0.0694      0.234      0.297      0.767      -0.389       0.528\n",
       "herbal             -0.2842      0.304     -0.935      0.350      -0.880       0.312\n",
       "herbs              -0.1880      0.319     -0.590      0.555      -0.813       0.437\n",
       "herby               0.1025      0.317      0.323      0.747      -0.520       0.725\n",
       "hibiscus            0.7984      0.172      4.653      0.000       0.462       1.135\n",
       "high               -0.1069      0.181     -0.592      0.554      -0.461       0.247\n",
       "hint               -0.0455      0.127     -0.359      0.720      -0.294       0.203\n",
       "hints               0.4792      0.166      2.884      0.004       0.153       0.805\n",
       "honey               0.3049      0.303      1.007      0.314      -0.289       0.899\n",
       "honeyish            0.4174      0.201      2.076      0.038       0.023       0.812\n",
       "honeysuckle        -0.0242      0.301     -0.080      0.936      -0.615       0.566\n",
       "hop                -0.7413      0.308     -2.406      0.016      -1.346      -0.137\n",
       "hot                -0.4779      0.493     -0.969      0.332      -1.444       0.489\n",
       "impression          0.0057      0.371      0.015      0.988      -0.721       0.732\n",
       "impressive          0.1434      0.205      0.698      0.485      -0.259       0.546\n",
       "impressively        0.3662      0.242      1.511      0.131      -0.109       0.841\n",
       "influenced          0.9392      0.362      2.596      0.009       0.230       1.648\n",
       "integrated          0.7996      0.302      2.646      0.008       0.207       1.392\n",
       "intense             0.3563      0.207      1.723      0.085      -0.049       0.762\n",
       "intensely           0.2445      0.300      0.815      0.415      -0.344       0.833\n",
       "intensity          -0.4625      0.400     -1.155      0.248      -1.248       0.323\n",
       "interesting         0.2541      0.165      1.537      0.124      -0.070       0.578\n",
       "intricate           0.3496      0.266      1.315      0.188      -0.171       0.871\n",
       "intricately         0.4992      0.329      1.519      0.129      -0.145       1.144\n",
       "intriguing          0.3181      0.406      0.783      0.434      -0.479       1.115\n",
       "jam                 0.3749      0.187      2.006      0.045       0.008       0.741\n",
       "jasmine             1.2109      0.515      2.349      0.019       0.200       2.222\n",
       "juicy               0.2752      0.131      2.093      0.036       0.017       0.533\n",
       "juniper             0.1107      0.349      0.317      0.751      -0.573       0.795\n",
       "just               -1.0510      0.348     -3.022      0.003      -1.733      -0.369\n",
       "ken                 1.7360      0.385      4.514      0.000       0.982       2.490\n",
       "kenya               2.5563      1.518      1.684      0.092      -0.419       5.532\n",
       "keurig             -0.2922      0.365     -0.801      0.423      -1.008       0.423\n",
       "key                 1.4538      0.501      2.899      0.004       0.471       2.437\n",
       "laden               0.5495      0.412      1.333      0.182      -0.258       1.357\n",
       "lasting             0.1659      0.233      0.711      0.477      -0.291       0.623\n",
       "lavender            0.2772      0.174      1.595      0.111      -0.064       0.618\n",
       "layered             0.0226      0.233      0.097      0.923      -0.435       0.480\n",
       "lead                0.0470      0.238      0.197      0.844      -0.420       0.514\n",
       "leads               0.1505      0.296      0.508      0.611      -0.430       0.731\n",
       "leaf                0.0590      0.141      0.419      0.675      -0.217       0.335\n",
       "lean                0.8662      0.474      1.827      0.068      -0.063       1.796\n",
       "leaning            -0.5858      0.303     -1.933      0.053      -1.180       0.008\n",
       "leanish             0.2545      0.216      1.176      0.240      -0.170       0.679\n",
       "leather            -0.8472      0.296     -2.859      0.004      -1.428      -0.266\n",
       "leaves              0.2725      0.307      0.887      0.375      -0.330       0.875\n",
       "lemon               0.6587      0.410      1.605      0.109      -0.146       1.463\n",
       "lemony              0.2414      0.139      1.740      0.082      -0.031       0.513\n",
       "light               0.2974      0.284      1.048      0.295      -0.259       0.854\n",
       "lightly            -0.3716      0.187     -1.990      0.047      -0.738      -0.005\n",
       "like               -0.2311      0.133     -1.734      0.083      -0.492       0.030\n",
       "liked              -0.3400      0.125     -2.725      0.006      -0.585      -0.095\n",
       "lilac               0.3072      0.183      1.678      0.094      -0.052       0.666\n",
       "lily                0.2284      0.188      1.218      0.223      -0.139       0.596\n",
       "lime                0.4429      0.241      1.841      0.066      -0.029       0.915\n",
       "limited            -0.5312      0.481     -1.103      0.270      -1.475       0.413\n",
       "linger              0.2809      0.256      1.097      0.273      -0.221       0.783\n",
       "lingering           0.0767      0.191      0.401      0.688      -0.298       0.452\n",
       "lingers             0.3041      0.221      1.377      0.168      -0.129       0.737\n",
       "little             -0.5005      0.284     -1.764      0.078      -1.057       0.056\n",
       "lively             -0.0165      0.148     -0.112      0.911      -0.306       0.273\n",
       "long                0.6841      0.123      5.567      0.000       0.443       0.925\n",
       "lovely              0.0723      0.367      0.197      0.844      -0.646       0.791\n",
       "low                 0.3970      0.218      1.823      0.068      -0.030       0.824\n",
       "lush                0.4369      0.158      2.761      0.006       0.127       0.747\n",
       "lushly              0.4225      0.223      1.897      0.058      -0.014       0.859\n",
       "lychee              0.2875      0.273      1.055      0.292      -0.247       0.822\n",
       "lyric               0.6615      0.318      2.079      0.038       0.038       1.286\n",
       "lyrically           0.3483      0.305      1.143      0.253      -0.249       0.946\n",
       "macadamia           0.0127      0.447      0.028      0.977      -0.864       0.890\n",
       "magnolia            0.1759      0.168      1.044      0.296      -0.154       0.506\n",
       "maintains           0.4993      0.328      1.521      0.128      -0.144       1.143\n",
       "malt                0.3689      0.441      0.837      0.403      -0.495       1.233\n",
       "malty               1.4242      0.397      3.584      0.000       0.645       2.203\n",
       "mango               0.2839      0.203      1.400      0.161      -0.114       0.681\n",
       "maple               0.8116      0.531      1.527      0.127      -0.230       1.854\n",
       "marjoram            0.4084      0.242      1.688      0.091      -0.066       0.883\n",
       "medium              0.1925      0.157      1.228      0.219      -0.115       0.500\n",
       "melon               0.4517      0.326      1.386      0.166      -0.187       1.091\n",
       "mesquite            0.2633      0.340      0.775      0.439      -0.403       0.930\n",
       "meyer               0.5324      0.307      1.737      0.082      -0.068       1.133\n",
       "mid                 0.0419      0.407      0.103      0.918      -0.756       0.840\n",
       "miguel              1.6655      0.455      3.658      0.000       0.773       2.558\n",
       "mild               -0.0233      0.382     -0.061      0.951      -0.772       0.726\n",
       "mildly             -0.2660      0.247     -1.077      0.282      -0.750       0.218\n",
       "milk                0.6795      0.213      3.195      0.001       0.263       1.096\n",
       "mint                0.7124      0.432      1.648      0.099      -0.135       1.560\n",
       "minty               0.6156      0.338      1.820      0.069      -0.048       1.279\n",
       "moist               0.3515      0.322      1.092      0.275      -0.280       0.983\n",
       "molasses            0.3738      0.189      1.981      0.048       0.004       0.744\n",
       "mulberry            0.3314      0.298      1.111      0.267      -0.254       0.916\n",
       "multi               0.3282      0.247      1.327      0.185      -0.157       0.813\n",
       "mushroom            0.4942      0.320      1.546      0.122      -0.133       1.121\n",
       "musk               -0.0681      0.236     -0.289      0.773      -0.530       0.394\n",
       "musky               0.1787      0.272      0.657      0.511      -0.354       0.712\n",
       "mustiness          -1.0809      0.393     -2.749      0.006      -1.852      -0.310\n",
       "musty               0.1019      0.210      0.484      0.628      -0.311       0.515\n",
       "muted               0.3501      0.264      1.325      0.185      -0.168       0.868\n",
       "myrrh               0.3424      0.170      2.018      0.044       0.010       0.675\n",
       "narcissus           0.5554      0.343      1.620      0.105      -0.117       1.227\n",
       "nectar              0.3814      0.277      1.378      0.168      -0.161       0.924\n",
       "nectarine          -0.1965      0.174     -1.126      0.260      -0.539       0.146\n",
       "nib                 0.3085      0.230      1.340      0.180      -0.143       0.760\n",
       "nice                0.0045      0.262      0.017      0.986      -0.508       0.517\n",
       "nicely             -0.2823      0.237     -1.191      0.234      -0.747       0.182\n",
       "night              -0.1114      0.187     -0.596      0.551      -0.478       0.255\n",
       "nominated           0.2944      0.120      2.454      0.014       0.059       0.530\n",
       "nose                0.1414      0.214      0.661      0.509      -0.278       0.561\n",
       "note               -0.1997      0.238     -0.840      0.401      -0.666       0.266\n",
       "notes               0.3382      0.212      1.594      0.111      -0.078       0.754\n",
       "nougat             -0.0185      0.171     -0.108      0.914      -0.354       0.317\n",
       "nuance              0.5327      0.429      1.243      0.214      -0.307       1.373\n",
       "nuanced             0.4283      0.364      1.177      0.239      -0.285       1.142\n",
       "nut                -0.9985      0.368     -2.710      0.007      -1.721      -0.276\n",
       "nutella             0.4089      0.216      1.889      0.059      -0.016       0.833\n",
       "nutmeg              0.1347      0.158      0.854      0.393      -0.175       0.444\n",
       "nuts               -0.0435      0.271     -0.161      0.872      -0.575       0.488\n",
       "nutty               0.6208      0.166      3.735      0.000       0.295       0.947\n",
       "oak                 0.0866      0.197      0.440      0.660      -0.299       0.473\n",
       "opulent             0.5496      0.394      1.396      0.163      -0.222       1.322\n",
       "opulently           0.0573      0.267      0.215      0.830      -0.466       0.580\n",
       "orange             -0.3235      1.018     -0.318      0.751      -2.320       1.673\n",
       "orangy             -0.8124      0.696     -1.167      0.243      -2.177       0.552\n",
       "orchid             -0.6641      0.500     -1.328      0.184      -1.645       0.316\n",
       "original            0.3482      0.296      1.178      0.239      -0.232       0.928\n",
       "ounce              -0.1498      0.135     -1.110      0.267      -0.414       0.115\n",
       "ounces             -0.1180      0.256     -0.461      0.645      -0.620       0.384\n",
       "owing              -0.1140      0.342     -0.333      0.739      -0.785       0.557\n",
       "panelists           0.0361      0.355      0.102      0.919      -0.660       0.732\n",
       "papaya              0.2041      0.470      0.434      0.664      -0.717       1.125\n",
       "particular          0.2382      0.161      1.477      0.140      -0.078       0.554\n",
       "particularly        0.3529      0.201      1.758      0.079      -0.041       0.747\n",
       "parts               0.0444      0.320      0.139      0.890      -0.583       0.671\n",
       "passion            -0.0062      0.407     -0.015      0.988      -0.804       0.792\n",
       "passionfruit        0.3912      0.258      1.518      0.129      -0.114       0.897\n",
       "peach               0.1607      0.371      0.433      0.665      -0.567       0.888\n",
       "pear                0.6476      0.286      2.266      0.024       0.087       1.208\n",
       "pecan               0.0961      0.208      0.463      0.644      -0.311       0.504\n",
       "peel                0.6682      0.353      1.894      0.058      -0.024       1.360\n",
       "peppercorn          1.1911      0.348      3.423      0.001       0.509       1.873\n",
       "perfectly          -0.0722      0.163     -0.444      0.657      -0.391       0.247\n",
       "persimmon           0.0271      0.225      0.121      0.904      -0.414       0.469\n",
       "persist             0.5905      0.369      1.602      0.109      -0.132       1.313\n",
       "persistence         0.5695      0.315      1.808      0.071      -0.048       1.187\n",
       "persistent          0.1124      0.249      0.451      0.652      -0.376       0.601\n",
       "persists            0.0333      0.218      0.153      0.879      -0.395       0.461\n",
       "pert                0.0631      0.374      0.169      0.866      -0.670       0.797\n",
       "pie                 0.1320      0.277      0.477      0.633      -0.411       0.675\n",
       "pine               -0.7010      0.948     -0.739      0.460      -2.560       1.158\n",
       "pineapple           1.0411      0.320      3.257      0.001       0.414       1.668\n",
       "pink                0.6127      0.205      2.995      0.003       0.212       1.014\n",
       "pipe                0.3857      0.192      2.013      0.044       0.010       0.761\n",
       "pistachio           0.0187      0.226      0.083      0.934      -0.424       0.462\n",
       "platinum            0.3802      0.161      2.366      0.018       0.065       0.695\n",
       "pleasant            0.5145      0.340      1.516      0.130      -0.151       1.180\n",
       "pleasantly         -0.0096      0.194     -0.050      0.960      -0.389       0.370\n",
       "pleasing           -0.0369      0.110     -0.336      0.737      -0.252       0.179\n",
       "pleasingly         -0.0969      0.925     -0.105      0.917      -1.910       1.716\n",
       "plum                0.4757      0.212      2.249      0.025       0.061       0.890\n",
       "plumeria           -0.2378      0.608     -0.391      0.696      -1.429       0.954\n",
       "plump              -0.2777      0.247     -1.123      0.261      -0.763       0.207\n",
       "plush              -0.5221      0.317     -1.645      0.100      -1.144       0.100\n",
       "pod                 0.5610      0.254      2.212      0.027       0.064       1.058\n",
       "pomegranate        -0.3124      0.445     -0.702      0.483      -1.185       0.560\n",
       "port                0.0238      0.273      0.087      0.931      -0.512       0.560\n",
       "powder              1.3263      0.429      3.090      0.002       0.485       2.168\n",
       "power              -0.1052      0.457     -0.230      0.818      -1.001       0.790\n",
       "powerful            0.3375      0.866      0.390      0.697      -1.361       2.036\n",
       "presence           -0.2423      0.528     -0.459      0.646      -1.278       0.793\n",
       "probably            0.9734      0.202      4.822      0.000       0.578       1.369\n",
       "produce             0.0774      0.322      0.241      0.810      -0.553       0.708\n",
       "produced           -0.1575      0.241     -0.654      0.513      -0.629       0.314\n",
       "profile             0.4300      0.302      1.422      0.155      -0.163       1.023\n",
       "promise            -2.0942      0.305     -6.866      0.000      -2.692      -1.496\n",
       "pronounced          0.1113      0.264      0.421      0.673      -0.407       0.629\n",
       "prune               0.3266      0.132      2.479      0.013       0.068       0.585\n",
       "pruny               0.3032      0.273      1.110      0.267      -0.233       0.839\n",
       "pungency            0.8300      0.247      3.358      0.001       0.345       1.315\n",
       "pungent             0.2449      0.396      0.618      0.537      -0.532       1.022\n",
       "pungently           0.2311      0.149      1.554      0.120      -0.060       0.523\n",
       "pure                0.3149      0.151      2.092      0.037       0.020       0.610\n",
       "quickly            -0.5555      0.314     -1.767      0.077      -1.172       0.061\n",
       "quiet               0.2895      0.204      1.423      0.155      -0.110       0.689\n",
       "quietly             0.2967      0.384      0.772      0.440      -0.456       1.050\n",
       "quite               0.1574      0.260      0.604      0.546      -0.353       0.668\n",
       "raisin             -0.0566      0.338     -0.167      0.867      -0.720       0.607\n",
       "raisiny             0.3971      0.183      2.165      0.030       0.037       0.757\n",
       "range              -0.0230      0.363     -0.063      0.950      -0.735       0.689\n",
       "raspberry          -0.6823      0.501     -1.361      0.174      -1.666       0.301\n",
       "rating              0.0588      0.269      0.219      0.827      -0.468       0.585\n",
       "raw                 0.1765      0.323      0.547      0.585      -0.456       0.809\n",
       "read                0.7792      0.402      1.937      0.053      -0.009       1.568\n",
       "reader             -0.3179      0.306     -1.039      0.299      -0.918       0.282\n",
       "reads               0.4626      1.002      0.462      0.644      -1.503       2.428\n",
       "ready               0.1225      0.213      0.575      0.565      -0.295       0.540\n",
       "red                 0.0732      0.267      0.275      0.784      -0.449       0.596\n",
       "redolent            0.0717      0.328      0.219      0.827      -0.571       0.715\n",
       "redwood             0.5052      0.314      1.611      0.107      -0.110       1.120\n",
       "refreshing         -0.0644      0.344     -0.187      0.851      -0.738       0.609\n",
       "relatively         -0.6204      0.274     -2.267      0.023      -1.157      -0.084\n",
       "remains            -0.0519      0.346     -0.150      0.881      -0.731       0.627\n",
       "resonance           0.2080      0.109      1.902      0.057      -0.006       0.422\n",
       "resonant            0.4524      0.285      1.587      0.113      -0.106       1.011\n",
       "resonantly          0.4185      0.318      1.314      0.189      -0.206       1.043\n",
       "resonate            0.1137      0.318      0.358      0.720      -0.509       0.736\n",
       "resonates          -0.3838      0.299     -1.282      0.200      -0.971       0.203\n",
       "resurface           0.3384      0.276      1.227      0.220      -0.202       0.879\n",
       "resurfacing         0.4497      0.115      3.912      0.000       0.224       0.675\n",
       "rhododendron        0.3743      0.106      3.518      0.000       0.166       0.583\n",
       "rich                0.0866      0.284      0.305      0.760      -0.470       0.643\n",
       "richly              0.2964      0.296      1.003      0.316      -0.283       0.876\n",
       "richness            0.2867      0.168      1.706      0.088      -0.043       0.616\n",
       "rings              -0.3172      0.219     -1.449      0.147      -0.746       0.112\n",
       "ripe                0.1144      0.174      0.659      0.510      -0.226       0.455\n",
       "roast               0.8158      0.290      2.816      0.005       0.248       1.384\n",
       "roasted             0.2486      0.234      1.064      0.287      -0.209       0.706\n",
       "roastiness          0.3068      0.197      1.558      0.119      -0.079       0.693\n",
       "roasty             -0.7752      0.388     -2.000      0.046      -1.535      -0.015\n",
       "rose               -0.2192      0.267     -0.822      0.411      -0.742       0.304\n",
       "rough               0.0536      0.138      0.389      0.697      -0.217       0.324\n",
       "round               0.1547      0.198      0.780      0.436      -0.234       0.544\n",
       "rounded             0.0265      0.184      0.144      0.885      -0.334       0.387\n",
       "rounding            0.1973      0.175      1.130      0.259      -0.145       0.540\n",
       "roundly             0.1438      0.198      0.727      0.467      -0.244       0.532\n",
       "rounds              0.6327      0.363      1.745      0.081      -0.078       1.344\n",
       "rum                -0.1511      0.500     -0.302      0.762      -1.131       0.829\n",
       "rye                 0.0816      0.325      0.251      0.802      -0.555       0.718\n",
       "sage               -1.0513      0.360     -2.920      0.004      -1.757      -0.345\n",
       "salted             -0.3125      0.287     -1.089      0.276      -0.875       0.250\n",
       "salty              -1.4160      0.494     -2.867      0.004      -2.384      -0.448\n",
       "sample              0.2781      0.137      2.025      0.043       0.009       0.547\n",
       "sandalwood         -0.2376      0.105     -2.274      0.023      -0.443      -0.033\n",
       "sassafras           0.2901      0.296      0.980      0.327      -0.290       0.871\n",
       "satiny              0.5064      0.285      1.779      0.075      -0.052       1.064\n",
       "satisfying          0.4972      0.190      2.616      0.009       0.125       0.870\n",
       "saturated           0.2542      0.277      0.918      0.359      -0.289       0.797\n",
       "saturates           0.0296      0.138      0.214      0.831      -0.242       0.301\n",
       "savory             -0.3185      0.574     -0.555      0.579      -1.444       0.807\n",
       "scaled              0.0089      0.203      0.044      0.965      -0.388       0.406\n",
       "scorched            0.3825      0.254      1.505      0.132      -0.116       0.881\n",
       "semi               -0.0115      0.274     -0.042      0.966      -0.548       0.525\n",
       "sensation           0.4572      1.657      0.276      0.783      -2.791       3.706\n",
       "serve               1.5432      1.315      1.174      0.241      -1.034       4.121\n",
       "serving            -0.5729      0.393     -1.457      0.145      -1.344       0.198\n",
       "shadow             -2.0501      0.312     -6.570      0.000      -2.662      -1.438\n",
       "shallow            -2.1043      0.396     -5.313      0.000      -2.881      -1.328\n",
       "sharp              -1.5187      0.358     -4.239      0.000      -2.221      -0.816\n",
       "sharpness          -0.1321      0.332     -0.398      0.691      -0.783       0.518\n",
       "shifting            0.8846      0.254      3.484      0.000       0.387       1.382\n",
       "shimmer            -0.4472      0.113     -3.956      0.000      -0.669      -0.226\n",
       "short               0.1148      0.392      0.293      0.770      -0.655       0.884\n",
       "shot               -0.0383      0.103     -0.371      0.711      -0.241       0.164\n",
       "silky              -0.5856      0.190     -3.074      0.002      -0.959      -0.212\n",
       "simple              0.0295      0.208      0.142      0.887      -0.379       0.438\n",
       "simplifies         -0.1514      0.326     -0.464      0.643      -0.791       0.488\n",
       "simplify           -1.4182      0.820     -1.730      0.084      -3.025       0.189\n",
       "singed             -0.4510      0.538     -0.838      0.402      -1.506       0.604\n",
       "single              0.4441      0.204      2.182      0.029       0.045       0.843\n",
       "size                0.3368      0.200      1.686      0.092      -0.055       0.728\n",
       "slight              0.1423      0.254      0.561      0.575      -0.355       0.640\n",
       "slightly           -0.2937      0.341     -0.861      0.389      -0.962       0.375\n",
       "small               0.4133      0.243      1.704      0.089      -0.062       0.889\n",
       "smoke               0.0246      0.098      0.252      0.801      -0.167       0.216\n",
       "smoky               0.2561      0.234      1.095      0.273      -0.202       0.714\n",
       "smooth              0.4064      0.189      2.145      0.032       0.035       0.778\n",
       "smoothly           -0.6767      0.287     -2.356      0.019      -1.240      -0.114\n",
       "soft                0.4635      0.361      1.283      0.200      -0.245       1.172\n",
       "soften              0.0549      0.310      0.177      0.859      -0.552       0.662\n",
       "softened           -0.2937      0.233     -1.258      0.208      -0.751       0.164\n",
       "softening           0.2451      0.201      1.221      0.222      -0.148       0.639\n",
       "softens            -0.5461      0.268     -2.036      0.042      -1.072      -0.020\n",
       "softly             -0.5390      0.395     -1.364      0.173      -1.314       0.236\n",
       "solid              -0.0170      0.278     -0.061      0.951      -0.563       0.529\n",
       "sort               -1.3963      0.427     -3.273      0.001      -2.233      -0.560\n",
       "spearmint           0.3884      0.362      1.074      0.283      -0.321       1.097\n",
       "spice               0.1236      0.196      0.631      0.528      -0.261       0.508\n",
       "spices              0.1936      0.311      0.623      0.533      -0.416       0.803\n",
       "spicy               0.2039      0.206      0.989      0.323      -0.201       0.608\n",
       "star                0.3888      0.302      1.285      0.199      -0.204       0.982\n",
       "stone              -0.0553      0.251     -0.220      0.826      -0.547       0.436\n",
       "straight           -0.2575      0.342     -0.754      0.451      -0.927       0.412\n",
       "straightforward    -0.2390      0.254     -0.940      0.347      -0.738       0.260\n",
       "strawberry          0.2725      0.210      1.300      0.194      -0.138       0.683\n",
       "structure          -0.1309      0.131     -0.998      0.318      -0.388       0.126\n",
       "style               0.1115      0.308      0.362      0.718      -0.493       0.716\n",
       "subdued            -0.7553      0.335     -2.258      0.024      -1.411      -0.099\n",
       "substantial         0.0231      0.254      0.091      0.927      -0.474       0.521\n",
       "subtle             -0.7370      0.403     -1.831      0.067      -1.526       0.052\n",
       "subtly              0.9848      0.286      3.445      0.001       0.424       1.545\n",
       "sugar               0.1203      0.181      0.666      0.506      -0.234       0.474\n",
       "sugary             -0.0773      0.632     -0.122      0.903      -1.316       1.162\n",
       "suggest             1.0795      0.300      3.595      0.000       0.491       1.668\n",
       "suggesting          0.6856      0.338      2.031      0.042       0.024       1.347\n",
       "suggestion         -0.0085      0.263     -0.032      0.974      -0.524       0.507\n",
       "suggestions        -0.0397      0.203     -0.195      0.845      -0.438       0.359\n",
       "suggests           -0.1120      0.168     -0.668      0.504      -0.441       0.217\n",
       "sumatra             0.2093      0.277      0.754      0.451      -0.335       0.753\n",
       "superb              0.7611      0.351      2.167      0.030       0.072       1.450\n",
       "support             0.8032      0.294      2.736      0.006       0.228       1.379\n",
       "supported           0.1387      0.353      0.393      0.694      -0.553       0.831\n",
       "supporting          0.0957      0.124      0.772      0.440      -0.147       0.339\n",
       "surfaces           -0.8807      0.274     -3.212      0.001      -1.418      -0.343\n",
       "surprising          0.3754      0.239      1.569      0.117      -0.094       0.845\n",
       "surprisingly        0.1984      0.350      0.566      0.571      -0.489       0.885\n",
       "sustained           0.2757      0.119      2.307      0.021       0.041       0.510\n",
       "sweet               0.1520      0.233      0.653      0.514      -0.304       0.608\n",
       "sweetens            0.2073      0.401      0.518      0.605      -0.578       0.993\n",
       "sweetly             0.1494      0.118      1.265      0.206      -0.082       0.381\n",
       "sweetness           0.1197      0.247      0.484      0.628      -0.365       0.605\n",
       "syrup              -0.5728      0.527     -1.087      0.277      -1.606       0.460\n",
       "syrupy             -0.2637      0.152     -1.739      0.082      -0.561       0.034\n",
       "tamarind            0.4873      0.234      2.087      0.037       0.029       0.945\n",
       "tangerine           0.3708      0.171      2.171      0.030       0.036       0.706\n",
       "tangy               0.3335      0.409      0.815      0.415      -0.469       1.136\n",
       "tart               -0.0119      0.134     -0.089      0.929      -0.274       0.251\n",
       "tartly             -0.1709      0.223     -0.766      0.444      -0.608       0.267\n",
       "taste              -0.8590      0.345     -2.492      0.013      -1.535      -0.183\n",
       "taster              0.1036      0.531      0.195      0.845      -0.937       1.144\n",
       "tea                -0.3699      0.324     -1.140      0.254      -1.006       0.266\n",
       "ted                 0.4856      0.480      1.011      0.312      -0.456       1.427\n",
       "tested             -1.2060      0.706     -1.709      0.088      -2.589       0.177\n",
       "think               0.1954      0.298      0.656      0.512      -0.389       0.779\n",
       "throughline        -0.0545      0.334     -0.163      0.870      -0.709       0.600\n",
       "thyme               0.2655      0.217      1.224      0.221      -0.160       0.691\n",
       "tickle              0.9421      0.339      2.776      0.006       0.277       1.607\n",
       "tight              -1.2351      0.527     -2.346      0.019      -2.267      -0.203\n",
       "toast              -0.0037      0.253     -0.014      0.988      -0.500       0.493\n",
       "toasted            -0.4080      0.394     -1.036      0.300      -1.180       0.364\n",
       "toasty              0.9783      0.416      2.352      0.019       0.163       1.794\n",
       "tobacco             0.2870      0.315      0.911      0.362      -0.331       0.905\n",
       "toffee              0.0950      0.240      0.396      0.692      -0.375       0.565\n",
       "tomato             -0.1929      0.285     -0.677      0.498      -0.751       0.365\n",
       "toned              -0.0782      0.129     -0.606      0.544      -0.331       0.175\n",
       "tones              -0.3447      0.186     -1.856      0.063      -0.709       0.019\n",
       "touch               0.9531      0.284      3.354      0.001       0.396       1.510\n",
       "tropical            0.5248      0.272      1.927      0.054      -0.009       1.059\n",
       "turn                0.4286      0.279      1.536      0.125      -0.118       0.976\n",
       "turned              0.6992      0.401      1.742      0.082      -0.088       1.486\n",
       "turning            -0.4028      0.324     -1.244      0.214      -1.038       0.232\n",
       "turns              -0.3378      0.177     -1.905      0.057      -0.685       0.010\n",
       "underlying          1.0703      0.362      2.957      0.003       0.361       1.780\n",
       "understated         0.6765      0.290      2.335      0.020       0.109       1.244\n",
       "undertones          0.1769      0.140      1.260      0.208      -0.098       0.452\n",
       "unusual             0.6468      0.272      2.381      0.017       0.114       1.179\n",
       "using              -0.6848      0.751     -0.912      0.362      -2.157       0.788\n",
       "vanilla            -0.0399      0.209     -0.191      0.848      -0.449       0.369\n",
       "velvety            -0.2081      0.138     -1.511      0.131      -0.478       0.062\n",
       "verbena             0.0794      0.212      0.375      0.708      -0.336       0.495\n",
       "vibrant            -0.1947      0.355     -0.548      0.584      -0.891       0.502\n",
       "vibrantly           0.1343      0.114      1.176      0.240      -0.090       0.358\n",
       "violet              0.4258      0.218      1.955      0.051      -0.001       0.853\n",
       "viscous             0.2179      0.242      0.900      0.368      -0.257       0.692\n",
       "vivacious           0.1529      0.167      0.916      0.360      -0.175       0.480\n",
       "volume              0.7320      0.335      2.184      0.029       0.075       1.389\n",
       "walnut              0.3812      0.640      0.596      0.551      -0.873       1.636\n",
       "water               0.2322      0.217      1.070      0.285      -0.193       0.658\n",
       "watermelon         -0.0691      0.560     -0.124      0.902      -1.166       1.028\n",
       "way                 0.3625      0.392      0.925      0.355      -0.406       1.131\n",
       "wendy               0.4112      0.365      1.126      0.260      -0.305       1.127\n",
       "whisky             -0.1341      0.382     -0.351      0.726      -0.883       0.615\n",
       "white               0.5468      0.475      1.151      0.250      -0.385       1.478\n",
       "wild                0.1057      0.262      0.404      0.687      -0.408       0.619\n",
       "willem             -0.0802      0.307     -0.261      0.794      -0.683       0.523\n",
       "wine               -0.1421      0.466     -0.305      0.761      -1.056       0.772\n",
       "winey               1.9087      0.225      8.470      0.000       1.467       2.351\n",
       "winy                0.8862      0.290      3.055      0.002       0.318       1.455\n",
       "wisteria            0.2840      0.229      1.238      0.216      -0.166       0.734\n",
       "wood               -0.0446      0.254     -0.175      0.861      -0.543       0.454\n",
       "woody              -2.4231      0.360     -6.726      0.000      -3.129      -1.717\n",
       "zest               -0.0436      0.126     -0.346      0.729      -0.290       0.203\n",
       "zesty              -0.0805      0.277     -0.291      0.771      -0.623       0.462\n",
       "==============================================================================\n",
       "Omnibus:                      694.171   Durbin-Watson:                   2.022\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            11031.627\n",
       "Skew:                          -0.271   Prob(JB):                         0.00\n",
       "Kurtosis:                      10.927   Cond. No.                         379.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Instantiate Model\n",
    "sm_model = sm.OLS(y_remain, X_mm_remain_constant)\n",
    "\n",
    "# 2. Fit Model (this returns a seperate object with the parameters)\n",
    "sm_model_results = sm_model.fit()\n",
    "\n",
    "# Looking at the summary\n",
    "sm_model_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b89e79",
   "metadata": {},
   "source": [
    "Now that we've checked out the summary, we'll look at the coefficients and p-values more closely. This way we can see which coefficients are strongest, and which have signficant p-values (under 0.05), indicating that there is a real relationship there and its very unlikely that the finding is just due to chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "a97c23cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying the pvalues and coefficients\n",
    "pvalues = sm_model_results.pvalues\n",
    "sm_coeff = sm_model_results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "a768686e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>65.536859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>2.054687e-01</td>\n",
       "      <td>0.072528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>5.301533e-01</td>\n",
       "      <td>0.157418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bean_agtron</th>\n",
       "      <td>1.939661e-01</td>\n",
       "      <td>-0.488529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ground_agtron</th>\n",
       "      <td>2.899479e-05</td>\n",
       "      <td>1.210479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisteria</th>\n",
       "      <td>2.156571e-01</td>\n",
       "      <td>0.284021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wood</th>\n",
       "      <td>8.608898e-01</td>\n",
       "      <td>-0.044561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woody</th>\n",
       "      <td>2.024290e-11</td>\n",
       "      <td>-2.423064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zest</th>\n",
       "      <td>7.290521e-01</td>\n",
       "      <td>-0.043551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zesty</th>\n",
       "      <td>7.709014e-01</td>\n",
       "      <td>-0.080522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>631 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      pvals   sm_coeff\n",
       "const          0.000000e+00  65.536859\n",
       "month          2.054687e-01   0.072528\n",
       "year           5.301533e-01   0.157418\n",
       "bean_agtron    1.939661e-01  -0.488529\n",
       "ground_agtron  2.899479e-05   1.210479\n",
       "...                     ...        ...\n",
       "wisteria       2.156571e-01   0.284021\n",
       "wood           8.608898e-01  -0.044561\n",
       "woody          2.024290e-11  -2.423064\n",
       "zest           7.290521e-01  -0.043551\n",
       "zesty          7.709014e-01  -0.080522\n",
       "\n",
       "[631 rows x 2 columns]"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a dataframe using the pvales and coeff for easier sorting\n",
    "results_df = pd.DataFrame({'pvals': pvalues, 'sm_coeff': coeff})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb56f6",
   "metadata": {},
   "source": [
    "Looking at coefficients first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "301de1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "guava        -4.391720\n",
       "dark         -3.681606\n",
       "burned       -2.450735\n",
       "woody        -2.423064\n",
       "shallow      -2.104348\n",
       "promise      -2.094222\n",
       "shadow       -2.050135\n",
       "fine         -1.669873\n",
       "sharp        -1.518694\n",
       "cardamom     -1.493389\n",
       "simplify     -1.418179\n",
       "salty        -1.416034\n",
       "sort         -1.396261\n",
       "complex      -1.391581\n",
       "far          -1.321146\n",
       "good         -1.287013\n",
       "tight        -1.235067\n",
       "tested       -1.205977\n",
       "felt         -1.162052\n",
       "displayed    -1.129755\n",
       "mustiness    -1.080856\n",
       "fading       -1.067961\n",
       "sage         -1.051258\n",
       "just         -1.051006\n",
       "nut          -0.998518\n",
       "blood        -0.981551\n",
       "surfaces     -0.880723\n",
       "attractive   -0.875157\n",
       "taste        -0.859022\n",
       "leather      -0.847235\n",
       "bitterness   -0.818127\n",
       "orangy       -0.812396\n",
       "distinct     -0.807550\n",
       "faint        -0.789192\n",
       "roasty       -0.775215\n",
       "displays     -0.758956\n",
       "subdued      -0.755349\n",
       "hop          -0.741316\n",
       "subtle       -0.736966\n",
       "bean         -0.704215\n",
       "pine         -0.700972\n",
       "origin_lat   -0.700406\n",
       "authority    -0.690649\n",
       "using        -0.684840\n",
       "raspberry    -0.682340\n",
       "bitterish    -0.680678\n",
       "smoothly     -0.676657\n",
       "orchid       -0.664086\n",
       "brightness   -0.660767\n",
       "agreeably    -0.640937\n",
       "Name: sm_coeff, dtype: float64"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at the 50 strongest negative coefficients\n",
    "results_df['sm_coeff'].sort_values()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "44bdccb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const            65.536859\n",
       "aroma             8.759889\n",
       "flavor            5.936844\n",
       "aftertaste        5.682272\n",
       "acidity           5.498047\n",
       "body              4.843770\n",
       "kenya             2.556258\n",
       "almond            2.095864\n",
       "winey             1.908736\n",
       "ken               1.735960\n",
       "miguel            1.665479\n",
       "espresso          1.637659\n",
       "bittersweet       1.548675\n",
       "serve             1.543205\n",
       "key               1.453849\n",
       "big               1.435144\n",
       "malty             1.424228\n",
       "cacao             1.377187\n",
       "powder            1.326349\n",
       "framed            1.267691\n",
       "cut               1.247453\n",
       "jasmine           1.210871\n",
       "ground_agtron     1.210479\n",
       "peppercorn        1.191075\n",
       "butterscotch      1.149092\n",
       "dimensioned       1.146258\n",
       "gentle            1.094913\n",
       "suggest           1.079499\n",
       "underlying        1.070264\n",
       "pineapple         1.041139\n",
       "best              1.005992\n",
       "subtly            0.984827\n",
       "exotic            0.981204\n",
       "toasty            0.978334\n",
       "probably          0.973359\n",
       "alive             0.963704\n",
       "touch             0.953083\n",
       "tickle            0.942066\n",
       "influenced        0.939211\n",
       "admired           0.910263\n",
       "apricot           0.887182\n",
       "winy              0.886190\n",
       "shifting          0.884636\n",
       "balancing         0.879505\n",
       "chocolate         0.876727\n",
       "lean              0.866234\n",
       "blooms            0.862709\n",
       "crisp             0.846988\n",
       "pungency          0.829971\n",
       "fermenty          0.827473\n",
       "Name: sm_coeff, dtype: float64"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#narrowing down to top 50 positive coefficients\n",
    "results_df['sm_coeff'].sort_values(ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaba7a2",
   "metadata": {},
   "source": [
    "Looking at p-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "a03ffeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const             0.000000e+00\n",
       "aroma            6.134476e-214\n",
       "body             1.133589e-109\n",
       "aftertaste        1.326236e-66\n",
       "acidity           3.064564e-58\n",
       "flavor            3.895840e-50\n",
       "guava             2.967798e-26\n",
       "winey             3.531391e-17\n",
       "dark              1.404555e-16\n",
       "burned            2.425949e-12\n",
       "promise           7.737566e-12\n",
       "woody             2.024290e-11\n",
       "shadow            5.752931e-11\n",
       "almond            7.353500e-10\n",
       "best              2.743912e-09\n",
       "long              2.775282e-08\n",
       "fine              6.833035e-08\n",
       "shallow           1.144348e-07\n",
       "flower            7.085884e-07\n",
       "cocoaish          1.084489e-06\n",
       "probably          1.480563e-06\n",
       "far               1.838168e-06\n",
       "hibiscus          3.394166e-06\n",
       "apricot           5.084346e-06\n",
       "admired           6.248807e-06\n",
       "ken               6.555040e-06\n",
       "origin_lat        1.152560e-05\n",
       "authority         1.651621e-05\n",
       "sharp             2.301934e-05\n",
       "ground_agtron     2.899479e-05\n",
       "attractive        4.124175e-05\n",
       "butterscotch      4.548934e-05\n",
       "fading            5.125122e-05\n",
       "blood             6.693724e-05\n",
       "shimmer           7.772683e-05\n",
       "resurfacing       9.332160e-05\n",
       "balancing         1.166784e-04\n",
       "nutty             1.909000e-04\n",
       "bittersweet       2.136536e-04\n",
       "fermenty          2.286111e-04\n",
       "miguel            2.576339e-04\n",
       "cut               2.649594e-04\n",
       "fresh             2.825473e-04\n",
       "good              2.875504e-04\n",
       "cardamom          2.966580e-04\n",
       "flavor.1          3.275269e-04\n",
       "suggest           3.287273e-04\n",
       "malty             3.434134e-04\n",
       "big               3.437256e-04\n",
       "rhododendron      4.399816e-04\n",
       "Name: pvals, dtype: float64"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting to see which have lowest pvalues\n",
    "results_df['pvals'].sort_values()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "77918bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emerge          0.998627\n",
       "baker           0.988568\n",
       "toast           0.988486\n",
       "ferment         0.988196\n",
       "passion         0.987885\n",
       "impression      0.987772\n",
       "nice            0.986275\n",
       "fig             0.984799\n",
       "bit             0.977655\n",
       "macadamia       0.977314\n",
       "suggestion      0.974339\n",
       "continued       0.967054\n",
       "semi            0.966349\n",
       "scaled          0.965134\n",
       "deeply          0.964160\n",
       "pleasantly      0.960412\n",
       "solid           0.951312\n",
       "mild            0.951296\n",
       "range           0.949576\n",
       "consolidates    0.944858\n",
       "carries         0.941009\n",
       "honeysuckle     0.935950\n",
       "pistachio       0.933875\n",
       "port            0.930751\n",
       "tart            0.929358\n",
       "substantial     0.927386\n",
       "buoyant         0.924957\n",
       "hard            0.923441\n",
       "layered         0.922967\n",
       "deepen          0.921789\n",
       "carob           0.919189\n",
       "panelists       0.919029\n",
       "mid             0.917973\n",
       "pleasingly      0.916582\n",
       "nougat          0.914056\n",
       "lively          0.911004\n",
       "blooming        0.905420\n",
       "persimmon       0.904074\n",
       "sugary          0.902698\n",
       "watermelon      0.901686\n",
       "carried         0.896138\n",
       "parts           0.889572\n",
       "complication    0.888257\n",
       "simple          0.887211\n",
       "rounded         0.885425\n",
       "edge            0.884909\n",
       "remains         0.880943\n",
       "persists        0.878624\n",
       "nuts            0.872430\n",
       "throughline     0.870139\n",
       "Name: pvals, dtype: float64"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting to see which have highest pvalues\n",
    "results_df['pvals'].sort_values(ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0957df73",
   "metadata": {},
   "source": [
    "It's interesting to see which coefficients are strongest, but we're really only interested in the coefficients with p-values under 0.05, indicating its very likely that a real relationship between the independent variable and the `overall_score` exists. To see this, we'll filter out coefficients that don't have significant p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "15356419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>65.536859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ground_agtron</th>\n",
       "      <td>2.899479e-05</td>\n",
       "      <td>1.210479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aroma</th>\n",
       "      <td>6.134476e-214</td>\n",
       "      <td>8.759889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acidity</th>\n",
       "      <td>3.064564e-58</td>\n",
       "      <td>5.498047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>1.133589e-109</td>\n",
       "      <td>4.843770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unusual</th>\n",
       "      <td>1.730142e-02</td>\n",
       "      <td>0.646765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume</th>\n",
       "      <td>2.905414e-02</td>\n",
       "      <td>0.731980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winey</th>\n",
       "      <td>3.531391e-17</td>\n",
       "      <td>1.908736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winy</th>\n",
       "      <td>2.264489e-03</td>\n",
       "      <td>0.886190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woody</th>\n",
       "      <td>2.024290e-11</td>\n",
       "      <td>-2.423064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       pvals   sm_coeff\n",
       "const           0.000000e+00  65.536859\n",
       "ground_agtron   2.899479e-05   1.210479\n",
       "aroma          6.134476e-214   8.759889\n",
       "acidity         3.064564e-58   5.498047\n",
       "body           1.133589e-109   4.843770\n",
       "...                      ...        ...\n",
       "unusual         1.730142e-02   0.646765\n",
       "volume          2.905414e-02   0.731980\n",
       "winey           3.531391e-17   1.908736\n",
       "winy            2.264489e-03   0.886190\n",
       "woody           2.024290e-11  -2.423064\n",
       "\n",
       "[176 rows x 2 columns]"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating filtered dataframe that only includes coefficients that have significant p-values\n",
    "sig_results_df = results_df.loc[results_df['pvals'] <= 0.05]\n",
    "sig_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "7b49e886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>origin_lat</th>\n",
       "      <td>1.152560e-05</td>\n",
       "      <td>-0.700406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acidy</th>\n",
       "      <td>4.671268e-02</td>\n",
       "      <td>-0.350595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attractive</th>\n",
       "      <td>4.124175e-05</td>\n",
       "      <td>-0.875157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authority</th>\n",
       "      <td>1.651621e-05</td>\n",
       "      <td>-0.690649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bitter</th>\n",
       "      <td>3.854676e-02</td>\n",
       "      <td>-0.394103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bitterish</th>\n",
       "      <td>2.099023e-02</td>\n",
       "      <td>-0.680678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blood</th>\n",
       "      <td>6.693724e-05</td>\n",
       "      <td>-0.981551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>burned</th>\n",
       "      <td>2.425949e-12</td>\n",
       "      <td>-2.450735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cardamom</th>\n",
       "      <td>2.966580e-04</td>\n",
       "      <td>-1.493389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complicating</th>\n",
       "      <td>2.023784e-02</td>\n",
       "      <td>-0.360589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dark</th>\n",
       "      <td>1.404555e-16</td>\n",
       "      <td>-3.681606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displays</th>\n",
       "      <td>9.491496e-03</td>\n",
       "      <td>-0.758956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distinct</th>\n",
       "      <td>5.894242e-03</td>\n",
       "      <td>-0.807550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dominates</th>\n",
       "      <td>4.946802e-02</td>\n",
       "      <td>-0.385766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dusk</th>\n",
       "      <td>1.662166e-03</td>\n",
       "      <td>-0.375243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fading</th>\n",
       "      <td>5.125122e-05</td>\n",
       "      <td>-1.067961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far</th>\n",
       "      <td>1.838168e-06</td>\n",
       "      <td>-1.321146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>felt</th>\n",
       "      <td>1.587495e-03</td>\n",
       "      <td>-1.162052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fine</th>\n",
       "      <td>6.833035e-08</td>\n",
       "      <td>-1.669873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>2.875504e-04</td>\n",
       "      <td>-1.287013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guava</th>\n",
       "      <td>2.967798e-26</td>\n",
       "      <td>-4.391720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heavy</th>\n",
       "      <td>4.160421e-02</td>\n",
       "      <td>-0.559174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hop</th>\n",
       "      <td>1.620036e-02</td>\n",
       "      <td>-0.741316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>2.532151e-03</td>\n",
       "      <td>-1.051006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leather</th>\n",
       "      <td>4.277191e-03</td>\n",
       "      <td>-0.847235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightly</th>\n",
       "      <td>4.665814e-02</td>\n",
       "      <td>-0.371610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liked</th>\n",
       "      <td>6.468193e-03</td>\n",
       "      <td>-0.340030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mustiness</th>\n",
       "      <td>6.014215e-03</td>\n",
       "      <td>-1.080856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nut</th>\n",
       "      <td>6.763874e-03</td>\n",
       "      <td>-0.998518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promise</th>\n",
       "      <td>7.737566e-12</td>\n",
       "      <td>-2.094222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relatively</th>\n",
       "      <td>2.346076e-02</td>\n",
       "      <td>-0.620369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roasty</th>\n",
       "      <td>4.561874e-02</td>\n",
       "      <td>-0.775215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sage</th>\n",
       "      <td>3.523614e-03</td>\n",
       "      <td>-1.051258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salty</th>\n",
       "      <td>4.162580e-03</td>\n",
       "      <td>-1.416034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sandalwood</th>\n",
       "      <td>2.302682e-02</td>\n",
       "      <td>-0.237635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shadow</th>\n",
       "      <td>5.752931e-11</td>\n",
       "      <td>-2.050135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shallow</th>\n",
       "      <td>1.144348e-07</td>\n",
       "      <td>-2.104348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sharp</th>\n",
       "      <td>2.301934e-05</td>\n",
       "      <td>-1.518694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shimmer</th>\n",
       "      <td>7.772683e-05</td>\n",
       "      <td>-0.447212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>silky</th>\n",
       "      <td>2.127324e-03</td>\n",
       "      <td>-0.585598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothly</th>\n",
       "      <td>1.850702e-02</td>\n",
       "      <td>-0.676657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>softens</th>\n",
       "      <td>4.184135e-02</td>\n",
       "      <td>-0.546090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort</th>\n",
       "      <td>1.073160e-03</td>\n",
       "      <td>-1.396261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subdued</th>\n",
       "      <td>2.401644e-02</td>\n",
       "      <td>-0.755349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surfaces</th>\n",
       "      <td>1.328125e-03</td>\n",
       "      <td>-0.880723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taste</th>\n",
       "      <td>1.273443e-02</td>\n",
       "      <td>-0.859022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tight</th>\n",
       "      <td>1.904406e-02</td>\n",
       "      <td>-1.235067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woody</th>\n",
       "      <td>2.024290e-11</td>\n",
       "      <td>-2.423064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pvals  sm_coeff\n",
       "origin_lat    1.152560e-05 -0.700406\n",
       "acidy         4.671268e-02 -0.350595\n",
       "attractive    4.124175e-05 -0.875157\n",
       "authority     1.651621e-05 -0.690649\n",
       "bitter        3.854676e-02 -0.394103\n",
       "bitterish     2.099023e-02 -0.680678\n",
       "blood         6.693724e-05 -0.981551\n",
       "burned        2.425949e-12 -2.450735\n",
       "cardamom      2.966580e-04 -1.493389\n",
       "complicating  2.023784e-02 -0.360589\n",
       "dark          1.404555e-16 -3.681606\n",
       "displays      9.491496e-03 -0.758956\n",
       "distinct      5.894242e-03 -0.807550\n",
       "dominates     4.946802e-02 -0.385766\n",
       "dusk          1.662166e-03 -0.375243\n",
       "fading        5.125122e-05 -1.067961\n",
       "far           1.838168e-06 -1.321146\n",
       "felt          1.587495e-03 -1.162052\n",
       "fine          6.833035e-08 -1.669873\n",
       "good          2.875504e-04 -1.287013\n",
       "guava         2.967798e-26 -4.391720\n",
       "heavy         4.160421e-02 -0.559174\n",
       "hop           1.620036e-02 -0.741316\n",
       "just          2.532151e-03 -1.051006\n",
       "leather       4.277191e-03 -0.847235\n",
       "lightly       4.665814e-02 -0.371610\n",
       "liked         6.468193e-03 -0.340030\n",
       "mustiness     6.014215e-03 -1.080856\n",
       "nut           6.763874e-03 -0.998518\n",
       "promise       7.737566e-12 -2.094222\n",
       "relatively    2.346076e-02 -0.620369\n",
       "roasty        4.561874e-02 -0.775215\n",
       "sage          3.523614e-03 -1.051258\n",
       "salty         4.162580e-03 -1.416034\n",
       "sandalwood    2.302682e-02 -0.237635\n",
       "shadow        5.752931e-11 -2.050135\n",
       "shallow       1.144348e-07 -2.104348\n",
       "sharp         2.301934e-05 -1.518694\n",
       "shimmer       7.772683e-05 -0.447212\n",
       "silky         2.127324e-03 -0.585598\n",
       "smoothly      1.850702e-02 -0.676657\n",
       "softens       4.184135e-02 -0.546090\n",
       "sort          1.073160e-03 -1.396261\n",
       "subdued       2.401644e-02 -0.755349\n",
       "surfaces      1.328125e-03 -0.880723\n",
       "taste         1.273443e-02 -0.859022\n",
       "tight         1.904406e-02 -1.235067\n",
       "woody         2.024290e-11 -2.423064"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#isolating significant, negative coefficients\n",
    "neg_sig_results_df = results_df.loc[(results_df['pvals'] <= 0.05)&(results_df['sm_coeff']<0)]\n",
    "neg_sig_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "dd12fcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "guava          -4.391720\n",
       "dark           -3.681606\n",
       "burned         -2.450735\n",
       "woody          -2.423064\n",
       "shallow        -2.104348\n",
       "promise        -2.094222\n",
       "shadow         -2.050135\n",
       "fine           -1.669873\n",
       "sharp          -1.518694\n",
       "cardamom       -1.493389\n",
       "salty          -1.416034\n",
       "sort           -1.396261\n",
       "far            -1.321146\n",
       "good           -1.287013\n",
       "tight          -1.235067\n",
       "felt           -1.162052\n",
       "mustiness      -1.080856\n",
       "fading         -1.067961\n",
       "sage           -1.051258\n",
       "just           -1.051006\n",
       "nut            -0.998518\n",
       "blood          -0.981551\n",
       "surfaces       -0.880723\n",
       "attractive     -0.875157\n",
       "taste          -0.859022\n",
       "leather        -0.847235\n",
       "distinct       -0.807550\n",
       "roasty         -0.775215\n",
       "displays       -0.758956\n",
       "subdued        -0.755349\n",
       "hop            -0.741316\n",
       "origin_lat     -0.700406\n",
       "authority      -0.690649\n",
       "bitterish      -0.680678\n",
       "smoothly       -0.676657\n",
       "relatively     -0.620369\n",
       "silky          -0.585598\n",
       "heavy          -0.559174\n",
       "softens        -0.546090\n",
       "shimmer        -0.447212\n",
       "bitter         -0.394103\n",
       "dominates      -0.385766\n",
       "dusk           -0.375243\n",
       "lightly        -0.371610\n",
       "complicating   -0.360589\n",
       "acidy          -0.350595\n",
       "liked          -0.340030\n",
       "sandalwood     -0.237635\n",
       "Name: sm_coeff, dtype: float64"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting to see which variables with significant pvalues have strongest neg coeff\n",
    "neg_visual = neg_sig_results_df['sm_coeff'].sort_values()\n",
    "neg_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "f51ac319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>65.536859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ground_agtron</th>\n",
       "      <td>2.899479e-05</td>\n",
       "      <td>1.210479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aroma</th>\n",
       "      <td>6.134476e-214</td>\n",
       "      <td>8.759889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acidity</th>\n",
       "      <td>3.064564e-58</td>\n",
       "      <td>5.498047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>1.133589e-109</td>\n",
       "      <td>4.843770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>understated</th>\n",
       "      <td>1.958853e-02</td>\n",
       "      <td>0.676496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unusual</th>\n",
       "      <td>1.730142e-02</td>\n",
       "      <td>0.646765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume</th>\n",
       "      <td>2.905414e-02</td>\n",
       "      <td>0.731980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winey</th>\n",
       "      <td>3.531391e-17</td>\n",
       "      <td>1.908736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winy</th>\n",
       "      <td>2.264489e-03</td>\n",
       "      <td>0.886190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       pvals   sm_coeff\n",
       "const           0.000000e+00  65.536859\n",
       "ground_agtron   2.899479e-05   1.210479\n",
       "aroma          6.134476e-214   8.759889\n",
       "acidity         3.064564e-58   5.498047\n",
       "body           1.133589e-109   4.843770\n",
       "...                      ...        ...\n",
       "understated     1.958853e-02   0.676496\n",
       "unusual         1.730142e-02   0.646765\n",
       "volume          2.905414e-02   0.731980\n",
       "winey           3.531391e-17   1.908736\n",
       "winy            2.264489e-03   0.886190\n",
       "\n",
       "[128 rows x 2 columns]"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#isolating significant, positive coefficients\n",
    "pos_sig_results_df = results_df.loc[(results_df['pvals'] <= 0.05)&(results_df['sm_coeff']>=0)]\n",
    "pos_sig_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15678a5a",
   "metadata": {},
   "source": [
    "Good news - there are a number of coefficients with p-values under 0.05. \n",
    "\n",
    "### 7.2 Comparing Stats Models and SKLearn Results <a class=\"anchor\" id=\"subheader72\"></a>\n",
    "\n",
    "Now, let's compare the coefficients from the Stats Models Linear Regression model to the coefficients from our Lasso Regression model to see if they are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "ed723303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lasso_Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0.005458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>-0.013003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bean_agtron</th>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ground_agtron</th>\n",
       "      <td>0.925823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aroma</th>\n",
       "      <td>9.733670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisteria</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wood</th>\n",
       "      <td>-0.067110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woody</th>\n",
       "      <td>-2.004037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zest</th>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zesty</th>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Lasso_Coef\n",
       "month            0.005458\n",
       "year            -0.013003\n",
       "bean_agtron     -0.000000\n",
       "ground_agtron    0.925823\n",
       "aroma            9.733670\n",
       "...                   ...\n",
       "wisteria         0.000000\n",
       "wood            -0.067110\n",
       "woody           -2.004037\n",
       "zest            -0.000000\n",
       "zesty           -0.000000\n",
       "\n",
       "[630 rows x 1 columns]"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the coefficients from the elastic net model with labels\n",
    "lasso_coef_df = pd.DataFrame(lr_model_lasso_full.coef_, X_mm_scaled_remain.columns, columns=['Lasso_Coef'])\n",
    "lasso_coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "1b560917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>2.054687e-01</td>\n",
       "      <td>0.072528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>5.301533e-01</td>\n",
       "      <td>0.157418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bean_agtron</th>\n",
       "      <td>1.939661e-01</td>\n",
       "      <td>-0.488529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ground_agtron</th>\n",
       "      <td>2.899479e-05</td>\n",
       "      <td>1.210479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aroma</th>\n",
       "      <td>6.134476e-214</td>\n",
       "      <td>8.759889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisteria</th>\n",
       "      <td>2.156571e-01</td>\n",
       "      <td>0.284021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wood</th>\n",
       "      <td>8.608898e-01</td>\n",
       "      <td>-0.044561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woody</th>\n",
       "      <td>2.024290e-11</td>\n",
       "      <td>-2.423064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zest</th>\n",
       "      <td>7.290521e-01</td>\n",
       "      <td>-0.043551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zesty</th>\n",
       "      <td>7.709014e-01</td>\n",
       "      <td>-0.080522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       pvals  sm_coeff\n",
       "month           2.054687e-01  0.072528\n",
       "year            5.301533e-01  0.157418\n",
       "bean_agtron     1.939661e-01 -0.488529\n",
       "ground_agtron   2.899479e-05  1.210479\n",
       "aroma          6.134476e-214  8.759889\n",
       "...                      ...       ...\n",
       "wisteria        2.156571e-01  0.284021\n",
       "wood            8.608898e-01 -0.044561\n",
       "woody           2.024290e-11 -2.423064\n",
       "zest            7.290521e-01 -0.043551\n",
       "zesty           7.709014e-01 -0.080522\n",
       "\n",
       "[630 rows x 2 columns]"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping constant from stats model DF so we can combine and compare our dataframes\n",
    "sm_df = results_df.drop('const')\n",
    "sm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "98f9f84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Lasso_Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>month</td>\n",
       "      <td>0.005458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>year</td>\n",
       "      <td>-0.013003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bean_agtron</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ground_agtron</td>\n",
       "      <td>0.925823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aroma</td>\n",
       "      <td>9.733670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>wisteria</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>wood</td>\n",
       "      <td>-0.067110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>woody</td>\n",
       "      <td>-2.004037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>zest</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>zesty</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index  Lasso_Coef\n",
       "0            month    0.005458\n",
       "1             year   -0.013003\n",
       "2      bean_agtron   -0.000000\n",
       "3    ground_agtron    0.925823\n",
       "4            aroma    9.733670\n",
       "..             ...         ...\n",
       "625       wisteria    0.000000\n",
       "626           wood   -0.067110\n",
       "627          woody   -2.004037\n",
       "628           zest   -0.000000\n",
       "629          zesty   -0.000000\n",
       "\n",
       "[630 rows x 2 columns]"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#resetting index so we can combine\n",
    "lasso_coef_df.reset_index(inplace = True)\n",
    "lasso_coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "dc68859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting index so we can combine\n",
    "sm_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "b6835c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming index column so they have diff labels and can drop duplicate after combining\n",
    "#dropping after combining to check that rows are lining up correctly\n",
    "sm_df.rename(columns={'index': \"sm_index\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "f4872aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine dataframes for comparison\n",
    "comparison_df = pd.concat([sm_df, lasso_coef_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "c0fd6ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm_index</th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "      <th>index</th>\n",
       "      <th>Lasso_Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>month</td>\n",
       "      <td>2.054687e-01</td>\n",
       "      <td>0.072528</td>\n",
       "      <td>month</td>\n",
       "      <td>0.005458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>year</td>\n",
       "      <td>5.301533e-01</td>\n",
       "      <td>0.157418</td>\n",
       "      <td>year</td>\n",
       "      <td>-0.013003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bean_agtron</td>\n",
       "      <td>1.939661e-01</td>\n",
       "      <td>-0.488529</td>\n",
       "      <td>bean_agtron</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ground_agtron</td>\n",
       "      <td>2.899479e-05</td>\n",
       "      <td>1.210479</td>\n",
       "      <td>ground_agtron</td>\n",
       "      <td>0.925823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aroma</td>\n",
       "      <td>6.134476e-214</td>\n",
       "      <td>8.759889</td>\n",
       "      <td>aroma</td>\n",
       "      <td>9.733670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>wisteria</td>\n",
       "      <td>2.156571e-01</td>\n",
       "      <td>0.284021</td>\n",
       "      <td>wisteria</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>wood</td>\n",
       "      <td>8.608898e-01</td>\n",
       "      <td>-0.044561</td>\n",
       "      <td>wood</td>\n",
       "      <td>-0.067110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>woody</td>\n",
       "      <td>2.024290e-11</td>\n",
       "      <td>-2.423064</td>\n",
       "      <td>woody</td>\n",
       "      <td>-2.004037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>zest</td>\n",
       "      <td>7.290521e-01</td>\n",
       "      <td>-0.043551</td>\n",
       "      <td>zest</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>zesty</td>\n",
       "      <td>7.709014e-01</td>\n",
       "      <td>-0.080522</td>\n",
       "      <td>zesty</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sm_index          pvals  sm_coeff          index  Lasso_Coef\n",
       "0            month   2.054687e-01  0.072528          month    0.005458\n",
       "1             year   5.301533e-01  0.157418           year   -0.013003\n",
       "2      bean_agtron   1.939661e-01 -0.488529    bean_agtron   -0.000000\n",
       "3    ground_agtron   2.899479e-05  1.210479  ground_agtron    0.925823\n",
       "4            aroma  6.134476e-214  8.759889          aroma    9.733670\n",
       "..             ...            ...       ...            ...         ...\n",
       "625       wisteria   2.156571e-01  0.284021       wisteria    0.000000\n",
       "626           wood   8.608898e-01 -0.044561           wood   -0.067110\n",
       "627          woody   2.024290e-11 -2.423064          woody   -2.004037\n",
       "628           zest   7.290521e-01 -0.043551           zest   -0.000000\n",
       "629          zesty   7.709014e-01 -0.080522          zesty   -0.000000\n",
       "\n",
       "[630 rows x 5 columns]"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check dataframe to make sure rows line up as expected (they do)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "72e40128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate index column\n",
    "comparison_df.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "5de09f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm_index</th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "      <th>Lasso_Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>month</td>\n",
       "      <td>2.054687e-01</td>\n",
       "      <td>0.072528</td>\n",
       "      <td>0.005458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>year</td>\n",
       "      <td>5.301533e-01</td>\n",
       "      <td>0.157418</td>\n",
       "      <td>-0.013003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bean_agtron</td>\n",
       "      <td>1.939661e-01</td>\n",
       "      <td>-0.488529</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ground_agtron</td>\n",
       "      <td>2.899479e-05</td>\n",
       "      <td>1.210479</td>\n",
       "      <td>0.925823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aroma</td>\n",
       "      <td>6.134476e-214</td>\n",
       "      <td>8.759889</td>\n",
       "      <td>9.733670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>wisteria</td>\n",
       "      <td>2.156571e-01</td>\n",
       "      <td>0.284021</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>wood</td>\n",
       "      <td>8.608898e-01</td>\n",
       "      <td>-0.044561</td>\n",
       "      <td>-0.067110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>woody</td>\n",
       "      <td>2.024290e-11</td>\n",
       "      <td>-2.423064</td>\n",
       "      <td>-2.004037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>zest</td>\n",
       "      <td>7.290521e-01</td>\n",
       "      <td>-0.043551</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>zesty</td>\n",
       "      <td>7.709014e-01</td>\n",
       "      <td>-0.080522</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sm_index          pvals  sm_coeff  Lasso_Coef\n",
       "0            month   2.054687e-01  0.072528    0.005458\n",
       "1             year   5.301533e-01  0.157418   -0.013003\n",
       "2      bean_agtron   1.939661e-01 -0.488529   -0.000000\n",
       "3    ground_agtron   2.899479e-05  1.210479    0.925823\n",
       "4            aroma  6.134476e-214  8.759889    9.733670\n",
       "..             ...            ...       ...         ...\n",
       "625       wisteria   2.156571e-01  0.284021    0.000000\n",
       "626           wood   8.608898e-01 -0.044561   -0.067110\n",
       "627          woody   2.024290e-11 -2.423064   -2.004037\n",
       "628           zest   7.290521e-01 -0.043551   -0.000000\n",
       "629          zesty   7.709014e-01 -0.080522   -0.000000\n",
       "\n",
       "[630 rows x 4 columns]"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0575f1d6",
   "metadata": {},
   "source": [
    "Now that the dataframe is ready, let's see how our coefficients compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "a215d6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm_index</th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "      <th>Lasso_Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>origin_lat</td>\n",
       "      <td>1.152560e-05</td>\n",
       "      <td>-0.700406</td>\n",
       "      <td>-0.684403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>acidy</td>\n",
       "      <td>4.671268e-02</td>\n",
       "      <td>-0.350595</td>\n",
       "      <td>-0.315289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>attractive</td>\n",
       "      <td>4.124175e-05</td>\n",
       "      <td>-0.875157</td>\n",
       "      <td>-0.569292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>authority</td>\n",
       "      <td>1.651621e-05</td>\n",
       "      <td>-0.690649</td>\n",
       "      <td>-0.653338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>bitter</td>\n",
       "      <td>3.854676e-02</td>\n",
       "      <td>-0.394103</td>\n",
       "      <td>-0.380116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>bitterish</td>\n",
       "      <td>2.099023e-02</td>\n",
       "      <td>-0.680678</td>\n",
       "      <td>-0.426146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>blood</td>\n",
       "      <td>6.693724e-05</td>\n",
       "      <td>-0.981551</td>\n",
       "      <td>-0.696301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>burned</td>\n",
       "      <td>2.425949e-12</td>\n",
       "      <td>-2.450735</td>\n",
       "      <td>-1.424700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>cardamom</td>\n",
       "      <td>2.966580e-04</td>\n",
       "      <td>-1.493389</td>\n",
       "      <td>-0.225784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>complicating</td>\n",
       "      <td>2.023784e-02</td>\n",
       "      <td>-0.360589</td>\n",
       "      <td>-0.165565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>dark</td>\n",
       "      <td>1.404555e-16</td>\n",
       "      <td>-3.681606</td>\n",
       "      <td>-2.117352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>displays</td>\n",
       "      <td>9.491496e-03</td>\n",
       "      <td>-0.758956</td>\n",
       "      <td>-0.357077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>distinct</td>\n",
       "      <td>5.894242e-03</td>\n",
       "      <td>-0.807550</td>\n",
       "      <td>-0.182705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>dominates</td>\n",
       "      <td>4.946802e-02</td>\n",
       "      <td>-0.385766</td>\n",
       "      <td>-0.316366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>dusk</td>\n",
       "      <td>1.662166e-03</td>\n",
       "      <td>-0.375243</td>\n",
       "      <td>-0.373874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>fading</td>\n",
       "      <td>5.125122e-05</td>\n",
       "      <td>-1.067961</td>\n",
       "      <td>-0.614771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>far</td>\n",
       "      <td>1.838168e-06</td>\n",
       "      <td>-1.321146</td>\n",
       "      <td>-0.767235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>felt</td>\n",
       "      <td>1.587495e-03</td>\n",
       "      <td>-1.162052</td>\n",
       "      <td>-0.513954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>fine</td>\n",
       "      <td>6.833035e-08</td>\n",
       "      <td>-1.669873</td>\n",
       "      <td>-1.056521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>good</td>\n",
       "      <td>2.875504e-04</td>\n",
       "      <td>-1.287013</td>\n",
       "      <td>-0.644849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sm_index         pvals  sm_coeff  Lasso_Coef\n",
       "11     origin_lat  1.152560e-05 -0.700406   -0.684403\n",
       "15          acidy  4.671268e-02 -0.350595   -0.315289\n",
       "34     attractive  4.124175e-05 -0.875157   -0.569292\n",
       "35      authority  1.651621e-05 -0.690649   -0.653338\n",
       "55         bitter  3.854676e-02 -0.394103   -0.380116\n",
       "56      bitterish  2.099023e-02 -0.680678   -0.426146\n",
       "62          blood  6.693724e-05 -0.981551   -0.696301\n",
       "84         burned  2.425949e-12 -2.450735   -1.424700\n",
       "99       cardamom  2.966580e-04 -1.493389   -0.225784\n",
       "139  complicating  2.023784e-02 -0.360589   -0.165565\n",
       "156          dark  1.404555e-16 -3.681606   -2.117352\n",
       "171      displays  9.491496e-03 -0.758956   -0.357077\n",
       "172      distinct  5.894242e-03 -0.807550   -0.182705\n",
       "177     dominates  4.946802e-02 -0.385766   -0.316366\n",
       "184          dusk  1.662166e-03 -0.375243   -0.373874\n",
       "209        fading  5.125122e-05 -1.067961   -0.614771\n",
       "213           far  1.838168e-06 -1.321146   -0.767235\n",
       "214          felt  1.587495e-03 -1.162052   -0.513954\n",
       "219          fine  6.833035e-08 -1.669873   -1.056521\n",
       "245          good  2.875504e-04 -1.287013   -0.644849"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#isolating significant, negative coefficients\n",
    "neg_sig_results_cdf = comparison_df.loc[(comparison_df['pvals'] <= 0.05)&(comparison_df['sm_coeff']<0)]\n",
    "neg_sig_results_cdf.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e95c7",
   "metadata": {},
   "source": [
    "While the strength of the coefficient differs somewhat between the two models, they are in agreement on the direction (meaning, both models find the above have a negative relationship with the target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "139bf43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm_index</th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "      <th>Lasso_Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ground_agtron</td>\n",
       "      <td>2.899479e-05</td>\n",
       "      <td>1.210479</td>\n",
       "      <td>0.925823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aroma</td>\n",
       "      <td>6.134476e-214</td>\n",
       "      <td>8.759889</td>\n",
       "      <td>9.733670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acidity</td>\n",
       "      <td>3.064564e-58</td>\n",
       "      <td>5.498047</td>\n",
       "      <td>6.085170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>body</td>\n",
       "      <td>1.133589e-109</td>\n",
       "      <td>4.843770</td>\n",
       "      <td>5.196880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flavor</td>\n",
       "      <td>3.895840e-50</td>\n",
       "      <td>5.936844</td>\n",
       "      <td>5.595205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aftertaste</td>\n",
       "      <td>1.326236e-66</td>\n",
       "      <td>5.682272</td>\n",
       "      <td>7.174108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>admired</td>\n",
       "      <td>6.248807e-06</td>\n",
       "      <td>0.910263</td>\n",
       "      <td>0.427380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>agave</td>\n",
       "      <td>1.739529e-02</td>\n",
       "      <td>0.617147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>alive</td>\n",
       "      <td>5.581216e-03</td>\n",
       "      <td>0.963704</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>almond</td>\n",
       "      <td>7.353500e-10</td>\n",
       "      <td>2.095864</td>\n",
       "      <td>1.205609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>apricot</td>\n",
       "      <td>5.084346e-06</td>\n",
       "      <td>0.887182</td>\n",
       "      <td>0.330035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>balancing</td>\n",
       "      <td>1.166784e-04</td>\n",
       "      <td>0.879505</td>\n",
       "      <td>0.472179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>banana</td>\n",
       "      <td>9.008446e-03</td>\n",
       "      <td>0.287543</td>\n",
       "      <td>0.137511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>best</td>\n",
       "      <td>2.743912e-09</td>\n",
       "      <td>1.005992</td>\n",
       "      <td>0.736170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>big</td>\n",
       "      <td>3.437256e-04</td>\n",
       "      <td>1.435144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>bittersweet</td>\n",
       "      <td>2.136536e-04</td>\n",
       "      <td>1.548675</td>\n",
       "      <td>0.161656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>blooms</td>\n",
       "      <td>2.926419e-02</td>\n",
       "      <td>0.862709</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>bodied</td>\n",
       "      <td>1.840670e-02</td>\n",
       "      <td>0.520611</td>\n",
       "      <td>0.036912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>body.1</td>\n",
       "      <td>2.805838e-03</td>\n",
       "      <td>0.515541</td>\n",
       "      <td>0.102742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>butter</td>\n",
       "      <td>4.313767e-03</td>\n",
       "      <td>0.429823</td>\n",
       "      <td>0.134376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>butterscotch</td>\n",
       "      <td>4.548934e-05</td>\n",
       "      <td>1.149092</td>\n",
       "      <td>0.338571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>buttery</td>\n",
       "      <td>2.462675e-02</td>\n",
       "      <td>0.520025</td>\n",
       "      <td>0.152913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>cacao</td>\n",
       "      <td>2.961323e-03</td>\n",
       "      <td>1.377187</td>\n",
       "      <td>0.090132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>caramel</td>\n",
       "      <td>4.944013e-02</td>\n",
       "      <td>0.317564</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>center</td>\n",
       "      <td>4.920944e-02</td>\n",
       "      <td>0.521539</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>cherryish</td>\n",
       "      <td>1.496251e-03</td>\n",
       "      <td>0.486250</td>\n",
       "      <td>0.368852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>5.268633e-04</td>\n",
       "      <td>0.876727</td>\n",
       "      <td>0.322651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>chocolaty</td>\n",
       "      <td>6.817224e-04</td>\n",
       "      <td>0.463837</td>\n",
       "      <td>0.237981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>cinnamon</td>\n",
       "      <td>3.297017e-02</td>\n",
       "      <td>0.349187</td>\n",
       "      <td>0.060613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>citrusy</td>\n",
       "      <td>4.608179e-02</td>\n",
       "      <td>0.324734</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>clean</td>\n",
       "      <td>1.595640e-02</td>\n",
       "      <td>0.703809</td>\n",
       "      <td>0.193252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>cleanly</td>\n",
       "      <td>2.637635e-03</td>\n",
       "      <td>0.544299</td>\n",
       "      <td>0.581434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>cocoaish</td>\n",
       "      <td>1.084489e-06</td>\n",
       "      <td>0.746897</td>\n",
       "      <td>0.207834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>coffee</td>\n",
       "      <td>6.958686e-03</td>\n",
       "      <td>0.762770</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>complexity</td>\n",
       "      <td>8.766327e-04</td>\n",
       "      <td>0.716977</td>\n",
       "      <td>0.279330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>complexly</td>\n",
       "      <td>1.856765e-02</td>\n",
       "      <td>0.294325</td>\n",
       "      <td>0.265432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>complicate</td>\n",
       "      <td>2.843965e-02</td>\n",
       "      <td>0.473242</td>\n",
       "      <td>0.253380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>cools</td>\n",
       "      <td>4.235173e-02</td>\n",
       "      <td>0.346260</td>\n",
       "      <td>0.342857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>cupping</td>\n",
       "      <td>9.239893e-04</td>\n",
       "      <td>0.372694</td>\n",
       "      <td>0.129572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>cups</td>\n",
       "      <td>1.044312e-02</td>\n",
       "      <td>0.600809</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>cut</td>\n",
       "      <td>2.649594e-04</td>\n",
       "      <td>1.247453</td>\n",
       "      <td>0.610953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>date</td>\n",
       "      <td>4.963556e-02</td>\n",
       "      <td>0.410995</td>\n",
       "      <td>0.195624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>deepening</td>\n",
       "      <td>2.182389e-02</td>\n",
       "      <td>0.442199</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>deepens</td>\n",
       "      <td>1.199258e-02</td>\n",
       "      <td>0.382546</td>\n",
       "      <td>0.197966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>delicately</td>\n",
       "      <td>1.527396e-02</td>\n",
       "      <td>0.707926</td>\n",
       "      <td>0.063291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>depth</td>\n",
       "      <td>3.315809e-02</td>\n",
       "      <td>0.261392</td>\n",
       "      <td>0.161691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>device</td>\n",
       "      <td>2.356815e-02</td>\n",
       "      <td>0.295169</td>\n",
       "      <td>0.157359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>dimension</td>\n",
       "      <td>6.692773e-03</td>\n",
       "      <td>0.659526</td>\n",
       "      <td>0.288482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>dimensioned</td>\n",
       "      <td>2.601767e-02</td>\n",
       "      <td>1.146258</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>earth</td>\n",
       "      <td>1.467634e-02</td>\n",
       "      <td>0.791776</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sm_index          pvals  sm_coeff  Lasso_Coef\n",
       "3    ground_agtron   2.899479e-05  1.210479    0.925823\n",
       "4            aroma  6.134476e-214  8.759889    9.733670\n",
       "5          acidity   3.064564e-58  5.498047    6.085170\n",
       "6             body  1.133589e-109  4.843770    5.196880\n",
       "7           flavor   3.895840e-50  5.936844    5.595205\n",
       "8       aftertaste   1.326236e-66  5.682272    7.174108\n",
       "16         admired   6.248807e-06  0.910263    0.427380\n",
       "18           agave   1.739529e-02  0.617147    0.000000\n",
       "22           alive   5.581216e-03  0.963704    0.000000\n",
       "23          almond   7.353500e-10  2.095864    1.205609\n",
       "28         apricot   5.084346e-06  0.887182    0.330035\n",
       "43       balancing   1.166784e-04  0.879505    0.472179\n",
       "44          banana   9.008446e-03  0.287543    0.137511\n",
       "51            best   2.743912e-09  1.005992    0.736170\n",
       "52             big   3.437256e-04  1.435144    0.000000\n",
       "58     bittersweet   2.136536e-04  1.548675    0.161656\n",
       "65          blooms   2.926419e-02  0.862709    0.000000\n",
       "68          bodied   1.840670e-02  0.520611    0.036912\n",
       "69          body.1   2.805838e-03  0.515541    0.102742\n",
       "85          butter   4.313767e-03  0.429823    0.134376\n",
       "86    butterscotch   4.548934e-05  1.149092    0.338571\n",
       "87         buttery   2.462675e-02  0.520025    0.152913\n",
       "89           cacao   2.961323e-03  1.377187    0.090132\n",
       "96         caramel   4.944013e-02  0.317564   -0.000000\n",
       "108         center   4.920944e-02  0.521539    0.000000\n",
       "114      cherryish   1.496251e-03  0.486250    0.368852\n",
       "115      chocolate   5.268633e-04  0.876727    0.322651\n",
       "116      chocolaty   6.817224e-04  0.463837    0.237981\n",
       "117       cinnamon   3.297017e-02  0.349187    0.060613\n",
       "120        citrusy   4.608179e-02  0.324734    0.000000\n",
       "122          clean   1.595640e-02  0.703809    0.193252\n",
       "123        cleanly   2.637635e-03  0.544299    0.581434\n",
       "127       cocoaish   1.084489e-06  0.746897    0.207834\n",
       "129         coffee   6.958686e-03  0.762770    0.000000\n",
       "134     complexity   8.766327e-04  0.716977    0.279330\n",
       "135      complexly   1.856765e-02  0.294325    0.265432\n",
       "136     complicate   2.843965e-02  0.473242    0.253380\n",
       "147          cools   4.235173e-02  0.346260    0.342857\n",
       "152        cupping   9.239893e-04  0.372694    0.129572\n",
       "153           cups   1.044312e-02  0.600809    0.000000\n",
       "155            cut   2.649594e-04  1.247453    0.610953\n",
       "157           date   4.963556e-02  0.410995    0.195624\n",
       "160      deepening   2.182389e-02  0.442199    0.000000\n",
       "161        deepens   1.199258e-02  0.382546    0.197966\n",
       "164     delicately   1.527396e-02  0.707926    0.063291\n",
       "165          depth   3.315809e-02  0.261392    0.161691\n",
       "167         device   2.356815e-02  0.295169    0.157359\n",
       "168      dimension   6.692773e-03  0.659526    0.288482\n",
       "169    dimensioned   2.601767e-02  1.146258    0.000000\n",
       "185          earth   1.467634e-02  0.791776    0.000000"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#isolating significant, positive coefficients\n",
    "pos_sig_results_cdf = comparison_df.loc[(comparison_df['pvals'] <= 0.05)&(comparison_df['sm_coeff']>=0)]\n",
    "pos_sig_results_cdf[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13127ed",
   "metadata": {},
   "source": [
    "Similarly, the models seem to agree which coefficients have a positive relationship. There's some difference regarding which coefficients have the strongest relationship, but both seem to agree that `aroma`, `acidity`, `body`, `aftertaste`, and `flavor` are most important of the positive coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "ede9baff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm_index</th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "      <th>Lasso_Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sm_index, pvals, sm_coeff, Lasso_Coef]\n",
       "Index: []"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see if there are any variables the two models disagree about direction on\n",
    "conflict_df = comparison_df.loc[(comparison_df['pvals'] <= 0.05)&(comparison_df['sm_coeff']>=0)&(comparison_df['Lasso_Coef']<0)]\n",
    "conflict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538a0a2",
   "metadata": {},
   "source": [
    "There aren't any variables where the models disagree on the relationship direction.\n",
    "\n",
    "Below, we'll save these coefficients and p-values to csv files that can be used in Tableau for easy visualization creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "6ddd0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for easy reference to create visualizations\n",
    "neg_sig_results_cdf.to_csv('neg_visual_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "21eae9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for easy reference to create visualizations\n",
    "pos_sig_results_cdf.to_csv('pos_visual_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "55b7fc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm_index</th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "      <th>Lasso_Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ground_agtron</td>\n",
       "      <td>2.899479e-05</td>\n",
       "      <td>1.210479</td>\n",
       "      <td>0.925823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aroma</td>\n",
       "      <td>6.134476e-214</td>\n",
       "      <td>8.759889</td>\n",
       "      <td>9.733670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acidity</td>\n",
       "      <td>3.064564e-58</td>\n",
       "      <td>5.498047</td>\n",
       "      <td>6.085170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>body</td>\n",
       "      <td>1.133589e-109</td>\n",
       "      <td>4.843770</td>\n",
       "      <td>5.196880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flavor</td>\n",
       "      <td>3.895840e-50</td>\n",
       "      <td>5.936844</td>\n",
       "      <td>5.595205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>understated</td>\n",
       "      <td>1.958853e-02</td>\n",
       "      <td>0.676496</td>\n",
       "      <td>0.411227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>unusual</td>\n",
       "      <td>1.730142e-02</td>\n",
       "      <td>0.646765</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>volume</td>\n",
       "      <td>2.905414e-02</td>\n",
       "      <td>0.731980</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>winey</td>\n",
       "      <td>3.531391e-17</td>\n",
       "      <td>1.908736</td>\n",
       "      <td>1.288508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>winy</td>\n",
       "      <td>2.264489e-03</td>\n",
       "      <td>0.886190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sm_index          pvals  sm_coeff  Lasso_Coef\n",
       "3    ground_agtron   2.899479e-05  1.210479    0.925823\n",
       "4            aroma  6.134476e-214  8.759889    9.733670\n",
       "5          acidity   3.064564e-58  5.498047    6.085170\n",
       "6             body  1.133589e-109  4.843770    5.196880\n",
       "7           flavor   3.895840e-50  5.936844    5.595205\n",
       "..             ...            ...       ...         ...\n",
       "600    understated   1.958853e-02  0.676496    0.411227\n",
       "602        unusual   1.730142e-02  0.646765    0.000000\n",
       "612         volume   2.905414e-02  0.731980    0.000000\n",
       "623          winey   3.531391e-17  1.908736    1.288508\n",
       "624           winy   2.264489e-03  0.886190    0.000000\n",
       "\n",
       "[127 rows x 4 columns]"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sig_results_cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa099b19",
   "metadata": {},
   "source": [
    "### Visualizing the Coefficients\n",
    "\n",
    "So which coefficients are important? Let's visualize it below.\n",
    "\n",
    "We will use the combined dataframe that includes both the stats model Linear Regression model and the Lasso Regression model coefficients, as well as the p-values. \n",
    "\n",
    "While the p-values are from the stats model, we're going to lean on them to filter our dataframe and remove any features where the p-value was not significant. Then we'll sort the Lasso Regression coefficients to see the top 15 negative and top 15 positive coefficients from that model. We don't know for sure that these lasso coefficients are signficant, but we can feel more confident about using them knowing the stats model found them significant. And we're opting to go with the Lasso values because we were optimized the performance of that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "c6825ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm_index</th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "      <th>Lasso_Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>month</td>\n",
       "      <td>2.054687e-01</td>\n",
       "      <td>0.072528</td>\n",
       "      <td>0.005458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>year</td>\n",
       "      <td>5.301533e-01</td>\n",
       "      <td>0.157418</td>\n",
       "      <td>-0.013003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bean_agtron</td>\n",
       "      <td>1.939661e-01</td>\n",
       "      <td>-0.488529</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ground_agtron</td>\n",
       "      <td>2.899479e-05</td>\n",
       "      <td>1.210479</td>\n",
       "      <td>0.925823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aroma</td>\n",
       "      <td>6.134476e-214</td>\n",
       "      <td>8.759889</td>\n",
       "      <td>9.733670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>wisteria</td>\n",
       "      <td>2.156571e-01</td>\n",
       "      <td>0.284021</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>wood</td>\n",
       "      <td>8.608898e-01</td>\n",
       "      <td>-0.044561</td>\n",
       "      <td>-0.067110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>woody</td>\n",
       "      <td>2.024290e-11</td>\n",
       "      <td>-2.423064</td>\n",
       "      <td>-2.004037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>zest</td>\n",
       "      <td>7.290521e-01</td>\n",
       "      <td>-0.043551</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>zesty</td>\n",
       "      <td>7.709014e-01</td>\n",
       "      <td>-0.080522</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sm_index          pvals  sm_coeff  Lasso_Coef\n",
       "0            month   2.054687e-01  0.072528    0.005458\n",
       "1             year   5.301533e-01  0.157418   -0.013003\n",
       "2      bean_agtron   1.939661e-01 -0.488529   -0.000000\n",
       "3    ground_agtron   2.899479e-05  1.210479    0.925823\n",
       "4            aroma  6.134476e-214  8.759889    9.733670\n",
       "..             ...            ...       ...         ...\n",
       "625       wisteria   2.156571e-01  0.284021    0.000000\n",
       "626           wood   8.608898e-01 -0.044561   -0.067110\n",
       "627          woody   2.024290e-11 -2.423064   -2.004037\n",
       "628           zest   7.290521e-01 -0.043551   -0.000000\n",
       "629          zesty   7.709014e-01 -0.080522   -0.000000\n",
       "\n",
       "[630 rows x 4 columns]"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#previewing the dataframe referenced below\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "7080c882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm_index</th>\n",
       "      <th>pvals</th>\n",
       "      <th>sm_coeff</th>\n",
       "      <th>Lasso_Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aroma</td>\n",
       "      <td>6.134476e-214</td>\n",
       "      <td>8.759889</td>\n",
       "      <td>9.733670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aftertaste</td>\n",
       "      <td>1.326236e-66</td>\n",
       "      <td>5.682272</td>\n",
       "      <td>7.174108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acidity</td>\n",
       "      <td>3.064564e-58</td>\n",
       "      <td>5.498047</td>\n",
       "      <td>6.085170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flavor</td>\n",
       "      <td>3.895840e-50</td>\n",
       "      <td>5.936844</td>\n",
       "      <td>5.595205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>body</td>\n",
       "      <td>1.133589e-109</td>\n",
       "      <td>4.843770</td>\n",
       "      <td>5.196880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>winey</td>\n",
       "      <td>3.531391e-17</td>\n",
       "      <td>1.908736</td>\n",
       "      <td>1.288508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>almond</td>\n",
       "      <td>7.353500e-10</td>\n",
       "      <td>2.095864</td>\n",
       "      <td>1.205609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ground_agtron</td>\n",
       "      <td>2.899479e-05</td>\n",
       "      <td>1.210479</td>\n",
       "      <td>0.925823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>best</td>\n",
       "      <td>2.743912e-09</td>\n",
       "      <td>1.005992</td>\n",
       "      <td>0.736170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>ken</td>\n",
       "      <td>6.555040e-06</td>\n",
       "      <td>1.735960</td>\n",
       "      <td>0.662624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>cut</td>\n",
       "      <td>2.649594e-04</td>\n",
       "      <td>1.247453</td>\n",
       "      <td>0.610953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>cleanly</td>\n",
       "      <td>2.637635e-03</td>\n",
       "      <td>0.544299</td>\n",
       "      <td>0.581434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>hibiscus</td>\n",
       "      <td>3.394166e-06</td>\n",
       "      <td>0.798394</td>\n",
       "      <td>0.538857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>long</td>\n",
       "      <td>2.775282e-08</td>\n",
       "      <td>0.684149</td>\n",
       "      <td>0.538236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>flower</td>\n",
       "      <td>7.085884e-07</td>\n",
       "      <td>0.707152</td>\n",
       "      <td>0.485311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sm_index          pvals  sm_coeff  Lasso_Coef\n",
       "4            aroma  6.134476e-214  8.759889    9.733670\n",
       "8       aftertaste   1.326236e-66  5.682272    7.174108\n",
       "5          acidity   3.064564e-58  5.498047    6.085170\n",
       "7           flavor   3.895840e-50  5.936844    5.595205\n",
       "6             body  1.133589e-109  4.843770    5.196880\n",
       "623          winey   3.531391e-17  1.908736    1.288508\n",
       "23          almond   7.353500e-10  2.095864    1.205609\n",
       "3    ground_agtron   2.899479e-05  1.210479    0.925823\n",
       "51            best   2.743912e-09  1.005992    0.736170\n",
       "289            ken   6.555040e-06  1.735960    0.662624\n",
       "155            cut   2.649594e-04  1.247453    0.610953\n",
       "123        cleanly   2.637635e-03  0.544299    0.581434\n",
       "263       hibiscus   3.394166e-06  0.798394    0.538857\n",
       "320           long   2.775282e-08  0.684149    0.538236\n",
       "226         flower   7.085884e-07  0.707152    0.485311"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parsing out strongest 15 positive coefficients from the lasso model\n",
    "#only including values if our stats model found the feature significant\n",
    "positive_coefs = comparison_df.loc[(comparison_df['pvals'] <= 0.05)&(comparison_df['Lasso_Coef']>=0)].sort_values('Lasso_Coef', ascending = False)[:15]\n",
    "positive_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "001ea44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing out strongest 15 negative coefficients from the lasso model\n",
    "#only including values if our stats model found the feature significant\n",
    "negative_coefs = comparison_df.loc[(comparison_df['pvals'] <= 0.05)&(comparison_df['Lasso_Coef']<0)].sort_values('Lasso_Coef', ascending = True)[:15]\n",
    "negative_coefs.sort_values('Lasso_Coef', ascending = False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "57e95917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#graphing our results\n",
    "plt.figure(figsize=(7,10))\n",
    "plt.barh(positive_coefs['sm_index'], positive_coefs['Lasso_Coef'])\n",
    "plt.barh(negative_coefs['sm_index'], negative_coefs['Lasso_Coef'])\n",
    "plt.title('Lasso Regression Feature Coefficients', fontsize=16)\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e497958d",
   "metadata": {},
   "source": [
    "**Top Negative Coefficient Findings** \n",
    "\n",
    "Negative coefficients have a negative relationship with `overall_score`, meaning as they increase, `overall_score` decreases. The top 5 using the Lasso Regression model are:\n",
    "- Guava\n",
    "- Dark\n",
    "- Woody\n",
    "- Promise\n",
    "- Burned\n",
    "\n",
    "It's interesting to see `Guava` appear as the top negative coefficient. It could be worth talking to a coffee expert to get their opinion on this finding. It could suggest this is a flavor profile to avoid. The other top negative words make sense: `dark` might be going hand in hand with `burned`, indicating roasters should perhaps take care to not overroast the beans. `Promise` could be one of those words used to soften criticism, such as \"shows great promise but falls flat\". And `woody` may be a profile to avoid.\n",
    "\n",
    "**Top Positive Coefficient Findings** \n",
    " \n",
    "Positive coefficients have a positive relationship with `overall_score`, meaning as they increase, `overall_score` increases. The top 5 using the Lasso Regression model are:\n",
    "- Aroma\n",
    "- Aftertaste\n",
    "- Acidity\n",
    "- Flavor\n",
    "- Body\n",
    "\n",
    "It's not surprising that these last five have a strong positive relationship with `overall_score` because they are numeric values given as part of the coffee evaluation process. What is interesting is how the 5 compare to each other in terms of importance, especially that `aroma` and `aftertaste` are the highest. This could suggest that when it comes to securing high `overall_scores` first impressions are very important, followed by lingering impressions (`aftertaste`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7190d87",
   "metadata": {},
   "source": [
    "## 8. Conclusion <a class=\"anchor\" id=\"header8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bbb6c3",
   "metadata": {},
   "source": [
    "**Recap** \n",
    "\n",
    "To wrap up the project, we'll provide a brief conclusion here. See the supplementary report and presentation for additional synthesis of findings. \n",
    "\n",
    "First, we started with our problem area/question: **\"Can we predict coffee ratings using the expert review data?\"** Why study this? To start, coffee is a $100 billion plus global industry and growing. In addition, there's a highly competitive market for high-quality coffee. While the data from Coffee Review spans 25 years, the reviewers themselves share on the site that their focus has shifted to high quality coffee as the market has become more competitive and specialized. Roasters looking to standout in such a competitive market need to understand what features contribute to getting top scores. With this understanding, they can improve their coffee roasting, and as a result secure higher ratings and in turn sell more coffee. Finally, ratings have become commonplace and are a frequent tool consumers reference for making purchase decisions. The insights gained from the project could be applied to other industries that rely on user ratings. \n",
    "\n",
    "Once the problem area was identified, the data was acquired through web-scraping with Beautiful Soup. It was then cleaned and compiled.\n",
    "\n",
    "After, some initial baseline tests were run on the non-text data to see how our models might perform. As a refresher, the best validation data $R^2$s from the non-text data baseline models were:\n",
    "- Linear Regression: 0.898\n",
    "- XGBoost Regression: 91.8\n",
    "\n",
    "After baseline testing, the text was extracted and transformation explored. The best text transformation method was TFIDF Vectorizer without stemming. It's validation data $R^2$ was 0.744.\n",
    "\n",
    "Finally, after selecting the best text transformation process, the data was combined and different models applied to see which could yeild the best results. While some of the models yielded similar results, **the final model selected was Lasso Regression** because of its high $R^2$, low MAE, and interpretability. When trying the model on the test data, the $R^2$ score was 0.910 and the MAE was 0.691.\n",
    "\n",
    "With the text added and models optimized, there was a slight bump in performance using linear regression methods. XGBR tended to overfit and didn't see improvement in the $R^2$ value. On the positive side, the models remained very effective at predicting scores. Further, by adding text we are able to get additional insights into what contributes to a positive or negative review that would otherwise not be apparent, such as aversion to `guava` or caution against overroasting. \n",
    "\n",
    "**Recommendations for Roasters**\n",
    "\n",
    "Based on this project, here are a few of the recommendations could be given to roasters:\n",
    "- While all 5 subscore areas are important, when the scores are translated to `overall_score`, `aroma` is crucial. This is likely because it offers the first impression of the coffee. After `aroma`, `aftertaste` is also very important, perhaps because this is the impression that lingers with the taster. \n",
    "- Positive flavor profiles appear to include: `winey`, `almond` and `hibiscus`\n",
    "- Flavor profiles to avoid may include: `guava` and `woody`\n",
    "- Roast level seems to be important; there area  couple indications of this including `ground_agtron`, which is a score indicating roast level measured on the ground coffee. In addition, `dark` appears in the negative coefficient list alongside `burned`. \n",
    "\n",
    "\n",
    "**Future Work**\n",
    "\n",
    "On any project, there will always be areas for improvement or further work. This is especially true when there are time limitations (such as is the case with this project). Some areas this work could be furthered or improved include:\n",
    "- Explore other models in more depth and include the text data\n",
    "- Refine the stemming tokenizer or try lemmatization instead\n",
    "- Clean score values out of the text paragraphs earlier in the process\n",
    "- Include more of the text (the second and third paragraphs) when optimizing the models to see if new insights surface or if the additional information can improve the models\n",
    "- Try other methods for dealing with Nan values\n",
    "- Explore topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c13d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
